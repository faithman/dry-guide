{
    "docs": [
        {
            "location": "/", 
            "text": "Welcome to the Andersen Lab Dry Guide\n\n\nThe Dry Guide details the computational infrastructure and tasks used in the Andersen Lab.", 
            "title": "Home"
        }, 
        {
            "location": "/#welcome_to_the_andersen_lab_dry_guide", 
            "text": "The Dry Guide details the computational infrastructure and tasks used in the Andersen Lab.", 
            "title": "Welcome to the Andersen Lab Dry Guide"
        }, 
        {
            "location": "/bash/", 
            "text": "Bash\n\n\nBash\n is the default unix shell on Mac OS and most Linux operating systems. Many bioinformatic programs are run using the command line, so becoming familiar with Bash is important.\n\n\nStart with the fo \nIntroduction to bash\n. \n\n\nBasic Commands\n\n\nYou should familiarize yourself with the following commands.\n\n\n\n\nalias\n - create a shortcut for a command\n\n\nawk\n - file manipulation; Filtering; Rearranging columns\n\n\ncat\n - concatenate files\n\n\nzcat\n - concatenate zipped files\n\n\ncd\n - change directories\n\n\ncurl\n - download files\n\n\necho\n - print strings\n\n\nexport\n - Add a variable to the global environment so that they get passed on to child processes.\n\n\ngrep\n - filter by pattern\n\n\negrep\n - filter by regex\n\n\nrm\n - delete files\n\n\nsed\n - quick find/replace\n\n\nsudo\n - run as an administrator\n\n\nsort\n - sorts files\n\n\nsource\n - runs a file\n\n\nssh\n - connect to servers\n\n\nwhich\n - locate files on your PATH\n\n\nuniq\n - get unique lines. File must be sorted.\n\n\n\n\nMore Advanced\n\n\nYou should learn these once you have the basics down.\n\n\n\n\ngit\n\n\n\n\nGood Guides\n\n\nBelow I link to some good guides for various bash utilities.\n\n\nawk\n\n\n\n\nawk guide\n\n\nawk by example\n - hundreds of examples\n\n\n\n\nRearranging columns\n\n\ncat example.tsv | awk -f OFS=\n\\t\n '{ print $2, $3, $1 }'\n\n\n\n\nThe line above will print the second column, the third column and finally the first column.\n\n\nFiltering based on criteria\n\n\nPrint only lines that start with a comment (#) character\n\n\ncat example.tsv awk '$0 ~ \n^#\n { print }'\n\n\n\n\nbcftools\n\n\n\n\nbcftools manual\n\n\n\n\nScreen\n\n\nScreen can be used to run things in the background. It is extremely useful if you need to run things on quest without worry that they will be terminated if you log out or get kicked off. This is essential when running \nnextflow\n because pipelines can sometimes run for many hours and its likely you will be kicked off in that time or lose your connection.\n\n\n\n\nScreen basics", 
            "title": "Bash"
        }, 
        {
            "location": "/bash/#bash", 
            "text": "Bash  is the default unix shell on Mac OS and most Linux operating systems. Many bioinformatic programs are run using the command line, so becoming familiar with Bash is important.  Start with the fo  Introduction to bash .", 
            "title": "Bash"
        }, 
        {
            "location": "/bash/#basic_commands", 
            "text": "You should familiarize yourself with the following commands.   alias  - create a shortcut for a command  awk  - file manipulation; Filtering; Rearranging columns  cat  - concatenate files  zcat  - concatenate zipped files  cd  - change directories  curl  - download files  echo  - print strings  export  - Add a variable to the global environment so that they get passed on to child processes.  grep  - filter by pattern  egrep  - filter by regex  rm  - delete files  sed  - quick find/replace  sudo  - run as an administrator  sort  - sorts files  source  - runs a file  ssh  - connect to servers  which  - locate files on your PATH  uniq  - get unique lines. File must be sorted.", 
            "title": "Basic Commands"
        }, 
        {
            "location": "/bash/#more_advanced", 
            "text": "You should learn these once you have the basics down.   git", 
            "title": "More Advanced"
        }, 
        {
            "location": "/bash/#good_guides", 
            "text": "Below I link to some good guides for various bash utilities.", 
            "title": "Good Guides"
        }, 
        {
            "location": "/bash/#awk", 
            "text": "awk guide  awk by example  - hundreds of examples", 
            "title": "awk"
        }, 
        {
            "location": "/bash/#rearranging_columns", 
            "text": "cat example.tsv | awk -f OFS= \\t  '{ print $2, $3, $1 }'  The line above will print the second column, the third column and finally the first column.", 
            "title": "Rearranging columns"
        }, 
        {
            "location": "/bash/#filtering_based_on_criteria", 
            "text": "Print only lines that start with a comment (#) character  cat example.tsv awk '$0 ~  ^#  { print }'", 
            "title": "Filtering based on criteria"
        }, 
        {
            "location": "/bash/#bcftools", 
            "text": "bcftools manual", 
            "title": "bcftools"
        }, 
        {
            "location": "/bash/#screen", 
            "text": "Screen can be used to run things in the background. It is extremely useful if you need to run things on quest without worry that they will be terminated if you log out or get kicked off. This is essential when running  nextflow  because pipelines can sometimes run for many hours and its likely you will be kicked off in that time or lose your connection.   Screen basics", 
            "title": "Screen"
        }, 
        {
            "location": "/quest-intro/", 
            "text": "Introduction\n\n\n\n\n\n\nIntroduction\n\n\nSigning into Quest\n\n\nLogin Nodes\n\n\nHome Directory\n\n\nProjects\n\n\nRunning interactive jobs on Quest\n\n\n\n\n\n\n\n\n\n\nThe Andersen Lab makes use of Quest, the supercomputer at Northwestern. Take some time to read over the overview of what Quest is, what it does, how to use it, and how to sign up:\n\n\nQuest Documentation\n\n\nSigning into Quest\n\n\nAfter you gain access to the cluster you can login using:\n\n\nssh \nnetid\n@quest.it.northwestern.edu\n\n\n\n\nI recommend setting an alias in your \n.bash_profile\n to make logging in quicker:\n\n\nalias quest=\nssh \nnetid\n@quest.it.northwestern.edu\n\n\n\n\n\nThe above line makes it so you simply type \nquest\n and the login process is initiated. \n\n\nIf you are not familiar with what a bash profile is, \ntake a look at this\n.\n\n\n\n\nImportant\n\n\nWhen you login it is important to be conscientious of the fact that you are on a login node. You should not be running any sort of heavy duty operation on these nodes. Instead, any analysis you perform should be submitted as a job or run using an interactive job (see below).\n\n\n\n\nLogin Nodes\n\n\nThere are four login nodes we use: quser10-13. When you login you will be assigned to a random login node. You can switch login nodes by typing ssh and the node desired (\ne.g.\n \nssh quser11\n).\n\n\n\n\nWarning\n\n\nWhen using \nscreen\n to submit and run jobs they will only persist on the login node you are currently on. If you log out and later log back in you may be logged in to a different login node. You will need to switch to that login node to access those screen sessions.\n\n\n\n\nHome Directory\n\n\nLogging in places you in your home directory. You can install software in your home directory for use when you run jobs, or for small files/analysis.\n\n\nYour home directory has a quota of 80 Gb. \nMore information on quotas, storage, etc\n.\n\n\nMore information is provided below to help install and use software.\n\n\nProjects\n\n\nQuest is broadly organized into projects. Projects have associated with them storage, nodes, and users.\n\n\nThe Andersen lab has access to two projects.\n\n\nb1042\n - The 'Genomics' Project has 155 Tb of space and 100 nodes associated with it. This space is shared with other labs and is designed for temporary use only (covered in greater detail in the Nextflow Section). The space is available at \n/projects/b1042/AndersenLab/\n. By default, files are deleted after 30 days.\n\n\nb1059\n - The Andersen Lab Project. \nb1059\n does not have any nodes associated with it, but it does have 10 Tb of storage. b1059 storage is located at: \n/projects/b1059/\n.\n\n\nRunning interactive jobs on Quest\n\n\nIf you are running a few simple commands or want to experiment with files directly you can start an interactive session on Quest. The command below will give you access to a node where you can run your commands\n\n\nmsub -I -A b1042\n\n\n\n\n\n\nImportant\n\n\nDo not run commands on \nquser11-13\n. These are login nodes and are not meant for running heavy-load workflows.", 
            "title": "Introduction"
        }, 
        {
            "location": "/quest-intro/#introduction", 
            "text": "Introduction  Signing into Quest  Login Nodes  Home Directory  Projects  Running interactive jobs on Quest      The Andersen Lab makes use of Quest, the supercomputer at Northwestern. Take some time to read over the overview of what Quest is, what it does, how to use it, and how to sign up:  Quest Documentation", 
            "title": "Introduction"
        }, 
        {
            "location": "/quest-intro/#signing_into_quest", 
            "text": "After you gain access to the cluster you can login using:  ssh  netid @quest.it.northwestern.edu  I recommend setting an alias in your  .bash_profile  to make logging in quicker:  alias quest= ssh  netid @quest.it.northwestern.edu   The above line makes it so you simply type  quest  and the login process is initiated.   If you are not familiar with what a bash profile is,  take a look at this .   Important  When you login it is important to be conscientious of the fact that you are on a login node. You should not be running any sort of heavy duty operation on these nodes. Instead, any analysis you perform should be submitted as a job or run using an interactive job (see below).", 
            "title": "Signing into Quest"
        }, 
        {
            "location": "/quest-intro/#login_nodes", 
            "text": "There are four login nodes we use: quser10-13. When you login you will be assigned to a random login node. You can switch login nodes by typing ssh and the node desired ( e.g.   ssh quser11 ).   Warning  When using  screen  to submit and run jobs they will only persist on the login node you are currently on. If you log out and later log back in you may be logged in to a different login node. You will need to switch to that login node to access those screen sessions.", 
            "title": "Login Nodes"
        }, 
        {
            "location": "/quest-intro/#home_directory", 
            "text": "Logging in places you in your home directory. You can install software in your home directory for use when you run jobs, or for small files/analysis.  Your home directory has a quota of 80 Gb.  More information on quotas, storage, etc .  More information is provided below to help install and use software.", 
            "title": "Home Directory"
        }, 
        {
            "location": "/quest-intro/#projects", 
            "text": "Quest is broadly organized into projects. Projects have associated with them storage, nodes, and users.  The Andersen lab has access to two projects.  b1042  - The 'Genomics' Project has 155 Tb of space and 100 nodes associated with it. This space is shared with other labs and is designed for temporary use only (covered in greater detail in the Nextflow Section). The space is available at  /projects/b1042/AndersenLab/ . By default, files are deleted after 30 days.  b1059  - The Andersen Lab Project.  b1059  does not have any nodes associated with it, but it does have 10 Tb of storage. b1059 storage is located at:  /projects/b1059/ .", 
            "title": "Projects"
        }, 
        {
            "location": "/quest-intro/#running_interactive_jobs_on_quest", 
            "text": "If you are running a few simple commands or want to experiment with files directly you can start an interactive session on Quest. The command below will give you access to a node where you can run your commands  msub -I -A b1042   Important  Do not run commands on  quser11-13 . These are login nodes and are not meant for running heavy-load workflows.", 
            "title": "Running interactive jobs on Quest"
        }, 
        {
            "location": "/quest-andersen-lab-env/", 
            "text": "The Andersen Lab Software Environment\n\n\n\n\n\n\nThe Andersen Lab Software Environment\n\n\nandersen-lab-env\n\n\npyenv\n\n\nSetting the global version\n\n\nSetting the local version\n\n\n\n\n\n\npyenv-virtualenv\n\n\nconda\n\n\nconda integrates with pyenv and pyenv-virtualenv\n\n\n\n\n\n\npyenv environments are inherited\n\n\nandersen-lab-env structure\n\n\nInstalling the andersen-lab-env\n\n\nandersen-lab-env git structure\n\n\nadding new software\n\n\n\n\n\n\n\n\n\n\nandersen-lab-env\n\n\nComputational Reproducibility\n is the ability to reproduce an analysis exactly. In order for comutational research to be reproducible you need to keep track of the code, software, and data. We keep track of code using git and GitHub. Our starting data (usually FASTQs) is almost always static, so we don't need to track changes to the data. We perform a new analysis when data is added.\n\n\nTo track software we've developed the \nandersen-lab-env\n. The andersen-lab-env is a set of software environments that can be used in conjunction with the bioinformatic pipelines we have developed for Quest. These environments can be installed locally on a Mac or on Quest. The andersen-lab-env is designed to change over time, but we explicitly define the software versions, and we track changes to the environments over time.\n\n\nThe system is in some ways complex. This page is designed to try to explain how it works. We rely on three different tools to manage software environments. In concert they provide a lot of flexibility when it comes to setting up the software environment.\n\n\n\n\nNote\n\n\nThe software environments on Mac and Linux are not exactly identical...but they are very close.\n\n\n\n\nThere is an installation script you can use to install the \nandersen-lab-env\n, but it is recommended that you read this page before doing so.\n\n\npyenv\n\n\npyenv documentation\n\n\npyenv\n is used to install and manage different versions of python. For example, you might have a python 3 script for one project and a python 2 script for another. You want to be able to run both scripts on your system. One option is to modify the python 2 script to work with python 3, but this is not always an option.\n\n\nThe solution is to be able to install \nmultiple\n versions of python simultaneously. \npyenv\n allows you to do this. More than this, pyenv allows you to set the python version that you want to use at the local or global level.\n\n\n\n\nlocal\n - Sets the python version to a specific directory.\n\n\nglobal\n - Sets the python version to use everywhere unless a local version is set.\n\n\n\n\nLets look at an example of this. First, lets install two different versions of python.\n\n\npyenv install 2.7.11\npyenv install 3.6.0\n\n\n\n\nNow you can see the installed versions by typing \npyenv versions\n:\n\n\n$ pyenv versions\n*  system\n   2.7.11\n   3.6.0\n\n\n\n\nThe \n*\n indicates that that is the current version of python you are using. In the case above it is set to use the \nsystem\n python which is preinstalled and is often python 2.\n\n\nSetting the global version\n\n\nExample setting the global version of python to 3.6.0\n\n\npyenv global 3.6.0\n\n\n\n\nNow when we run \npython\n it will use python version 3.6.0.\n\n\nSetting the local version\n\n\nExample setting the local version of python to 2.7.11\n\n\nmkdir my_python2_project\ncd my_python2_project\npyenv local 2.7.11\n\n\n\n\nNow if we go into a particular directoy and type \npyenv local 2.7.11\n, a \n.python-version\n file is created that says \n2.7.11\n and makes it so that directory always uses python 2.7.11.\n\n\nNow lets see what this looks like:\n\n\n\n\nAs is illustrated above, versions are inherited from parent directories. When you \ncd\n to a directory, pyenv searches up through each directory looking for a \n.python-version\n file to identify which version of python to use. If it reaches the top before finding one it uses the global version.\n\n\ntl;dr;\n - pyenv allows us to install separate versions of python and set them at the directory level.\n\n\npyenv-virtualenv\n\n\nDocumentation\n\n\npyenv\n lets us install multiple versions of python, and lets us use those versions locally within certain directories or globally. But what if we have two projects that use Python 2.7.11 and one requires a python module with a specific version: \nnetworkx==1.0\n. Another project the same module greater than version 2.0 \nnetworkx\n2.0\n. How can we simultaneously work on both projects on the same system?\n\n\nvirtualenv\n is a python tool for creating isolated python environments (also known as virtualenvs; The usage tends to be specific for python virtual environments and is short for 'virtual environment'). You can create a virtualenv for every project that you do - and these can be used to ensure that when you update or install modules for a given project that they do not interfere with each other. \n\n\nWe won't be using virtualenv directly, but instead will the pyenv flavor of virtualenvs. \npyenv-virtualenv\n is a tool that can create virtual environments that operate similar to the way \npyenv\n python environments do. You can create virtualenvs that act globally or you can create virtualenvs that are local to a specific directory.\n\n\nTo create a \npyenv-virtualenv\n you must provide a base python environment that you have installed and a name for the environment. For example, below \nthe python environment is \n2.7.11\n and the name of the environment is \nc_elegans_project\n:\n\n\npyenv virtualenv 2.7.11 c_elegans_project\n\n\n\n\nThen you can set that virtualenv to a local directory using:\n\n\nmkdir c_elegans_project\ncd c_elegans_project\npyenv local c_elegans_project\n\n\n\n\nNotice that the folder name is the same as the virtualenv. This can be a good idea for clarity.\n\n\nYou can see a list of python versions and virtual environments by typing:\n\n\npyenv versions\n\n\n\n\nOutput:\n\n\n    2.7.11\n  * 2.7.11/envs/c_elegans_project\n    3.6.0\n\n\n\n\nVirtual environments are designated as \nversion\n/envs/\nname\n. Now we can also install the module we need for that specific project. \npyenv\n installs a python-specific package manager called \npip\n:\n\n\npip install networkx==1.0\n\n\n\n\nNotice that at this point we have isolated independent environments that do not interfere with one another. If we leave them alone for a year we should be able to come back and the software environment should be the same... and if they work with data they should reproduce the identical result.\n\n\ntl;dr\n - pyenv-virtualenv can define custom isolated python environments and set them the same way pyenv sets python installations.\n\n\nconda\n\n\nConda Documentation\n\n\nThus far we've managed to install multiple versions of python and figured out how to use them in independent, isolated environments. But we obviously use a lot more than just Python. We need to be able to install things like \nbcftools\n to work with variant data. We need to be able to install Java packages, and R packages, and all kinds of software. \nConda\n can help us with this.\n\n\nConda\n is a language-agnostic package manager. That means it can be used to install packages from python, R, Java, C/C++, etc. \n\n\nFor example, the command below will install R and the \nR Tidyverse\n.\n\n\nconda install r-tidyverse\n\n\n\n\nconda integrates with pyenv and pyenv-virtualenv\n\n\nImportant for our purposes, \nconda\n can be installed by \npyenv\n. When I stated earlier that pyenv is used to install and manage versions of python I ommitted the fact that it can \nalso\n install conda to avoid confusion. \nconda is not a version of python\n, but it is written in python, and it can be used to install python modules in addition to lots of other stuff.\n\n\nSimilar to python virtualenvs, isolated conda environments can be created as was demonstrated above. You would run something like:\n\n\npyenv install miniconda3-4.3.27\npyenv virtualenv miniconda3-4.3.27 my_new_conda_env\npyenv local my_new_conda_env\nconda install bcftools\npip install requests # This version of pip is specific gto \n\n\n\n\nWhat is great about these environments is that we can create custom software environments to suit any project. We can install R packages, python modules, C/C++ executables, and more.\n\n\npyenv environments are inherited\n\n\nWe can now install custom environments for each project. Even better, pyenv allows you to specify multiple environments together. Consider the example in this diagram:\n\n\n\n\nThere are two environments defined:\n\n\nenv_1\n\n\n\n\nbcftools v1.6\n\n\nbedtools v1.2\n\n\nR-tidyverse 1.0\n\n\n\n\nenv_2\n\n\n\n\nbcftools v1.6\n\n\nvcf-kit v1.6\n\n\n\n\nThose environments on their own appear in blue above.\n\n\nIf we were to use the following command to specify these environments:\n\n\npyenv local env_2 env_1 base_version\n\n\n\n\nWe would produce the green environment in the diagram. What you are seeing are two environments being combined. However, the order you specify them in matters. Notice that \nbcftools v1.7\n is used and not \nbcftools v1.6\n. This is because \nenv_2\n is searched first when commands libraries are retrieved. After pulling all the libraries in \nenv_2\n, the combined library will inherit anything remaining in \nenv_1\n. This allows to easily combine environments for analysis.\n\n\nRemember that each of these virtual environments is based on a version of python or conda. But you can also put a plain version of python or conda as your last environment. This is useful when using conda because the \nconda\n command does not inherit from conda-based virtualenvs.\n\n\nandersen-lab-env structure\n\n\nThe anderse-lab-env uses two conda environments:\n\n\n\n\nprimary\n - The primary environment contains the majority of the tools required for performing sequence analysis.\n\n\npy2\n - For programs that require python 2.\n\n\n\n\nYou can create and use your own conda environments for projects, but these are designed to be comprehensive.\n\n\nInstalling the andersen-lab-env\n\n\nIf you are on Quest\n\n\nEdit your \n.bashrc\n file to contain the following:\n\n\n# .bashrc\nexport PKG_CONFIG_PATH=/usr/share/pkgconfig:$PKG_CONFIG_PATH\n# Source global definitions\nif [ -f /etc/bashrc ]; then\n    . /etc/bashrc\nfi\n\n\n\n\nInstallation\n\n\nThe andersen-lab-env can be installed by running the following command:\n\n\ncd \n if cd ~/andersen-lab-env; then git pull; else git clone http://www.github.com/andersenlab/andersen-lab-env; fi\nbash setup.sh\n\n\n\n\nThis command will clone the repo, \ncd\n into it, and run the \nsetup.sh\n script. \n\n\nWhen you run the setup.sh script it will install the latest version of the primary and py2 environments, and it will assign these environments globally as:\n\n\npyenv primary-(date) py2-(date) minicondax-x.x.x\n\n\n\n\nYou should not need to change your global environment\n\n\n\n\nNote\n\n\nIf you have existing versions of the primary and py2 environments installed they will remain. You can set them locally at the project\nlevel if necessary.\n\n\n\n\nandersen-lab-env git structure\n\n\nThe \nandersen-lab-env\n is used to manage and version the software environments. The repo has the following structure:\n\n\n\u251c\u2500\u2500 Brewfile                             \n\u251c\u2500\u2500 LICENSE\n\u251c\u2500\u2500 README.md                            \n\u251c\u2500\u2500 primary.environment.yaml             \n\u251c\u2500\u2500 py2.environment.yaml                 \n\u251c\u2500\u2500 rebuild_envs.sh                      \n\u251c\u2500\u2500 setup.sh                             \n\u251c\u2500\u2500 user_bash_profile.sh\n\u2514\u2500\u2500 versions\n    \u251c\u2500\u2500 Linux.2018-03-08.primary.yaml\n    \u251c\u2500\u2500 Linux.2018-03-08.py2.yaml\n    \u251c\u2500\u2500 Mac.2018-03-08.primary.yaml\n    \u2514\u2500\u2500 Mac.2018-03-08.py2.yaml\n\n\n\n\n\n\nprimary.environment.yaml\n - base primary environment. This lists the software to be installed, but not specific versions of it.\n\n\npy2.environment.yaml\n - The base py2 environment. This lists the software to be installed, but not specific versions of it.\n\n\nBrewfile\n - Defines the software software-dependencies to be installed when running \nsetup.sh\n\n\nrebuild_envs\n - Used to construct new versions of the environments. Note that you need to do this on a Linux and Mac computer.\n\n\nuser_bash_profile.sh\n - The optional bash profile that is created with \nsetup.sh\n.\n\n\nversions/\n - Software required for each environment with all dependencies. Versioned in git and by platform and date.\n\n\n\n\nadding new software\n\n\nWhen you want to add new software a new version of the \nprimary\n and \npy2\n environments should be created. You must modify the primary.environment.yaml or py2.environment.yaml files and build the files you see in the versions folder which define the required software by specific version and includes all the dependencies.\n\n\nbash rebuild_envs.sh\n\n\n\n\nThis will output two new versions specific to your platform in the \nversions/\n folder. You must run this script and generate the appropriate version files on both Mac and Linux. Commit the updated versions to git. Other users can then install them by running the command in \ninstalling the andersen-lab-env", 
            "title": "Andersen-Lab-Env"
        }, 
        {
            "location": "/quest-andersen-lab-env/#the_andersen_lab_software_environment", 
            "text": "The Andersen Lab Software Environment  andersen-lab-env  pyenv  Setting the global version  Setting the local version    pyenv-virtualenv  conda  conda integrates with pyenv and pyenv-virtualenv    pyenv environments are inherited  andersen-lab-env structure  Installing the andersen-lab-env  andersen-lab-env git structure  adding new software", 
            "title": "The Andersen Lab Software Environment"
        }, 
        {
            "location": "/quest-andersen-lab-env/#andersen-lab-env", 
            "text": "Computational Reproducibility  is the ability to reproduce an analysis exactly. In order for comutational research to be reproducible you need to keep track of the code, software, and data. We keep track of code using git and GitHub. Our starting data (usually FASTQs) is almost always static, so we don't need to track changes to the data. We perform a new analysis when data is added.  To track software we've developed the  andersen-lab-env . The andersen-lab-env is a set of software environments that can be used in conjunction with the bioinformatic pipelines we have developed for Quest. These environments can be installed locally on a Mac or on Quest. The andersen-lab-env is designed to change over time, but we explicitly define the software versions, and we track changes to the environments over time.  The system is in some ways complex. This page is designed to try to explain how it works. We rely on three different tools to manage software environments. In concert they provide a lot of flexibility when it comes to setting up the software environment.   Note  The software environments on Mac and Linux are not exactly identical...but they are very close.   There is an installation script you can use to install the  andersen-lab-env , but it is recommended that you read this page before doing so.", 
            "title": "andersen-lab-env"
        }, 
        {
            "location": "/quest-andersen-lab-env/#pyenv", 
            "text": "pyenv documentation  pyenv  is used to install and manage different versions of python. For example, you might have a python 3 script for one project and a python 2 script for another. You want to be able to run both scripts on your system. One option is to modify the python 2 script to work with python 3, but this is not always an option.  The solution is to be able to install  multiple  versions of python simultaneously.  pyenv  allows you to do this. More than this, pyenv allows you to set the python version that you want to use at the local or global level.   local  - Sets the python version to a specific directory.  global  - Sets the python version to use everywhere unless a local version is set.   Lets look at an example of this. First, lets install two different versions of python.  pyenv install 2.7.11\npyenv install 3.6.0  Now you can see the installed versions by typing  pyenv versions :  $ pyenv versions\n*  system\n   2.7.11\n   3.6.0  The  *  indicates that that is the current version of python you are using. In the case above it is set to use the  system  python which is preinstalled and is often python 2.", 
            "title": "pyenv"
        }, 
        {
            "location": "/quest-andersen-lab-env/#setting_the_global_version", 
            "text": "Example setting the global version of python to 3.6.0  pyenv global 3.6.0  Now when we run  python  it will use python version 3.6.0.", 
            "title": "Setting the global version"
        }, 
        {
            "location": "/quest-andersen-lab-env/#setting_the_local_version", 
            "text": "Example setting the local version of python to 2.7.11  mkdir my_python2_project\ncd my_python2_project\npyenv local 2.7.11  Now if we go into a particular directoy and type  pyenv local 2.7.11 , a  .python-version  file is created that says  2.7.11  and makes it so that directory always uses python 2.7.11.  Now lets see what this looks like:   As is illustrated above, versions are inherited from parent directories. When you  cd  to a directory, pyenv searches up through each directory looking for a  .python-version  file to identify which version of python to use. If it reaches the top before finding one it uses the global version.  tl;dr;  - pyenv allows us to install separate versions of python and set them at the directory level.", 
            "title": "Setting the local version"
        }, 
        {
            "location": "/quest-andersen-lab-env/#pyenv-virtualenv", 
            "text": "Documentation  pyenv  lets us install multiple versions of python, and lets us use those versions locally within certain directories or globally. But what if we have two projects that use Python 2.7.11 and one requires a python module with a specific version:  networkx==1.0 . Another project the same module greater than version 2.0  networkx 2.0 . How can we simultaneously work on both projects on the same system?  virtualenv  is a python tool for creating isolated python environments (also known as virtualenvs; The usage tends to be specific for python virtual environments and is short for 'virtual environment'). You can create a virtualenv for every project that you do - and these can be used to ensure that when you update or install modules for a given project that they do not interfere with each other.   We won't be using virtualenv directly, but instead will the pyenv flavor of virtualenvs.  pyenv-virtualenv  is a tool that can create virtual environments that operate similar to the way  pyenv  python environments do. You can create virtualenvs that act globally or you can create virtualenvs that are local to a specific directory.  To create a  pyenv-virtualenv  you must provide a base python environment that you have installed and a name for the environment. For example, below \nthe python environment is  2.7.11  and the name of the environment is  c_elegans_project :  pyenv virtualenv 2.7.11 c_elegans_project  Then you can set that virtualenv to a local directory using:  mkdir c_elegans_project\ncd c_elegans_project\npyenv local c_elegans_project  Notice that the folder name is the same as the virtualenv. This can be a good idea for clarity.  You can see a list of python versions and virtual environments by typing:  pyenv versions  Output:      2.7.11\n  * 2.7.11/envs/c_elegans_project\n    3.6.0  Virtual environments are designated as  version /envs/ name . Now we can also install the module we need for that specific project.  pyenv  installs a python-specific package manager called  pip :  pip install networkx==1.0  Notice that at this point we have isolated independent environments that do not interfere with one another. If we leave them alone for a year we should be able to come back and the software environment should be the same... and if they work with data they should reproduce the identical result.  tl;dr  - pyenv-virtualenv can define custom isolated python environments and set them the same way pyenv sets python installations.", 
            "title": "pyenv-virtualenv"
        }, 
        {
            "location": "/quest-andersen-lab-env/#conda", 
            "text": "Conda Documentation  Thus far we've managed to install multiple versions of python and figured out how to use them in independent, isolated environments. But we obviously use a lot more than just Python. We need to be able to install things like  bcftools  to work with variant data. We need to be able to install Java packages, and R packages, and all kinds of software.  Conda  can help us with this.  Conda  is a language-agnostic package manager. That means it can be used to install packages from python, R, Java, C/C++, etc.   For example, the command below will install R and the  R Tidyverse .  conda install r-tidyverse", 
            "title": "conda"
        }, 
        {
            "location": "/quest-andersen-lab-env/#conda_integrates_with_pyenv_and_pyenv-virtualenv", 
            "text": "Important for our purposes,  conda  can be installed by  pyenv . When I stated earlier that pyenv is used to install and manage versions of python I ommitted the fact that it can  also  install conda to avoid confusion.  conda is not a version of python , but it is written in python, and it can be used to install python modules in addition to lots of other stuff.  Similar to python virtualenvs, isolated conda environments can be created as was demonstrated above. You would run something like:  pyenv install miniconda3-4.3.27\npyenv virtualenv miniconda3-4.3.27 my_new_conda_env\npyenv local my_new_conda_env\nconda install bcftools\npip install requests # This version of pip is specific gto   What is great about these environments is that we can create custom software environments to suit any project. We can install R packages, python modules, C/C++ executables, and more.", 
            "title": "conda integrates with pyenv and pyenv-virtualenv"
        }, 
        {
            "location": "/quest-andersen-lab-env/#pyenv_environments_are_inherited", 
            "text": "We can now install custom environments for each project. Even better, pyenv allows you to specify multiple environments together. Consider the example in this diagram:   There are two environments defined:  env_1   bcftools v1.6  bedtools v1.2  R-tidyverse 1.0   env_2   bcftools v1.6  vcf-kit v1.6   Those environments on their own appear in blue above.  If we were to use the following command to specify these environments:  pyenv local env_2 env_1 base_version  We would produce the green environment in the diagram. What you are seeing are two environments being combined. However, the order you specify them in matters. Notice that  bcftools v1.7  is used and not  bcftools v1.6 . This is because  env_2  is searched first when commands libraries are retrieved. After pulling all the libraries in  env_2 , the combined library will inherit anything remaining in  env_1 . This allows to easily combine environments for analysis.  Remember that each of these virtual environments is based on a version of python or conda. But you can also put a plain version of python or conda as your last environment. This is useful when using conda because the  conda  command does not inherit from conda-based virtualenvs.", 
            "title": "pyenv environments are inherited"
        }, 
        {
            "location": "/quest-andersen-lab-env/#andersen-lab-env_structure", 
            "text": "The anderse-lab-env uses two conda environments:   primary  - The primary environment contains the majority of the tools required for performing sequence analysis.  py2  - For programs that require python 2.   You can create and use your own conda environments for projects, but these are designed to be comprehensive.", 
            "title": "andersen-lab-env structure"
        }, 
        {
            "location": "/quest-andersen-lab-env/#installing_the_andersen-lab-env", 
            "text": "If you are on Quest  Edit your  .bashrc  file to contain the following:  # .bashrc\nexport PKG_CONFIG_PATH=/usr/share/pkgconfig:$PKG_CONFIG_PATH\n# Source global definitions\nif [ -f /etc/bashrc ]; then\n    . /etc/bashrc\nfi  Installation  The andersen-lab-env can be installed by running the following command:  cd   if cd ~/andersen-lab-env; then git pull; else git clone http://www.github.com/andersenlab/andersen-lab-env; fi\nbash setup.sh  This command will clone the repo,  cd  into it, and run the  setup.sh  script.   When you run the setup.sh script it will install the latest version of the primary and py2 environments, and it will assign these environments globally as:  pyenv primary-(date) py2-(date) minicondax-x.x.x  You should not need to change your global environment   Note  If you have existing versions of the primary and py2 environments installed they will remain. You can set them locally at the project\nlevel if necessary.", 
            "title": "Installing the andersen-lab-env"
        }, 
        {
            "location": "/quest-andersen-lab-env/#andersen-lab-env_git_structure", 
            "text": "The  andersen-lab-env  is used to manage and version the software environments. The repo has the following structure:  \u251c\u2500\u2500 Brewfile                             \n\u251c\u2500\u2500 LICENSE\n\u251c\u2500\u2500 README.md                            \n\u251c\u2500\u2500 primary.environment.yaml             \n\u251c\u2500\u2500 py2.environment.yaml                 \n\u251c\u2500\u2500 rebuild_envs.sh                      \n\u251c\u2500\u2500 setup.sh                             \n\u251c\u2500\u2500 user_bash_profile.sh\n\u2514\u2500\u2500 versions\n    \u251c\u2500\u2500 Linux.2018-03-08.primary.yaml\n    \u251c\u2500\u2500 Linux.2018-03-08.py2.yaml\n    \u251c\u2500\u2500 Mac.2018-03-08.primary.yaml\n    \u2514\u2500\u2500 Mac.2018-03-08.py2.yaml   primary.environment.yaml  - base primary environment. This lists the software to be installed, but not specific versions of it.  py2.environment.yaml  - The base py2 environment. This lists the software to be installed, but not specific versions of it.  Brewfile  - Defines the software software-dependencies to be installed when running  setup.sh  rebuild_envs  - Used to construct new versions of the environments. Note that you need to do this on a Linux and Mac computer.  user_bash_profile.sh  - The optional bash profile that is created with  setup.sh .  versions/  - Software required for each environment with all dependencies. Versioned in git and by platform and date.", 
            "title": "andersen-lab-env git structure"
        }, 
        {
            "location": "/quest-andersen-lab-env/#adding_new_software", 
            "text": "When you want to add new software a new version of the  primary  and  py2  environments should be created. You must modify the primary.environment.yaml or py2.environment.yaml files and build the files you see in the versions folder which define the required software by specific version and includes all the dependencies.  bash rebuild_envs.sh  This will output two new versions specific to your platform in the  versions/  folder. You must run this script and generate the appropriate version files on both Mac and Linux. Commit the updated versions to git. Other users can then install them by running the command in  installing the andersen-lab-env", 
            "title": "adding new software"
        }, 
        {
            "location": "/quest-nextflow/", 
            "text": "Installation\n\n\nQuest cluster configuration\n\n\nGlobal Configuration: ~/.nextflow/config\n\n\n\n\n\n\nScreen\n\n\nResources\n\n\n\n\n\n\nInstallation\n\n\n\n\nImportant\n\n\nIf you haven't already, take a look at \nAndersen-Lab-Env\n for more information on how to setup your environment on Quest.\n\n\n\n\nNextflow can be installed with linuxbrew or homebrew. Use:\n\n\nbrew tap homebrew/science\nbrew install nextflow\n\n\n\n\nQuest cluster configuration\n\n\nConfiguration files allow you to define the way a pipeline is executed on Quest. \n\n\nRead the quest documentation on configuration files\n\n\nConfiguration files are defined at a global level in \n~/.nextflow/config\n and on a per-pipeline basis within \npipeline_directory\n/nextflow.config\n. Settings written in \npipeline_directory\n/nextflow.config\n override settings written in \n~/.nextflow/config\n.\n\n\nGlobal Configuration: \n~/.nextflow/config\n\n\nIn order to use nextflow on quest you will need to define some global variables regarding the process. Our lab utilizies nodes and space dedicated to genomics projects. In order to access these resources your account will need to be granted access. Contact Quest and request access to the genomics nodes and project \nb1042\n. Once you have access you will need to modify your global configuration. Set your \n~/.nextflow/config\n file to be the following:\n\n\nprocess {\n    executor = 'pbs'\n    queue = 'genomicsguest'\n    clusterOptions = '-A b1042 -l walltime=24:00:00 -e errlog.txt'\n    genome = \nWS245\n\n    reference = \n/projects/b1059/data/genomes/c_elegans/${genome}/${genome}.fa.gz\n\n}\n\nworkDir = \n/projects/b1042/AndersenLab/work\n\ntmpDir = \n/projects/b1042/AndersenLab/tmp\n\n\n\n\n\n\nThis configuration file does the following:\n\n\n\n\nSets the executor to \npbs\n (which is what Quest uses)\n\n\nSets the queue to \ngenomicsguest\n which submits jobs to genomics nodes.\n\n\n\n\nclusterOptions\n - Sets the account to \nb1042\n; granting access to genomics-dedicated scratch space.\n\n\n\n\n\n\nworkDir\n - Sets the working directory to scratch space on b1042.\n\n\n\n\ntmpDir\n - Creates a temporary working directory. This can be used within workflows when necessary.\n\n\n\n\nScreen\n\n\nWhen jobs run for a very long time you should run them in screen. Screen lets you continue to run jobs in the background even if you get kicked off the cluster or log off.\n\n\n\n\nScreen Tutorial\n\n\n\n\nKeep in mind that quest has several login nodes. We use \nquser10-13\n. Screen sessions only persist on \nONE\n of these login nodes. You can jump between nodes by simply typing ssh and the login node you want (\ne.g.\n \nssh quser 10\n). \n\n\nResources\n\n\n\n\nNextflow documentation\n\n\nAwesome Nextflow pipeline examples\n - Repository of great nextflow pipelines.", 
            "title": "Nextflow"
        }, 
        {
            "location": "/quest-nextflow/#installation", 
            "text": "Important  If you haven't already, take a look at  Andersen-Lab-Env  for more information on how to setup your environment on Quest.   Nextflow can be installed with linuxbrew or homebrew. Use:  brew tap homebrew/science\nbrew install nextflow", 
            "title": "Installation"
        }, 
        {
            "location": "/quest-nextflow/#quest_cluster_configuration", 
            "text": "Configuration files allow you to define the way a pipeline is executed on Quest.   Read the quest documentation on configuration files  Configuration files are defined at a global level in  ~/.nextflow/config  and on a per-pipeline basis within  pipeline_directory /nextflow.config . Settings written in  pipeline_directory /nextflow.config  override settings written in  ~/.nextflow/config .", 
            "title": "Quest cluster configuration"
        }, 
        {
            "location": "/quest-nextflow/#global_configuration_nextflowconfig", 
            "text": "In order to use nextflow on quest you will need to define some global variables regarding the process. Our lab utilizies nodes and space dedicated to genomics projects. In order to access these resources your account will need to be granted access. Contact Quest and request access to the genomics nodes and project  b1042 . Once you have access you will need to modify your global configuration. Set your  ~/.nextflow/config  file to be the following:  process {\n    executor = 'pbs'\n    queue = 'genomicsguest'\n    clusterOptions = '-A b1042 -l walltime=24:00:00 -e errlog.txt'\n    genome =  WS245 \n    reference =  /projects/b1059/data/genomes/c_elegans/${genome}/${genome}.fa.gz \n}\n\nworkDir =  /projects/b1042/AndersenLab/work \ntmpDir =  /projects/b1042/AndersenLab/tmp   This configuration file does the following:   Sets the executor to  pbs  (which is what Quest uses)  Sets the queue to  genomicsguest  which submits jobs to genomics nodes.   clusterOptions  - Sets the account to  b1042 ; granting access to genomics-dedicated scratch space.    workDir  - Sets the working directory to scratch space on b1042.   tmpDir  - Creates a temporary working directory. This can be used within workflows when necessary.", 
            "title": "Global Configuration: ~/.nextflow/config"
        }, 
        {
            "location": "/quest-nextflow/#screen", 
            "text": "When jobs run for a very long time you should run them in screen. Screen lets you continue to run jobs in the background even if you get kicked off the cluster or log off.   Screen Tutorial   Keep in mind that quest has several login nodes. We use  quser10-13 . Screen sessions only persist on  ONE  of these login nodes. You can jump between nodes by simply typing ssh and the login node you want ( e.g.   ssh quser 10 ).", 
            "title": "Screen"
        }, 
        {
            "location": "/quest-nextflow/#resources", 
            "text": "Nextflow documentation  Awesome Nextflow pipeline examples  - Repository of great nextflow pipelines.", 
            "title": "Resources"
        }, 
        {
            "location": "/quest-mount/", 
            "text": "1. Download and Install Fuse for Mac OS\n\n\nhttps://osxfuse.github.io/\n\n\n2. Install sshfs\n\n\nYou can use the link on https://osxfuse.github.io/ or use:\n\n\nbrew install sshfs\n\n\n\n\n3. Create a folder in your documents called \nb1059\n\n\nmkdir ~/b1059\n\n\n\n\n4. Mount our labs quest project folder (\nb1059\n) to the \nb1059\n folder you created locally\n\n\nsshfs \nNETID\n@quest.it.northwestern.edu:/projects/b1059/ ~/Documents/b1059 -ovolname=b1059\n\n\n\n\nTo mount alignments of isotypes at this location:\n\n\nsshfs \nNETID\n@quest.it.northwestern.edu:/projects/b1059/data/alignments/WI/isotype ~/Documents/b1059 -ovolname=b1059", 
            "title": "Mounting Quest"
        }, 
        {
            "location": "/pipeline-overview/", 
            "text": "Pipeline Overview\n\n\n\n\nAn overview of the sequencing pipelines is shown above. Wild isolate data are processed by multiple pipelines. NIL/RIL sequence data are only processed by one pipeline.", 
            "title": "Overview"
        }, 
        {
            "location": "/pipeline-overview/#pipeline_overview", 
            "text": "An overview of the sequencing pipelines is shown above. Wild isolate data are processed by multiple pipelines. NIL/RIL sequence data are only processed by one pipeline.", 
            "title": "Pipeline Overview"
        }, 
        {
            "location": "/adding-seq-data/", 
            "text": "Adding Sequencing Data", 
            "title": "Adding Sequencing Data"
        }, 
        {
            "location": "/adding-seq-data/#adding_sequencing_data", 
            "text": "", 
            "title": "Adding Sequencing Data"
        }, 
        {
            "location": "/sample-sheets/", 
            "text": "Sample Sheets\n\n\n\n\n\n\nSample Sheets\n\n\nCreating sample sheets\n\n\nwi-nf and concordance-nf pipelines\n\n\nnil-ril-nf\n\n\n\n\n\n\nSample-Sheet Format\n\n\nAbsolute vs. relative paths\n\n\n\n\n\n\n\n\n\n\nThe \nwi-nf\n, \nconcordance-nf\n, and \nnil-ril-nf\n pipelines all make use of sample sheets. Sample sheets specify which fastqs belong to a given strain or isotype.\n\n\nCreating sample sheets\n\n\nwi-nf and concordance-nf pipelines\n\n\nFor the \nwi-nf\n and \nconcordance-nf\n pipelines, sample-sheets are generated using the file located (in each of these repos) in the \nscripts/construct_sample_sheet.sh\n. Importantly, these scripts are almost identical except that the \nconcordance-nf\n pipeline constructs a sample sheet for \nstrains\n whereas the \nwi-nf\n sample sheet is for \nisotypes\n.\n\n\nWhen adding new sequence data you need to update these scripts. See \nAdding Sequencing Data\n for more information.\n\n\n\n\nNote\n\n\nThe nomenclature regarding sample sheets and scripts was changed in March of 2018 to make it clearer. You may encounter older files with the following names that correspond to the newer names\n\n\n\n\nSM_sample_sheet\n --\n \nsample_sheet.tsv\n\n\nconstruct_SM_sheet.sh\n --\n \nconstruct_sample_sheet.tsv\n\n\n\n\n\n\nnil-ril-nf\n\n\nFor the nil-ril-nf pipelines you must manually create the sample sheets according to the format below.\n\n\nSample-Sheet Format\n\n\nThe sample sheet defines which FASTQs belong to which strain/isotype and specifies additional information regarding a sample. Additional information specfieid are the FASTQ ID (a unique identifier for a FASTQ-pair), Sequencing POOL (which defines the group of samples that were sequenced together), the locations of the FASTQs, and the sequencing folder.\n\n\n\n\nNote\n\n\nInternally, the 'sequencing pool' information as treated as the DNA-library identifier by BWA (\nLB\n).\nOur lab processes sequence data such that the pool name uniquely identifies DNA-libraries for each sample.\n\n\n\n\nSample sheet structure\n\n\nAll columns are required.\n\n\n\n\nSample Identifier\n - How FASTQs should be grouped in the pipeline. Usually this is by strain or isotype.\n\n\nFASTQ ID\n - A unique ID for the FASTQ pair. It must be unique for all sequencing runs defined in the sample sheet.\n\n\nSequencing pool\n - The sequencing pool is often defined arbitrarily. It refers to the set of strains that were sequenced together. It acts as an identifer of the DNA library within the pipelines.\n\n\nFASTQ1\n - A relative or absolute path to the first FASTQ.\n\n\nFASTQ2\n - A relative or absolute path to the second FASTQ.\n\n\nSequencing Folder\n - This column is provided for informational purposes. It generally refers to the name of the folder containing the FASTQs.\n\n\n\n\nExample\n \n\n\nAB1 BGI1-RET2-AB1   RET2    /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI1-RET2-AB1-trim-1P.fq.gz /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI1-RET2-AB1-trim-2P.fq.gz original_wi_set\nAB1 BGI2-RET2-AB1   RET2    /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI2-RET2-AB1-trim-1P.fq.gz /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI2-RET2-AB1-trim-2P.fq.gz original_wi_set\nAB1 BGI3-RET2b-AB1  RET2b   /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI3-RET2b-AB1-trim-1P.fq.gz    /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI3-RET2b-AB1-trim-2P.fq.gz    original_wi_set\n\n\n\n\nNotice that the file \ndoes not include a header\n. The table with corresponding header included below look like this:\n\n\n\n\n\n\n\n\nSample Identifeir\n\n\nFASTQ ID\n\n\nSequencing Pool\n\n\nfastq-1-path\n\n\nfastq-2-path\n\n\nsequencing_folder\n\n\n\n\n\n\n\n\n\n\nAB1\n\n\nBGI1-RET2-AB1\n\n\nRET2\n\n\n/projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI1-RET2-AB1-trim-1P.fq.gz\n\n\n/projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI1-RET2-AB1-trim-2P.fq.gz\n\n\noriginal_wi_set\n\n\n\n\n\n\nAB1\n\n\nBGI2-RET2-AB1\n\n\nRET2\n\n\n/projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI2-RET2-AB1-trim-1P.fq.gz\n\n\n/projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI2-RET2-AB1-trim-2P.fq.gz\n\n\noriginal_wi_set\n\n\n\n\n\n\nAB1\n\n\nBGI3-RET2b-AB1\n\n\nRET2b\n\n\n/projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI3-RET2b-AB1-trim-1P.fq.gz\n\n\n/projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI3-RET2b-AB1-trim-2P.fq.gz\n\n\noriginal_wi_set\n\n\n\n\n\n\n\n\nAbsolute vs. relative paths\n\n\nWhen constructing the sample sheet for the \nwi-nf\n and \nconcordance-nf\n pipelines you are required to use the absolute paths to each FASTQ. The \nnil-ril-nf\n pipeline can use relative paths to FASTQs by specifying the \n--fq_file_prefix\n option to the parent directory containing FASTQs.", 
            "title": "Sample Sheets"
        }, 
        {
            "location": "/sample-sheets/#sample_sheets", 
            "text": "Sample Sheets  Creating sample sheets  wi-nf and concordance-nf pipelines  nil-ril-nf    Sample-Sheet Format  Absolute vs. relative paths      The  wi-nf ,  concordance-nf , and  nil-ril-nf  pipelines all make use of sample sheets. Sample sheets specify which fastqs belong to a given strain or isotype.", 
            "title": "Sample Sheets"
        }, 
        {
            "location": "/sample-sheets/#creating_sample_sheets", 
            "text": "", 
            "title": "Creating sample sheets"
        }, 
        {
            "location": "/sample-sheets/#wi-nf_and_concordance-nf_pipelines", 
            "text": "For the  wi-nf  and  concordance-nf  pipelines, sample-sheets are generated using the file located (in each of these repos) in the  scripts/construct_sample_sheet.sh . Importantly, these scripts are almost identical except that the  concordance-nf  pipeline constructs a sample sheet for  strains  whereas the  wi-nf  sample sheet is for  isotypes .  When adding new sequence data you need to update these scripts. See  Adding Sequencing Data  for more information.   Note  The nomenclature regarding sample sheets and scripts was changed in March of 2018 to make it clearer. You may encounter older files with the following names that correspond to the newer names   SM_sample_sheet  --   sample_sheet.tsv  construct_SM_sheet.sh  --   construct_sample_sheet.tsv", 
            "title": "wi-nf and concordance-nf pipelines"
        }, 
        {
            "location": "/sample-sheets/#nil-ril-nf", 
            "text": "For the nil-ril-nf pipelines you must manually create the sample sheets according to the format below.", 
            "title": "nil-ril-nf"
        }, 
        {
            "location": "/sample-sheets/#sample-sheet_format", 
            "text": "The sample sheet defines which FASTQs belong to which strain/isotype and specifies additional information regarding a sample. Additional information specfieid are the FASTQ ID (a unique identifier for a FASTQ-pair), Sequencing POOL (which defines the group of samples that were sequenced together), the locations of the FASTQs, and the sequencing folder.   Note  Internally, the 'sequencing pool' information as treated as the DNA-library identifier by BWA ( LB ).\nOur lab processes sequence data such that the pool name uniquely identifies DNA-libraries for each sample.   Sample sheet structure  All columns are required.   Sample Identifier  - How FASTQs should be grouped in the pipeline. Usually this is by strain or isotype.  FASTQ ID  - A unique ID for the FASTQ pair. It must be unique for all sequencing runs defined in the sample sheet.  Sequencing pool  - The sequencing pool is often defined arbitrarily. It refers to the set of strains that were sequenced together. It acts as an identifer of the DNA library within the pipelines.  FASTQ1  - A relative or absolute path to the first FASTQ.  FASTQ2  - A relative or absolute path to the second FASTQ.  Sequencing Folder  - This column is provided for informational purposes. It generally refers to the name of the folder containing the FASTQs.   Example    AB1 BGI1-RET2-AB1   RET2    /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI1-RET2-AB1-trim-1P.fq.gz /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI1-RET2-AB1-trim-2P.fq.gz original_wi_set\nAB1 BGI2-RET2-AB1   RET2    /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI2-RET2-AB1-trim-1P.fq.gz /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI2-RET2-AB1-trim-2P.fq.gz original_wi_set\nAB1 BGI3-RET2b-AB1  RET2b   /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI3-RET2b-AB1-trim-1P.fq.gz    /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI3-RET2b-AB1-trim-2P.fq.gz    original_wi_set  Notice that the file  does not include a header . The table with corresponding header included below look like this:     Sample Identifeir  FASTQ ID  Sequencing Pool  fastq-1-path  fastq-2-path  sequencing_folder      AB1  BGI1-RET2-AB1  RET2  /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI1-RET2-AB1-trim-1P.fq.gz  /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI1-RET2-AB1-trim-2P.fq.gz  original_wi_set    AB1  BGI2-RET2-AB1  RET2  /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI2-RET2-AB1-trim-1P.fq.gz  /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI2-RET2-AB1-trim-2P.fq.gz  original_wi_set    AB1  BGI3-RET2b-AB1  RET2b  /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI3-RET2b-AB1-trim-1P.fq.gz  /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI3-RET2b-AB1-trim-2P.fq.gz  original_wi_set", 
            "title": "Sample-Sheet Format"
        }, 
        {
            "location": "/sample-sheets/#absolute_vs_relative_paths", 
            "text": "When constructing the sample sheet for the  wi-nf  and  concordance-nf  pipelines you are required to use the absolute paths to each FASTQ. The  nil-ril-nf  pipeline can use relative paths to FASTQs by specifying the  --fq_file_prefix  option to the parent directory containing FASTQs.", 
            "title": "Absolute vs. relative paths"
        }, 
        {
            "location": "/pipeline-trimming/", 
            "text": "trimmomatic-nf\n\n\nThe trimmomatic workflow performs trimming to remove poor quality sequences and technical sequences such as adapters. It should be used with high-coverage genomic DNA. You should not use the trimmomatic workflow on low-coverage NIL or RIL data.\n\n\n\n\n\n\ntrimmomatic-nf\n\n\nUsage\n\n\nPrerequisites\n\n\nRunning the pipeline\n\n\nParameters\n\n\n\n\n\n\n\n\n\n\nOverview\n\n\nOutput\n\n\nCleanup\n\n\nPoor quality data\n\n\nBackup\n\n\n\n\n\n\n\n\n\n\nUsage\n\n\nPrerequisites\n\n\n\n\nYou have downloaded FASTQ Data to a subdirectory within a raw directory. For wild isolates this will be \n/projects/b1059/data/fastq/WI/dna/raw/\nfolder_name\n\n\nFASTQs \nmust\n end in a \n.fq.gz\n extension for the pipeline to work..\n\n\nYou have modified FASTQ names if necessary to add strain names or other identifying information.\n\n\nYou have installed software-requirements, preferably using the \nandersen-lab-env\n. You can learn how to install the environment \nhere\n.\n\n\n\n\nSoftware requirements\n\n\n\n\ntrimmomatic\n\n\nfastqc\n\n\nmultiqc\n\n\n\n\n\n\nNote\n\n\nAll FASTQs should end with a \n_1.fq.gz\n or a \n_2.fq.gz\n. You can rename FASTQs using the rename command:\n\n\nrename --dry-run --subst .fastq.gz .fq.gz --subst _R1_001 _1 --subst _R2_001 _2 *.fastq.gz\n\n\nThe \n--dry-run\n flag will output how files will be renamed. Review the output and remove\nthe flag when you are ready. \n\n\n\n\nRunning the pipeline\n\n\nFirst you will need to \ncd\n to the directory containing the raw FASTQs. This directory will be downloaded into a \nraw\n parent directory.\nWhen you run the pipeline it will create a sequence directory of the same name in an existing or newly created \nprocessed\n directory and dump FASTQs there.\n\n\nUnlike all other pipelines, the \ntrimmomatic-nf\n pipeline is run directly from the git repo\n\n\nnextflow run andersenlab/trimmomatic-nf -latest --email \nyour email address\n\n\n\n\n\n\n\nNote\n\n\nThe pipeline is designed to not be destructive. Trimming creates from the \nraw\n parent directory to the processed parent directory as\nshown below.\n\n\n/projects/b1059/data/fastq/WI/dna/raw/\nseq_folder\n/S_1.fq.gz\n \n\n\n-- trimming --\n\n\n/projects/b1059/data/fastq/WI/dna/processed/\nfolder_name\n/S_1P.fq.gz\n\n\n\n\nParameters\n\n\n\n\n--email\n - Specify an email address to be notified when the pipeline succeeds or fails.\n\n\n\n\nOverview\n\n\n\n\nOutput\n\n\nThe resulting trimmed FASTQs will be output in the \nprocessed\n directory located up one level from the current directory. For example:\n\n\nFASTQs are originally deposited in this directory\n\n\n/projects/b1059/data/fastq/WI/dna/raw/new_wi_seq\n\n\nYou run the pipeline while sitting in the same directory:\n\n\n/projects/b1059/data/fastq/WI/dna/raw/new_wi_seq\n\n\nAnd results are output in the following directory:\n\n\n/projects/b1059/data/fastq/WI/dna/processed/new_wi_seq\n\n\nReport output\n\n\nThe \ntrimmomatic-nf\n pipeline outputs four files, all of which are located in the \nprocessed\n directory. Continuing with the example above, report files will be located here:\n\n\n/projects/b1059/data/fastq/WI/dna/processed/new_wi_seq/report\n\n\nThe report output files are:\n\n\n\n\nmd5sum.txt\n - md5 hashes of all the untrimmed FASTQs. These can be used to verify the integrity of the files.\n\n\ntrimming_log.txt\n - A summary of the pipeline-run and software environment.\n\n\nmultiqc_report_pre.html\n - Aggregated FASTQC report before trimming.\n\n\nmultiqc_report_post.html\n - Aggregated FASTQC report after trimming.\n\n\n\n\nAdditionally, the \nraw/seq\n and \nprocessed/seq\n will have \nfastqc/\n folders containing the original, unaggregatred FASTQC reports.\n\n\nCleanup\n\n\nIf you have triple-checked everything and are satisfied with the results, the original, raw sequence data can be deleted.\n\n\nPoor quality data\n\n\nIf you observe poor quality sequence data you should remove it.\n\n\nBackup\n\n\nOnce you have completed the trimmomatic-nf pipeline you should backup the FASTQs. More information on this is available in the \nbackup", 
            "title": "Trimming"
        }, 
        {
            "location": "/pipeline-trimming/#trimmomatic-nf", 
            "text": "The trimmomatic workflow performs trimming to remove poor quality sequences and technical sequences such as adapters. It should be used with high-coverage genomic DNA. You should not use the trimmomatic workflow on low-coverage NIL or RIL data.    trimmomatic-nf  Usage  Prerequisites  Running the pipeline  Parameters      Overview  Output  Cleanup  Poor quality data  Backup", 
            "title": "trimmomatic-nf"
        }, 
        {
            "location": "/pipeline-trimming/#usage", 
            "text": "", 
            "title": "Usage"
        }, 
        {
            "location": "/pipeline-trimming/#prerequisites", 
            "text": "You have downloaded FASTQ Data to a subdirectory within a raw directory. For wild isolates this will be  /projects/b1059/data/fastq/WI/dna/raw/ folder_name  FASTQs  must  end in a  .fq.gz  extension for the pipeline to work..  You have modified FASTQ names if necessary to add strain names or other identifying information.  You have installed software-requirements, preferably using the  andersen-lab-env . You can learn how to install the environment  here .   Software requirements   trimmomatic  fastqc  multiqc    Note  All FASTQs should end with a  _1.fq.gz  or a  _2.fq.gz . You can rename FASTQs using the rename command:  rename --dry-run --subst .fastq.gz .fq.gz --subst _R1_001 _1 --subst _R2_001 _2 *.fastq.gz  The  --dry-run  flag will output how files will be renamed. Review the output and remove\nthe flag when you are ready.", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/pipeline-trimming/#running_the_pipeline", 
            "text": "First you will need to  cd  to the directory containing the raw FASTQs. This directory will be downloaded into a  raw  parent directory.\nWhen you run the pipeline it will create a sequence directory of the same name in an existing or newly created  processed  directory and dump FASTQs there.  Unlike all other pipelines, the  trimmomatic-nf  pipeline is run directly from the git repo  nextflow run andersenlab/trimmomatic-nf -latest --email  your email address    Note  The pipeline is designed to not be destructive. Trimming creates from the  raw  parent directory to the processed parent directory as\nshown below.  /projects/b1059/data/fastq/WI/dna/raw/ seq_folder /S_1.fq.gz    -- trimming --  /projects/b1059/data/fastq/WI/dna/processed/ folder_name /S_1P.fq.gz", 
            "title": "Running the pipeline"
        }, 
        {
            "location": "/pipeline-trimming/#parameters", 
            "text": "--email  - Specify an email address to be notified when the pipeline succeeds or fails.", 
            "title": "Parameters"
        }, 
        {
            "location": "/pipeline-trimming/#overview", 
            "text": "", 
            "title": "Overview"
        }, 
        {
            "location": "/pipeline-trimming/#output", 
            "text": "The resulting trimmed FASTQs will be output in the  processed  directory located up one level from the current directory. For example:  FASTQs are originally deposited in this directory  /projects/b1059/data/fastq/WI/dna/raw/new_wi_seq  You run the pipeline while sitting in the same directory:  /projects/b1059/data/fastq/WI/dna/raw/new_wi_seq  And results are output in the following directory:  /projects/b1059/data/fastq/WI/dna/processed/new_wi_seq  Report output  The  trimmomatic-nf  pipeline outputs four files, all of which are located in the  processed  directory. Continuing with the example above, report files will be located here:  /projects/b1059/data/fastq/WI/dna/processed/new_wi_seq/report  The report output files are:   md5sum.txt  - md5 hashes of all the untrimmed FASTQs. These can be used to verify the integrity of the files.  trimming_log.txt  - A summary of the pipeline-run and software environment.  multiqc_report_pre.html  - Aggregated FASTQC report before trimming.  multiqc_report_post.html  - Aggregated FASTQC report after trimming.   Additionally, the  raw/seq  and  processed/seq  will have  fastqc/  folders containing the original, unaggregatred FASTQC reports.", 
            "title": "Output"
        }, 
        {
            "location": "/pipeline-trimming/#cleanup", 
            "text": "If you have triple-checked everything and are satisfied with the results, the original, raw sequence data can be deleted.", 
            "title": "Cleanup"
        }, 
        {
            "location": "/pipeline-trimming/#poor_quality_data", 
            "text": "If you observe poor quality sequence data you should remove it.", 
            "title": "Poor quality data"
        }, 
        {
            "location": "/pipeline-trimming/#backup", 
            "text": "Once you have completed the trimmomatic-nf pipeline you should backup the FASTQs. More information on this is available in the  backup", 
            "title": "Backup"
        }, 
        {
            "location": "/pipeline-nil-ril/", 
            "text": "nil-ril-nf\n\n\nThe \nnil-ril-nf\n pipeline will align, call variants, and generate datasets for NIL and RIL sequence data. It runs a hidden-markov-model to fill in missing genotypes from low-coverage sequence data.\n\n\n\n\n\n\nnil-ril-nf\n\n\nDocker image\n\n\nandersenlab/nil-ril-nf\n\n\n\n\n\n\nUsage\n\n\nOverview\n\n\nDocker image\n\n\nUsage\n\n\nProfiles and Running the Pipeline\n\n\nRunning the pipeline locally\n\n\nDebugging the pipeline on Quest\n\n\nRunning the pipeline on Quest\n\n\n\n\n\n\n\n\n\n\nTesting\n\n\nParameters\n\n\n--debug\n\n\n--cores\n\n\n--A, --B\n\n\n--cA, --cB\n\n\n--out\n\n\n--fqs (FASTQs)\n\n\n--relative\n\n\n--vcf (Parental VCF)\n\n\n--reference\n\n\n--tmpdir\n\n\n\n\n\n\nOutput\n\n\nlog.txt\n\n\nduplicates/\n\n\nfq/\n\n\nSM/\n\n\nhmm/\n\n\nplots/\n\n\nsitelist/\n\n\n\n\n\n\n\n\n\n\nDocker image\n\n\nThe docker image used by the \nnil-ril-nf\n pipeline is the \nnil-ril-nf\n docker image:\n\n\nandersenlab/nil-ril-nf\n\n\nThe \nDockerfile\n is stored in the root of the \nnil-nf\n github repo and is automatically built on \nDockerhub\n whenever the repo is pushed.\n\n\nUsage\n\n\n\n    \u2588\u2588\u2588\u2557   \u2588\u2588\u2557\u2588\u2588\u2557\u2588\u2588\u2557      \u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2557\u2588\u2588\u2557      \u2588\u2588\u2588\u2557   \u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\n    \u2588\u2588\u2588\u2588\u2557  \u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2551      \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2551\u2588\u2588\u2551      \u2588\u2588\u2588\u2588\u2557  \u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\n    \u2588\u2588\u2554\u2588\u2588\u2557 \u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2554\u2588\u2588\u2557 \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2557\n    \u2588\u2588\u2551\u255a\u2588\u2588\u2557\u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2551\u255a\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2551\u2588\u2588\u2551\u255a\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2551\u255a\u2588\u2588\u2557\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u255d\n    \u2588\u2588\u2551 \u255a\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2551  \u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2551 \u255a\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2551\n    \u255a\u2550\u255d  \u255a\u2550\u2550\u2550\u255d\u255a\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d \u255a\u2550\u255d  \u255a\u2550\u255d\u255a\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d \u255a\u2550\u255d  \u255a\u2550\u2550\u2550\u255d\u255a\u2550\u255d\n\n\n    parameters           description                    Set/Default\n    ==========           ===========                    =======\n\n    --debug              Set to 'true' to test          false\n    --cores              Number of cores                4\n    --A                  Parent A                       N2\n    --B                  Parent B                       CB4856\n    --cA                 Parent A color (for plots)     #0080FF\n    --cB                 Parent B color (for plots)     #FF8000\n    --out                Directory to output results    NIL-N2-CB4856-2017-09-27\n    --fqs                fastq file (see help)          (required)\n    --relative           use relative fastq prefix      ${params.relative}\n    --reference          Reference Genome               /Users/dancook/Documents/git/nil-nf/reference/WS245.fa.gz\n    --vcf                VCF to fetch parents from      (required)\n    --tmpdir             A temporary directory          tmp/\n\n\n    The Set/Default column shows what the value is currently set to\n    or would be set to if it is not specified (it's default).\n\n\n\n\n\n\nOverview\n\n\nThe \nnil-ril-nf\n pipeline:\n\n\n\n\n\n\nAlignment\n - Performed using bwa-mem\n\n\nMerge Bams\n - Combines bam files aligned individually for each fastq-pair. \nSambamba\n is actually used in place of samtools, but it's a drop-in, faster replacement.\n\n\nBam Stats\n - A variety of metrics are calculated for bams and combined into individual files for downstream analsyis.\n\n\nMark Duplicates\n - Duplicate reads are marked using Picard.\n\n\nCall Variants individual\n - Variants are called for each strain inidividually first. This generates a sitelist which is used to identify all variant sites in the population.\n\n\nPull parental genotypes\n - Pulls out parental genotypes from the given VCF. The list of genotypes is filtered for discordant calls (i.e. different genotypes). This is VCF is used to generate a sitelist for calling low-coverage bams and later is merged into the resulting VCF.\n\n\nCall variants union\n - Uses the sitelist from the previous step to call variants on low-coverage sequence data. The resulting VCF will have a lot of missing calls.\n\n\nMerge VCF\n - Merges in the parental VCF (which has been filtered only for variants with discordant calls).\n\n\nCall HMM\n - VCF-kit is run in various ways to infer the appropriate genotypes from the low-coverage sequence data.\n\n\n\n\nDocker image\n\n\nThe docker image used by the \nnil-ril-nf\n pipeline is the \nnil-ril-nf\n docker image:\n\n\nandersenlab/nil-ril-nf\n\n\nThe \nDockerfile\n is stored in the root of the \nnil-ril-nf\n github repo and is automatically built on \nDockerhub\n whenever the repo is pushed.\n\n\nUsage\n\n\nProfiles and Running the Pipeline\n\n\nThe \nnextflow.config\n file included with this pipeline contains four profiles. These set up the environment for testing local development, testing on Quest, and running the pipeline on Quest.\n\n\n\n\nlocal\n - Used for local development. Uses the docker container.\n\n\nquest_debug\n - Runs a small subset of available test data. Should complete within a couple of hours. For testing/diagnosing issues on Quest.\n\n\nquest\n - Runs the entire dataset.\n\n\ntravis\n - Used by travis-ci for testing purposes.\n\n\n\n\nRunning the pipeline locally\n\n\nWhen running locally, the pipeline will run using the \nandersenlab/nil-ril-nf\n docker image. You must have docker installed. You will need to obtain a reference genome to run the alignment with as well. You can use the following command to obtain the reference:\n\n\ncurl https://storage.googleapis.com/elegansvariation.org/genome/WS245/WS245.tar.gz \n WS245.tar.gz\ntar -xvzf WS245.tar.gz\n\n\n\n\nRun the pipeline locally with:\n\n\nnextflow run main.nf -profile local -resume\n\n\n\n\nDebugging the pipeline on Quest\n\n\nWhen running on Quest, you should first run the quest debug profile. The Quest debug profile will use a test dataset and sample sheet which runs much faster and will encounter errors much sooner should they need to be fixed. If the debug dataset runs to completion it is likely that the full dataset will as well.\n\n\nnextflow run main.nf -profile quest_debug -resume\n\n\n\n\nRunning the pipeline on Quest\n\n\nThe pipeline can be run on Quest using the following command:\n\n\nnextflow run main.nf -profile quest -resume\n\n\n\n\nTesting\n\n\nIf you are going to modify the pipeline, I highly recommend doing so in a testing environment. The pipeline includes a debug dataset that runs rather quickly (~10 minutes). If you cache results initially and re-run with the \n-resume\n option it is fairly easy to add new processes or modify existing ones and still ensure that things are output correctly.\n\n\nAdditionally - note that the pipeline is tested everytime a change is made and pushed to github. Testing takes place on travis-ci \nhere\n, and a badge is visible on the readme indicating the current 'build status'. If the pipeline encounters any errors when being run on travis-ci the 'build' will fail.\n\n\nThe command below can be used to test the pipeline locally.\n\n\n# Downloads a pre-indexed reference\ncurl https://storage.googleapis.com/elegansvariation.org/genome/WS245/WS245.tar.gz \n WS245.tar.gz\ntar -xvzf WS245.tar.gz\n# Run nextflow\nnextflow run andersenlab/nil-ril-nf \\\n             -with-docker andersenlab/nil-ril-nf \\\n             --debug \\\n             --reference=WS245.fa.gz \\\n             -resume\n\n\n\n\n\n\nNote that the path to the vcf will change slightly in releases later than WI-20170531; See the \nwi-nf\n pipeline for details.\n\n\nThe command above will automatically place results in a folder: \nNIL-N2-CB4856-YYYY-MM-DD\n\n\n\n\nParameters\n\n\n--debug\n\n\nThe pipeline comes pre-packed with fastq's and a VCF that can be used to debug. See the \nTesting\n section for more information.\n\n\n--cores\n\n\nThe number of cores to use during alignments and variant calling.\n\n\n--A, --B\n\n\nTwo parental strains must be provided. By default these are N2 and CB4856. The parental strains provided \nmust\n be present in the VCF provided. Their genotypes are pulled from that VCF and used to generate the HMM. See below for more details.\n\n\n--cA, --cB\n\n\nThe color to use for parental strain A and B on plots.\n\n\n--out\n\n\nA directory in which to output results. By default it will be \nNIL-A-B-YYYY-MM-DD\n where A and be are the parental strains.\n\n\n--fqs (FASTQs)\n\n\nIn order to process NIL/RIL data, you need to move the sequence data to a folder and create a \nfq_sheet.tsv\n. This file defines the fastqs that should be processed. The fastq can be specified as \nrelative\n or \nabsolute\n. By default, they are expected to be relative to the fastq file. The FASTQ sheet details strain names, ids, library, and files. It should be tab-delimited and look like this:\n\n\nNIL_01   NIL_01_ID    S16 NIL_01_1.fq.gz   NIL_01_2.fq.gz\nNIL_02   NIL_02_ID    S1  NIL_02_1.fq.gz   NIL_02_2.fq.gz\n\n\n\n\nNotice that the file does not include a header. The table with corresponding columns looks like this.\n\n\n\n\n\n\n\n\nstrain\n\n\nfastq_pair_id\n\n\nlibrary\n\n\nfastq-1-path\n\n\nfastq-2-path\n\n\n\n\n\n\n\n\n\n\nNIL_01\n\n\nNIL_01_ID\n\n\nS16\n\n\nNIL_01_1.fq.gz\n\n\nNIL_01_2.fq.gz\n\n\n\n\n\n\nNIL_02\n\n\nNIL_02_ID\n\n\nS1\n\n\nNIL_02_1.fq.gz\n\n\nNIL_02_2.fq.gz\n\n\n\n\n\n\n\n\nThe columns are detailed below:\n\n\n\n\nstrain\n - The name of the strain. If a strain was sequenced multiple times this file is used to identify that fact and merge those fastq-pairs together following alignment.\n\n\nfastq_pair_id\n - This must be unique identifier for all individual FASTQ pairs.\n\n\nlibrary\n - A string identifying the DNA library. If you sequenced a strain from different library preps it can be beneficial when calling variants. The string can be arbitrary (e.g. LIB1) as well if only one library prep was used.\n\n\nfastq-1-path\n - The \nrelative\n path of the first fastq.\n\n\nfastq-2-path\n - The \nrelative\n path of the second fastq.\n\n\n\n\nThis file needs to be placed along with the sequence data into a folder. The tree will look like this:\n\n\nNIL_SEQ_DATA/\n\u251c\u2500\u2500 NIL_01_1.fq.gz\n\u251c\u2500\u2500 NIL_01_2.fq.gz\n\u251c\u2500\u2500 NIL_02_1.fq.gz\n\u251c\u2500\u2500 NIL_02_2.fq.gz\n\u2514\u2500\u2500 fq_sheet.tsv\n\n\n\n\nSet \n--fqs\n as \n--fqs=/the/path/to/fq_sheet.tsv\n.\n\n\n\n\nImportant\n\n\nDo not perform any pre-processing on NIL data. NIL-data is low-coverage by design and you want to retain as much sequence data (however poor) as possible.\n\n\n\n\nIf you want to specify fastqs using an absolute path use \n--relative=false\n\n\n--relative\n\n\nSet to \ntrue\n by default. If you set \n--relative=false\n, fq's in the fq_sheet are expected to use an absolute path.\n\n\n--vcf (Parental VCF)\n\n\nBefore you begin, you will need access to a VCF with high-coverage data from the parental strains. In general, this can be obtained using the latest release of the wild-isolate data which is usually located in the b1059 analysis folder. For example, you would likely want to use:\n\n\n/projects/b1059/analysis/WI-20170531/vcf/WI.20170531.hard-filter.vcf.gz\n\n\nThis is the \nhard-filtered\n VCF, meaning that poor quality variants have been stripped. Use hard-filtered VCFs for this pipeline.\n\n\nSet the parental VCF as \n--vcf=/the/path/to/WI.20170531.hard-filter.vcf.gz\n\n\n--reference\n\n\nA fasta reference indexed with BWA. On Quest, the reference is available here:\n\n\n/projects/b1059/data/genomes/c_elegans/WS245/WS245.fa.gz\n\n\n\n\n--tmpdir\n\n\nA directory for storing temporary data.\n\n\nOutput\n\n\nThe final output directory looks like this:\n\n\n.\n\u251c\u2500\u2500 log.txt\n\u251c\u2500\u2500 fq\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 fq_bam_idxstats.tsv\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 fq_bam_stats.tsv\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 fq_coverage.full.tsv\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 fq_coverage.tsv\n\u251c\u2500\u2500 SM\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 SM_bam_idxstats.tsv\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 SM_bam_stats.tsv\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 SM_coverage.full.tsv\n\u2502   \u251c\u2500\u2500 SM_union_vcfs.txt\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 SM_coverage.tsv\n\u251c\u2500\u2500 hmm\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 gt_hmm.(png/svg)\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 gt_hmm.tsv\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 gt_hmm_fill.tsv\n\u2502   \u251c\u2500\u2500 NIL.filtered.stats.txt\n\u2502   \u251c\u2500\u2500 NIL.filtered.vcf.gz\n\u2502   \u251c\u2500\u2500 NIL.filtered.vcf.gz.csi\n\u2502   \u251c\u2500\u2500 NIL.hmm.vcf.gz\n\u2502   \u251c\u2500\u2500 NIL.hmm.vcf.gz.csi\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 gt_hmm_genotypes.tsv\n\u251c\u2500\u2500 bam\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 \nBAMS + indices\n\n\u251c\u2500\u2500 duplicates\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 bam_duplicates.tsv\n\u2514\u2500 sitelist\n \u00a0\u00a0 \u251c\u2500\u2500 N2.CB4856.sitelist.[tsv/vcf].gz\n \u00a0\u00a0 \u2514\u2500\u2500 N2.CB4856.sitelist.[tsv/vcf].gz.[tbi/csi]\n\n\n\n\nlog.txt\n\n\nA summary of the nextflow run.\n\n\nduplicates/\n\n\nbam_duplicates.tsv\n - A summary of duplicate reads from aligned bams.\n\n\nfq/\n\n\n\n\nfq_bam_idxstats.tsv\n - A summary of mapped and unmapped reads by fastq pair.\n\n\nfq_bam_stats.tsv\n - BAM summary by fastq pair.\n\n\nfq_coverage.full.tsv\n - Coverage summary by chromosome\n\n\nfq_coverage.tsv\n - Simple coverage file by fastq\n\n\n\n\nSM/\n\n\nIf you have multiple fastq pairs per sample, their alignments will be combined into a strain or sample-level BAM and the results will be output to this directory.\n\n\n\n\nSM_bam_idxstats.tsv\n - A summary of mapped and unmapped reads by sample.\n\n\nSM_bam_stats.tsv\n - BAM summary at the sample level\n\n\nSM_coverage.full.tsv\n - Coverage at the sample level\n\n\nSM_coverage.tsv\n - Simple coverage at the sample level.\n\n\nSM_union_vcfs.txt\n - A list of VCFs that were merged to generate RIL.filter.vcf.gz\n\n\n\n\nhmm/\n\n\n\n\nImportant\n\n\ngt_hmm_fill.tsv\n is for visualization purposes only. To determine breakpoints you should use \ngt_hmm.tsv\n.\n\n\n\n\n\n\nThe \n--infill\n and \n--endfill\n options are applied to the \ngt_hmm_fill.tsv\n file. You need to be cautious when examining this data as it is \ngenerated primarily for visualization purposes\n.\n\n\n\n\ngt_hmm.(png/svg)\n - Haplotype plot \nusing\n \n--infill\n and \n--endfill\n.\n\n\ngt_hmm_fill.tsv\n - Same as above, but using \n--infill\n and \n--endfill\n with VCF-Kit. For more information, see \nVCF-Kit Documentation\n. This file is used to generate the plots.\n\n\ngt_hmm.tsv\n - Haplotypes defined by region with associated information. Does \nnot\n use \n--infill\n and \n--endfill\n\n\ngt_hmm_genotypes.tsv\n - Long form genotypes file.\n\n\nNIL/RIL.filtered.vcf.gz\n - A VCF genotypes including the NILs and parental genotypes.\n\n\nNIL/RIL.filtered.stats.txt\n - Summary of filtered genotypes. Generated by \nbcftools stats NIL.filtered.vcf.gz\n\n\nNIL/RIL.hmm.vcf.gz\n - The NIL/RIL VCF as output by VCF-Kit; HMM applied to determine genotypes.\n\n\n\n\nplots/\n\n\n\n\ncoverage_comparison.(png/svg/pdf)\n - Compares FASTQ and Sample-level coverage. Note that coverage is not simply cumulative. Only uniquely mapped reads count towards coverage, so it is possible that the sample-level coverage will not equal to the cumulative sum of the coverages of individual FASTQ pairs.\n\n\nduplicates.(png/svg/pdf)\n - Coverage vs. percent duplicated.\n\n\nunmapped_reads.(png/svg/pdf)\n - Coverage vs. unmapped read percent.\n\n\n\n\nsitelist/\n\n\n\n\nA\n.\nB\n.sitelist.tsv.gz[+.tbi]\n - A tabix-indexed list of sites found to be different between both parental strains.\n\n\nA\n.\nB\n.sitelist.vcf.gz[+.tbi]\n - A vcf of sites found to be different between both parental strains.", 
            "title": "NIL / RIL"
        }, 
        {
            "location": "/pipeline-nil-ril/#nil-ril-nf", 
            "text": "The  nil-ril-nf  pipeline will align, call variants, and generate datasets for NIL and RIL sequence data. It runs a hidden-markov-model to fill in missing genotypes from low-coverage sequence data.    nil-ril-nf  Docker image  andersenlab/nil-ril-nf    Usage  Overview  Docker image  Usage  Profiles and Running the Pipeline  Running the pipeline locally  Debugging the pipeline on Quest  Running the pipeline on Quest      Testing  Parameters  --debug  --cores  --A, --B  --cA, --cB  --out  --fqs (FASTQs)  --relative  --vcf (Parental VCF)  --reference  --tmpdir    Output  log.txt  duplicates/  fq/  SM/  hmm/  plots/  sitelist/", 
            "title": "nil-ril-nf"
        }, 
        {
            "location": "/pipeline-nil-ril/#docker_image", 
            "text": "The docker image used by the  nil-ril-nf  pipeline is the  nil-ril-nf  docker image:", 
            "title": "Docker image"
        }, 
        {
            "location": "/pipeline-nil-ril/#andersenlabnil-ril-nf", 
            "text": "The  Dockerfile  is stored in the root of the  nil-nf  github repo and is automatically built on  Dockerhub  whenever the repo is pushed.", 
            "title": "andersenlab/nil-ril-nf"
        }, 
        {
            "location": "/pipeline-nil-ril/#usage", 
            "text": "\u2588\u2588\u2588\u2557   \u2588\u2588\u2557\u2588\u2588\u2557\u2588\u2588\u2557      \u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2557\u2588\u2588\u2557      \u2588\u2588\u2588\u2557   \u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\n    \u2588\u2588\u2588\u2588\u2557  \u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2551      \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2551\u2588\u2588\u2551      \u2588\u2588\u2588\u2588\u2557  \u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\n    \u2588\u2588\u2554\u2588\u2588\u2557 \u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2554\u2588\u2588\u2557 \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2557\n    \u2588\u2588\u2551\u255a\u2588\u2588\u2557\u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2551\u255a\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2551\u2588\u2588\u2551\u255a\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2551\u255a\u2588\u2588\u2557\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u255d\n    \u2588\u2588\u2551 \u255a\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2551  \u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2551 \u255a\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2551\n    \u255a\u2550\u255d  \u255a\u2550\u2550\u2550\u255d\u255a\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d \u255a\u2550\u255d  \u255a\u2550\u255d\u255a\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d \u255a\u2550\u255d  \u255a\u2550\u2550\u2550\u255d\u255a\u2550\u255d\n\n\n    parameters           description                    Set/Default\n    ==========           ===========                    =======\n\n    --debug              Set to 'true' to test          false\n    --cores              Number of cores                4\n    --A                  Parent A                       N2\n    --B                  Parent B                       CB4856\n    --cA                 Parent A color (for plots)     #0080FF\n    --cB                 Parent B color (for plots)     #FF8000\n    --out                Directory to output results    NIL-N2-CB4856-2017-09-27\n    --fqs                fastq file (see help)          (required)\n    --relative           use relative fastq prefix      ${params.relative}\n    --reference          Reference Genome               /Users/dancook/Documents/git/nil-nf/reference/WS245.fa.gz\n    --vcf                VCF to fetch parents from      (required)\n    --tmpdir             A temporary directory          tmp/\n\n\n    The Set/Default column shows what the value is currently set to\n    or would be set to if it is not specified (it's default).", 
            "title": "Usage"
        }, 
        {
            "location": "/pipeline-nil-ril/#overview", 
            "text": "The  nil-ril-nf  pipeline:    Alignment  - Performed using bwa-mem  Merge Bams  - Combines bam files aligned individually for each fastq-pair.  Sambamba  is actually used in place of samtools, but it's a drop-in, faster replacement.  Bam Stats  - A variety of metrics are calculated for bams and combined into individual files for downstream analsyis.  Mark Duplicates  - Duplicate reads are marked using Picard.  Call Variants individual  - Variants are called for each strain inidividually first. This generates a sitelist which is used to identify all variant sites in the population.  Pull parental genotypes  - Pulls out parental genotypes from the given VCF. The list of genotypes is filtered for discordant calls (i.e. different genotypes). This is VCF is used to generate a sitelist for calling low-coverage bams and later is merged into the resulting VCF.  Call variants union  - Uses the sitelist from the previous step to call variants on low-coverage sequence data. The resulting VCF will have a lot of missing calls.  Merge VCF  - Merges in the parental VCF (which has been filtered only for variants with discordant calls).  Call HMM  - VCF-kit is run in various ways to infer the appropriate genotypes from the low-coverage sequence data.", 
            "title": "Overview"
        }, 
        {
            "location": "/pipeline-nil-ril/#docker_image_1", 
            "text": "The docker image used by the  nil-ril-nf  pipeline is the  nil-ril-nf  docker image:  andersenlab/nil-ril-nf  The  Dockerfile  is stored in the root of the  nil-ril-nf  github repo and is automatically built on  Dockerhub  whenever the repo is pushed.", 
            "title": "Docker image"
        }, 
        {
            "location": "/pipeline-nil-ril/#usage_1", 
            "text": "", 
            "title": "Usage"
        }, 
        {
            "location": "/pipeline-nil-ril/#profiles_and_running_the_pipeline", 
            "text": "The  nextflow.config  file included with this pipeline contains four profiles. These set up the environment for testing local development, testing on Quest, and running the pipeline on Quest.   local  - Used for local development. Uses the docker container.  quest_debug  - Runs a small subset of available test data. Should complete within a couple of hours. For testing/diagnosing issues on Quest.  quest  - Runs the entire dataset.  travis  - Used by travis-ci for testing purposes.", 
            "title": "Profiles and Running the Pipeline"
        }, 
        {
            "location": "/pipeline-nil-ril/#running_the_pipeline_locally", 
            "text": "When running locally, the pipeline will run using the  andersenlab/nil-ril-nf  docker image. You must have docker installed. You will need to obtain a reference genome to run the alignment with as well. You can use the following command to obtain the reference:  curl https://storage.googleapis.com/elegansvariation.org/genome/WS245/WS245.tar.gz   WS245.tar.gz\ntar -xvzf WS245.tar.gz  Run the pipeline locally with:  nextflow run main.nf -profile local -resume", 
            "title": "Running the pipeline locally"
        }, 
        {
            "location": "/pipeline-nil-ril/#debugging_the_pipeline_on_quest", 
            "text": "When running on Quest, you should first run the quest debug profile. The Quest debug profile will use a test dataset and sample sheet which runs much faster and will encounter errors much sooner should they need to be fixed. If the debug dataset runs to completion it is likely that the full dataset will as well.  nextflow run main.nf -profile quest_debug -resume", 
            "title": "Debugging the pipeline on Quest"
        }, 
        {
            "location": "/pipeline-nil-ril/#running_the_pipeline_on_quest", 
            "text": "The pipeline can be run on Quest using the following command:  nextflow run main.nf -profile quest -resume", 
            "title": "Running the pipeline on Quest"
        }, 
        {
            "location": "/pipeline-nil-ril/#testing", 
            "text": "If you are going to modify the pipeline, I highly recommend doing so in a testing environment. The pipeline includes a debug dataset that runs rather quickly (~10 minutes). If you cache results initially and re-run with the  -resume  option it is fairly easy to add new processes or modify existing ones and still ensure that things are output correctly.  Additionally - note that the pipeline is tested everytime a change is made and pushed to github. Testing takes place on travis-ci  here , and a badge is visible on the readme indicating the current 'build status'. If the pipeline encounters any errors when being run on travis-ci the 'build' will fail.  The command below can be used to test the pipeline locally.  # Downloads a pre-indexed reference\ncurl https://storage.googleapis.com/elegansvariation.org/genome/WS245/WS245.tar.gz   WS245.tar.gz\ntar -xvzf WS245.tar.gz\n# Run nextflow\nnextflow run andersenlab/nil-ril-nf \\\n             -with-docker andersenlab/nil-ril-nf \\\n             --debug \\\n             --reference=WS245.fa.gz \\\n             -resume   Note that the path to the vcf will change slightly in releases later than WI-20170531; See the  wi-nf  pipeline for details.  The command above will automatically place results in a folder:  NIL-N2-CB4856-YYYY-MM-DD", 
            "title": "Testing"
        }, 
        {
            "location": "/pipeline-nil-ril/#parameters", 
            "text": "", 
            "title": "Parameters"
        }, 
        {
            "location": "/pipeline-nil-ril/#--debug", 
            "text": "The pipeline comes pre-packed with fastq's and a VCF that can be used to debug. See the  Testing  section for more information.", 
            "title": "--debug"
        }, 
        {
            "location": "/pipeline-nil-ril/#--cores", 
            "text": "The number of cores to use during alignments and variant calling.", 
            "title": "--cores"
        }, 
        {
            "location": "/pipeline-nil-ril/#--a_--b", 
            "text": "Two parental strains must be provided. By default these are N2 and CB4856. The parental strains provided  must  be present in the VCF provided. Their genotypes are pulled from that VCF and used to generate the HMM. See below for more details.", 
            "title": "--A, --B"
        }, 
        {
            "location": "/pipeline-nil-ril/#--ca_--cb", 
            "text": "The color to use for parental strain A and B on plots.", 
            "title": "--cA, --cB"
        }, 
        {
            "location": "/pipeline-nil-ril/#--out", 
            "text": "A directory in which to output results. By default it will be  NIL-A-B-YYYY-MM-DD  where A and be are the parental strains.", 
            "title": "--out"
        }, 
        {
            "location": "/pipeline-nil-ril/#--fqs_fastqs", 
            "text": "In order to process NIL/RIL data, you need to move the sequence data to a folder and create a  fq_sheet.tsv . This file defines the fastqs that should be processed. The fastq can be specified as  relative  or  absolute . By default, they are expected to be relative to the fastq file. The FASTQ sheet details strain names, ids, library, and files. It should be tab-delimited and look like this:  NIL_01   NIL_01_ID    S16 NIL_01_1.fq.gz   NIL_01_2.fq.gz\nNIL_02   NIL_02_ID    S1  NIL_02_1.fq.gz   NIL_02_2.fq.gz  Notice that the file does not include a header. The table with corresponding columns looks like this.     strain  fastq_pair_id  library  fastq-1-path  fastq-2-path      NIL_01  NIL_01_ID  S16  NIL_01_1.fq.gz  NIL_01_2.fq.gz    NIL_02  NIL_02_ID  S1  NIL_02_1.fq.gz  NIL_02_2.fq.gz     The columns are detailed below:   strain  - The name of the strain. If a strain was sequenced multiple times this file is used to identify that fact and merge those fastq-pairs together following alignment.  fastq_pair_id  - This must be unique identifier for all individual FASTQ pairs.  library  - A string identifying the DNA library. If you sequenced a strain from different library preps it can be beneficial when calling variants. The string can be arbitrary (e.g. LIB1) as well if only one library prep was used.  fastq-1-path  - The  relative  path of the first fastq.  fastq-2-path  - The  relative  path of the second fastq.   This file needs to be placed along with the sequence data into a folder. The tree will look like this:  NIL_SEQ_DATA/\n\u251c\u2500\u2500 NIL_01_1.fq.gz\n\u251c\u2500\u2500 NIL_01_2.fq.gz\n\u251c\u2500\u2500 NIL_02_1.fq.gz\n\u251c\u2500\u2500 NIL_02_2.fq.gz\n\u2514\u2500\u2500 fq_sheet.tsv  Set  --fqs  as  --fqs=/the/path/to/fq_sheet.tsv .   Important  Do not perform any pre-processing on NIL data. NIL-data is low-coverage by design and you want to retain as much sequence data (however poor) as possible.   If you want to specify fastqs using an absolute path use  --relative=false", 
            "title": "--fqs (FASTQs)"
        }, 
        {
            "location": "/pipeline-nil-ril/#--relative", 
            "text": "Set to  true  by default. If you set  --relative=false , fq's in the fq_sheet are expected to use an absolute path.", 
            "title": "--relative"
        }, 
        {
            "location": "/pipeline-nil-ril/#--vcf_parental_vcf", 
            "text": "Before you begin, you will need access to a VCF with high-coverage data from the parental strains. In general, this can be obtained using the latest release of the wild-isolate data which is usually located in the b1059 analysis folder. For example, you would likely want to use:  /projects/b1059/analysis/WI-20170531/vcf/WI.20170531.hard-filter.vcf.gz  This is the  hard-filtered  VCF, meaning that poor quality variants have been stripped. Use hard-filtered VCFs for this pipeline.  Set the parental VCF as  --vcf=/the/path/to/WI.20170531.hard-filter.vcf.gz", 
            "title": "--vcf (Parental VCF)"
        }, 
        {
            "location": "/pipeline-nil-ril/#--reference", 
            "text": "A fasta reference indexed with BWA. On Quest, the reference is available here:  /projects/b1059/data/genomes/c_elegans/WS245/WS245.fa.gz", 
            "title": "--reference"
        }, 
        {
            "location": "/pipeline-nil-ril/#--tmpdir", 
            "text": "A directory for storing temporary data.", 
            "title": "--tmpdir"
        }, 
        {
            "location": "/pipeline-nil-ril/#output", 
            "text": "The final output directory looks like this:  .\n\u251c\u2500\u2500 log.txt\n\u251c\u2500\u2500 fq\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 fq_bam_idxstats.tsv\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 fq_bam_stats.tsv\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 fq_coverage.full.tsv\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 fq_coverage.tsv\n\u251c\u2500\u2500 SM\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 SM_bam_idxstats.tsv\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 SM_bam_stats.tsv\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 SM_coverage.full.tsv\n\u2502   \u251c\u2500\u2500 SM_union_vcfs.txt\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 SM_coverage.tsv\n\u251c\u2500\u2500 hmm\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 gt_hmm.(png/svg)\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 gt_hmm.tsv\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 gt_hmm_fill.tsv\n\u2502   \u251c\u2500\u2500 NIL.filtered.stats.txt\n\u2502   \u251c\u2500\u2500 NIL.filtered.vcf.gz\n\u2502   \u251c\u2500\u2500 NIL.filtered.vcf.gz.csi\n\u2502   \u251c\u2500\u2500 NIL.hmm.vcf.gz\n\u2502   \u251c\u2500\u2500 NIL.hmm.vcf.gz.csi\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 gt_hmm_genotypes.tsv\n\u251c\u2500\u2500 bam\n\u2502\u00a0\u00a0 \u2514\u2500\u2500  BAMS + indices \n\u251c\u2500\u2500 duplicates\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 bam_duplicates.tsv\n\u2514\u2500 sitelist\n \u00a0\u00a0 \u251c\u2500\u2500 N2.CB4856.sitelist.[tsv/vcf].gz\n \u00a0\u00a0 \u2514\u2500\u2500 N2.CB4856.sitelist.[tsv/vcf].gz.[tbi/csi]", 
            "title": "Output"
        }, 
        {
            "location": "/pipeline-nil-ril/#logtxt", 
            "text": "A summary of the nextflow run.", 
            "title": "log.txt"
        }, 
        {
            "location": "/pipeline-nil-ril/#duplicates", 
            "text": "bam_duplicates.tsv  - A summary of duplicate reads from aligned bams.", 
            "title": "duplicates/"
        }, 
        {
            "location": "/pipeline-nil-ril/#fq", 
            "text": "fq_bam_idxstats.tsv  - A summary of mapped and unmapped reads by fastq pair.  fq_bam_stats.tsv  - BAM summary by fastq pair.  fq_coverage.full.tsv  - Coverage summary by chromosome  fq_coverage.tsv  - Simple coverage file by fastq", 
            "title": "fq/"
        }, 
        {
            "location": "/pipeline-nil-ril/#sm", 
            "text": "If you have multiple fastq pairs per sample, their alignments will be combined into a strain or sample-level BAM and the results will be output to this directory.   SM_bam_idxstats.tsv  - A summary of mapped and unmapped reads by sample.  SM_bam_stats.tsv  - BAM summary at the sample level  SM_coverage.full.tsv  - Coverage at the sample level  SM_coverage.tsv  - Simple coverage at the sample level.  SM_union_vcfs.txt  - A list of VCFs that were merged to generate RIL.filter.vcf.gz", 
            "title": "SM/"
        }, 
        {
            "location": "/pipeline-nil-ril/#hmm", 
            "text": "Important  gt_hmm_fill.tsv  is for visualization purposes only. To determine breakpoints you should use  gt_hmm.tsv .    The  --infill  and  --endfill  options are applied to the  gt_hmm_fill.tsv  file. You need to be cautious when examining this data as it is  generated primarily for visualization purposes .   gt_hmm.(png/svg)  - Haplotype plot  using   --infill  and  --endfill .  gt_hmm_fill.tsv  - Same as above, but using  --infill  and  --endfill  with VCF-Kit. For more information, see  VCF-Kit Documentation . This file is used to generate the plots.  gt_hmm.tsv  - Haplotypes defined by region with associated information. Does  not  use  --infill  and  --endfill  gt_hmm_genotypes.tsv  - Long form genotypes file.  NIL/RIL.filtered.vcf.gz  - A VCF genotypes including the NILs and parental genotypes.  NIL/RIL.filtered.stats.txt  - Summary of filtered genotypes. Generated by  bcftools stats NIL.filtered.vcf.gz  NIL/RIL.hmm.vcf.gz  - The NIL/RIL VCF as output by VCF-Kit; HMM applied to determine genotypes.", 
            "title": "hmm/"
        }, 
        {
            "location": "/pipeline-nil-ril/#plots", 
            "text": "coverage_comparison.(png/svg/pdf)  - Compares FASTQ and Sample-level coverage. Note that coverage is not simply cumulative. Only uniquely mapped reads count towards coverage, so it is possible that the sample-level coverage will not equal to the cumulative sum of the coverages of individual FASTQ pairs.  duplicates.(png/svg/pdf)  - Coverage vs. percent duplicated.  unmapped_reads.(png/svg/pdf)  - Coverage vs. unmapped read percent.", 
            "title": "plots/"
        }, 
        {
            "location": "/pipeline-nil-ril/#sitelist", 
            "text": "A . B .sitelist.tsv.gz[+.tbi]  - A tabix-indexed list of sites found to be different between both parental strains.  A . B .sitelist.vcf.gz[+.tbi]  - A vcf of sites found to be different between both parental strains.", 
            "title": "sitelist/"
        }, 
        {
            "location": "/pipeline-concordance/", 
            "text": "concordance-nf\n\n\nThe \nconcordance-nf\n pipeline...\n\n\n\n\n\n\nconcordance-nf\n\n\nUsage\n\n\nOverview\n\n\nUsage\n\n\nDebugging the pipeline locally\n\n\nDebugging the pipeline on Quest\n\n\nRunning the pipeline on Quest\n\n\n\n\n\n\n\n\n\n\nParameters\n\n\n--cores\n\n\n--out\n\n\n--sample_sheet\n\n\n--fq_prefix\n\n\n--reference\n\n\n--tmpdir\n\n\n--bamdir\n\n\n--email\n\n\n\n\n\n\nOutput\n\n\nConcordance/\n\n\nconcordance.png\n\n\nconcordance_above_99.png\n\n\nisotype_groups.tsv\n\n\ngtcheck\n\n\nisotype_count.txt\n\n\nHeterozygosity\n\n\nfq_concordance.tsv\n\n\nconcordance/pairwise/ (directory)\n\n\n\n\n\n\nDuplicates/\n\n\nbam_duplicates.tsv\n\n\n\n\n\n\nfq/\n\n\nstrain/\n\n\nvariation/\n\n\nsitelist.tsv.gz(+tbi)\n\n\nunion_vcf.txt\n\n\nmerged.raw.vcf.gz(+csi)\n\n\nconcordance.vcf.gz(+csi)\n\n\nconcordance.stats\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsage\n\n\n\n    \u250c\u2500\u2510\u250c\u2500\u2510\u250c\u2510\u250c\u250c\u2500\u2510\u250c\u2500\u2510\u252c\u2500\u2510\u250c\u252c\u2510\u250c\u2500\u2510\u250c\u2510\u250c\u250c\u2500\u2510\u250c\u2500\u2510  \u250c\u2510\u250c\u250c\u2500\u2510\n    \u2502  \u2502 \u2502\u2502\u2502\u2502\u2502  \u2502 \u2502\u251c\u252c\u2518 \u2502\u2502\u251c\u2500\u2524\u2502\u2502\u2502\u2502  \u251c\u2524\u2500\u2500\u2500\u2502\u2502\u2502\u251c\u2524 \n    \u2514\u2500\u2518\u2514\u2500\u2518\u2518\u2514\u2518\u2514\u2500\u2518\u2514\u2500\u2518\u2534\u2514\u2500\u2500\u2534\u2518\u2534 \u2534\u2518\u2514\u2518\u2514\u2500\u2518\u2514\u2500\u2518  \u2518\u2514\u2518\u2514  \n\n    parameters              description                    Set/Default\n    ==========              ===========                    =======\n\n    --debug                 Set to 'true' to test          false\n    --cores                 Regular job cores              4\n    --out                   Directory to output results    concordance-\ntodays date\n\n    --sample_sheet          fastq file (see help)          sample_sheet.tsv\n    --fq_prefix             fastq file (see help)          null\n    --reference             Reference Genome               WS245/WS245.fa.gz\n    --bamdir                Location for bams              bam\n    --tmpdir                A temporary directory          tmp/\n    --email                 Email to be sent results\n\n    HELP: http://andersenlab.org/dry-guide/pipeline-concordance/\n\n\n\n\n\nOverview\n\n\n\n\nThe concordance pipeline is used to detect sample swaps, identify samples with quality issues, and determine which wild isolate strains should be grouped together as an isotype. When performing sequencing, we often sequence the same DNA library or strain multiple times in order to attain adequate coverage. To ensure that samples are labeled properly we examine whether they contain discordant variant calls based on what strain they are labeled as. \n\n\nThe \nconcordance-nf\n pipeline will proceed to group FASTQs labeled as a strain regardless of the quality of the data. Therefore, if issues are suspected the problemetic data needs to be removed and the pipeline rerun. More details are available below.\n\n\nTo determine which strains belong to the same isotype we use two criteria. First we look at the strains that group together with a concordance threshold of 99.9%. Generally this will group most isotypes without issue. However, it is possible that you will run into cases where the grouping is not clean. For example, strain A groups with B, B groups with C, but C does not group with A. In these cases you must examine the data closely to identify why strains are incompletely grouping. Our second criteria we use to group isotypes may address these types of groupings.\n\n\nThe second criteria that we use to group isotypes regards looking for regional differences among strains. If two strains are similar but possess a region of their genome (binned at 1 Mb) that differs by more than 2% then we will separate them out into their own isotypes.\n\n\nThe process of grouping isotypes is very hand-on. This pipeline will help process the data but you must carefully review the output and investigate closely.\n\n\n\n\nNote\n\n\nThere is at least one exception to the 99.9% cutoff rule we use to determine isotypes.\n\n\nWe classify LSJ1 and N2 as separate isotypes despite the fact that they are greater than 99.9% identical. The strains are known to have diverged from one another, but work has demonstrated significant\ngenetic and phenotypic differences.\n\n\n\n\nUsage\n\n\nThe \nnextflow.config\n file sets most of the defaults you need to get the pipeline running locally for debugging purposes or on Quest.\n\n\nDebugging the pipeline locally\n\n\nThe pipeline comes with a test dataset that you can use to make changes or fix problems. When running locally, you should install the \nandersen-lab-env\n which will install all the required dependencies.\n\n\nFetching the reference\n\n\nYou will need a reference genome to align to. You can fetch one by running the following command:\n\n\ncurl https://storage.googleapis.com/elegansvariation.org/genome/WS245/WS245.tar.gz \n WS245.tar.gz\ntar -xvzf WS245.tar.gz\n\n\n\n\nRun the pipeline locally by setting \n-profile debug\n:\n\n\nnextflow run main.nf -profile debug -resume\n\n\n\n\nDebugging the pipeline on Quest\n\n\nWhen running on Quest, you should first run the quest debug profile. The Quest debug profile will use the test dataset and sample sheet which runs much faster and will encounter errors much sooner should they need to be fixed. If the debug dataset runs to completion it is likely that the full dataset will as well.\n\n\nnextflow run main.nf -profile quest_debug -resume --email \nyour email\n\n\n\n\n\nRunning the pipeline on Quest\n\n\nThe pipeline can be run on Quest using the following command:\n\n\nnextflow run main.nf -profile quest -resume --email \nyour email\n\n\n\n\n\nParameters\n\n\nThe nextflow profiles configured in \nnextflow.config\n are designed to make it so that you don't need to change the parameters. However, the pipeline offers this flexibility if it is ever called for.\n\n\n--cores\n\n\nThe number of cores to use during alignments and variant calling.\n\n\n--out\n\n\nA directory in which to output results. By default it will be \nconcordance-YYYY-MM-DD\n where YYYY-MM-DD is todays date.\n\n\n--sample_sheet\n\n\nThe sample sheet to use. Normally you can use the \nsample_sheet.tsv\n located in the base of the repo. This sample sheet is constructed usign \nscripts/construct_sample_sheet.sh\n.\n\n\nWhen running with \n-debug\n the sample sheet located in \ntest_data/sample_sheet.tsv\n will be used.\n\n\nMore information on the sample sheet and adding new sequence data on the \nSample sheet page\n section.\n\n\n--fq_prefix\n\n\nA prefix path for FASTQs defined in the sample sheet. The sample sheet designed for usage on Quest (\nsample_sheet.tsv\n) uses absolute paths so no FASTQ prefix is necessary. It is set to \nnull\n by default.\n\n\n--reference\n\n\nA fasta reference indexed with BWA. On Quest, the reference is available here:\n\n\n/projects/b1059/data/genomes/c_elegans/WS245/WS245.fa.gz\n\n\n\n\n--tmpdir\n\n\nA directory for storing temporary data.\n\n\n--bamdir\n\n\nA directory to output strain-level BAM files to. On Quest this is set by default to\n\n\n/projects/b1059/data/alignments/WI/strain/\n\n\n--email\n\n\nSet an email to get notified when the pipeline succeeds or fails.\n\n\nOutput\n\n\nConcordance/\n\n\nconcordance.png\n\n\n\n\nAn image showing the distribution of pairwise concordances across all strains. The cutoff is at 99.9% above which pairs are considered to be in the same isotype unless issues arise.\n\n\nconcordance_above_99.png\n\n\n\n\nA close up view of the concordances showing more detail. \n\n\nisotype_groups.tsv\n\n\nThis is the most important output file\n. It illustrates the isotypes identified for each strain and identifies potential issues.\n\n\nA file with the following structure:\n\n\n\n\n\n\n\n\ngroup\n\n\nstrain\n\n\nisotype\n\n\nlatitude\n\n\nlongitude\n\n\ncoverage\n\n\nunique_isotypes_per_group\n\n\nunique_groups_per_isotype\n\n\nstrain_in_multiple_isotypes\n\n\nlocation_issue\n\n\nstrain_conflict\n\n\n\n\n\n\n\n\n\n\n1\n\n\nAB1\n\n\nAB1\n\n\n-34.93\n\n\n138.59\n\n\n69.4687\n\n\n1\n\n\n1\n\n\nFALSE\n\n\nFALSE\n\n\nFALSE\n\n\n\n\n\n\n112\n\n\nAB4\n\n\nCB4858\n\n\n-34.93\n\n\n138.59\n\n\n158.358\n\n\n1\n\n\n1\n\n\nFALSE\n\n\nTRUE\n\n\nTRUE\n\n\n\n\n\n\n112\n\n\nECA251\n\n\nCB4858\n\n\n34.1\n\n\n-118.1\n\n\n73.5843\n\n\n1\n\n\n1\n\n\nFALSE\n\n\nTRUE\n\n\nTRUE\n\n\n\n\n\n\n112\n\n\nJU1960\n\n\nCB4858\n\n\n34.1897\n\n\n-118.131\n\n\n55.0373\n\n\n1\n\n\n1\n\n\nFALSE\n\n\nTRUE\n\n\nTRUE\n\n\n\n\n\n\n175\n\n\nBRC20067\n\n\nBRC20067\n\n\n24.073\n\n\n121.17\n\n\n33.5934\n\n\n1\n\n\n1\n\n\nFALSE\n\n\nFALSE\n\n\nFALSE\n\n\n\n\n\n\n175\n\n\nBRC20113\n\n\nBRC20067\n\n\n24.1242\n\n\n121.283\n\n\n38.9916\n\n\n1\n\n\n1\n\n\nFALSE\n\n\nFALSE\n\n\nFALSE\n\n\n\n\n\n\n186\n\n\nBRC20231\n\n\nMY23\n\n\n23.5415\n\n\n120.908\n\n\n44.1452\n\n\n1\n\n\n1\n\n\nFALSE\n\n\nTRUE\n\n\nTRUE\n\n\n\n\n\n\n186\n\n\nMY23\n\n\nMY23\n\n\n51.96\n\n\n7.53\n\n\n132.185\n\n\n1\n\n\n1\n\n\nFALSE\n\n\nTRUE\n\n\nTRUE\n\n\n\n\n\n\n\n\n\n\ngroup\n - A number used to group strains (in each row) into an isotype automatically. This number should be unique with the isotype column (e.g. 1--\n AB1, 112 --\n CB4858, BRC20067 --\n 175). The number can change between analyses.\n\n\nstrain\n - the strain\n\n\nisotype\n - the currently assigned isotype for a strain taken from the \nWI Strain Info\n spreadsheet. When new strains are added this is blank. \n\n\nlatitude\n\n\nlongitude\n\n\ncoverage\n - Depth of coverage for strain.\n\n\nunique_isotypes_per_group\n - Number of unique isotypes when grouping by the group column. This should be 1. If it is more than 1, it indicates that multiple isotypes were assigned to a grouping and that a previously assigned isotype is now being called into question.\n\n\nunique_groups_per_isotype\n Number of unique groups assigned to an isotype. This should be 1. If it is higher than 1, it indicates that a strain is concordant with strains in two different isotypes (including blanks). If it is equal to 2 and contains blanks in the isotype column it likely means that an isotype should be assigned to that strain.\n\n\nstrain_in_multiple_isotypes\n - Indicates that a strain is falling into multiple isotypes (a problem!).\n\n\nlocation_issue\n - Indicates a location issue. This occurs when strains fall into an isotype but are located far away from one another. Some are known issues and can be ignored.\n\n\nstrain_conflict\n - \nTRUE\n if any issue is present that should be investigated.\n\n\n\n\ngtcheck\n\n\nFile produced using \nbcftools gtcheck\n; Raw genotype differences between strains.\n\n\nisotype_count.txt\n\n\nGives a count of the number of isotypes identified.\n\n\nHeterozygosity\n\n\nNumber of heterozygous sites/strain. Can be an indicator of mixed samples or other issues.\n\n\nfq_concordance.tsv\n\n\nIntra-strain FASTQ-pair concordances. The format is:\n\n\nconcordance/pairwise/ (directory)\n\n\nContains images showing locations where regional discordance occurs among strains classified as being the isotype. You must look through all these images to ensure there are no strains being grouped that have regions with significant differences (\n 2%). The image below illustrates an example of this. ED3049 and ED3046 are highly similar (\n 99.9%). However, they differ in a region on the right arm of chromosome II. We believe this was enough reason to consider them separate isotypes.\n\n\n\n\nDuplicates/\n\n\nbam_duplicates.tsv\n\n\nSummary of duplicate reads (determined by picard).\n\n\nfq/\n\n\nThe following files are output in the fastq directory\n\n\nfq_bam_idxstats.tsv   # Stats generated with `samtools idxstats`\nfq_bam_stats.tsv      # Stats generated with `samtools stats`\nfq_coverage.full.tsv  # Detailed coverage numbers\nfq_coverage.tsv       # Summary coverage of individual fastqs.\n\n\n\n\nstrain/\n\n\nThe following files are output in the strain directory\n\n\nstrain_bam_idxstats.tsv   # Stats generated with `samtools idxstats`\nstrain_bam_stats.tsv      # Stats generated with `samtools stats`\nstrain_coverage.full.tsv  # Detailed coverage numbers\nstrain_coverage.tsv       # Summary coverage for strains.\n\n\n\n\nvariation/\n\n\nsitelist.tsv.gz(+tbi)\n\n\nThe union-variant sitelist from all strains.\n\n\nunion_vcf.txt\n\n\nLocations of the union VCFs.\n\n\nmerged.raw.vcf.gz(+csi)\n\n\nThe raw VCF. The following filters are applied before calculating concordance:\n\n\n    min_depth\n3        # Minimum Depth\n    qual\n30            # Quality (VCF=QUAL)\n    mq\n40              # Mapping Quality\n    dv_dp=0.5          # DV/DP \n 0.5 (high-quality allelic ALT bases over total depth)\n    max_missing\n0.05   # Max number of missing sites cannot exceed 5%\n\n\n\n\nconcordance.vcf.gz(+csi)\n\n\nFiltered VCF, filtered for true SNPs (no monomorphic sites)\n\n\nconcordance.stats\n\n\nStats from concordance vcf. Contains the number of SNPs.", 
            "title": "Concordance"
        }, 
        {
            "location": "/pipeline-concordance/#concordance-nf", 
            "text": "The  concordance-nf  pipeline...    concordance-nf  Usage  Overview  Usage  Debugging the pipeline locally  Debugging the pipeline on Quest  Running the pipeline on Quest      Parameters  --cores  --out  --sample_sheet  --fq_prefix  --reference  --tmpdir  --bamdir  --email    Output  Concordance/  concordance.png  concordance_above_99.png  isotype_groups.tsv  gtcheck  isotype_count.txt  Heterozygosity  fq_concordance.tsv  concordance/pairwise/ (directory)    Duplicates/  bam_duplicates.tsv    fq/  strain/  variation/  sitelist.tsv.gz(+tbi)  union_vcf.txt  merged.raw.vcf.gz(+csi)  concordance.vcf.gz(+csi)  concordance.stats", 
            "title": "concordance-nf"
        }, 
        {
            "location": "/pipeline-concordance/#usage", 
            "text": "\u250c\u2500\u2510\u250c\u2500\u2510\u250c\u2510\u250c\u250c\u2500\u2510\u250c\u2500\u2510\u252c\u2500\u2510\u250c\u252c\u2510\u250c\u2500\u2510\u250c\u2510\u250c\u250c\u2500\u2510\u250c\u2500\u2510  \u250c\u2510\u250c\u250c\u2500\u2510\n    \u2502  \u2502 \u2502\u2502\u2502\u2502\u2502  \u2502 \u2502\u251c\u252c\u2518 \u2502\u2502\u251c\u2500\u2524\u2502\u2502\u2502\u2502  \u251c\u2524\u2500\u2500\u2500\u2502\u2502\u2502\u251c\u2524 \n    \u2514\u2500\u2518\u2514\u2500\u2518\u2518\u2514\u2518\u2514\u2500\u2518\u2514\u2500\u2518\u2534\u2514\u2500\u2500\u2534\u2518\u2534 \u2534\u2518\u2514\u2518\u2514\u2500\u2518\u2514\u2500\u2518  \u2518\u2514\u2518\u2514  \n\n    parameters              description                    Set/Default\n    ==========              ===========                    =======\n\n    --debug                 Set to 'true' to test          false\n    --cores                 Regular job cores              4\n    --out                   Directory to output results    concordance- todays date \n    --sample_sheet          fastq file (see help)          sample_sheet.tsv\n    --fq_prefix             fastq file (see help)          null\n    --reference             Reference Genome               WS245/WS245.fa.gz\n    --bamdir                Location for bams              bam\n    --tmpdir                A temporary directory          tmp/\n    --email                 Email to be sent results\n\n    HELP: http://andersenlab.org/dry-guide/pipeline-concordance/", 
            "title": "Usage"
        }, 
        {
            "location": "/pipeline-concordance/#overview", 
            "text": "The concordance pipeline is used to detect sample swaps, identify samples with quality issues, and determine which wild isolate strains should be grouped together as an isotype. When performing sequencing, we often sequence the same DNA library or strain multiple times in order to attain adequate coverage. To ensure that samples are labeled properly we examine whether they contain discordant variant calls based on what strain they are labeled as.   The  concordance-nf  pipeline will proceed to group FASTQs labeled as a strain regardless of the quality of the data. Therefore, if issues are suspected the problemetic data needs to be removed and the pipeline rerun. More details are available below.  To determine which strains belong to the same isotype we use two criteria. First we look at the strains that group together with a concordance threshold of 99.9%. Generally this will group most isotypes without issue. However, it is possible that you will run into cases where the grouping is not clean. For example, strain A groups with B, B groups with C, but C does not group with A. In these cases you must examine the data closely to identify why strains are incompletely grouping. Our second criteria we use to group isotypes may address these types of groupings.  The second criteria that we use to group isotypes regards looking for regional differences among strains. If two strains are similar but possess a region of their genome (binned at 1 Mb) that differs by more than 2% then we will separate them out into their own isotypes.  The process of grouping isotypes is very hand-on. This pipeline will help process the data but you must carefully review the output and investigate closely.   Note  There is at least one exception to the 99.9% cutoff rule we use to determine isotypes.  We classify LSJ1 and N2 as separate isotypes despite the fact that they are greater than 99.9% identical. The strains are known to have diverged from one another, but work has demonstrated significant\ngenetic and phenotypic differences.", 
            "title": "Overview"
        }, 
        {
            "location": "/pipeline-concordance/#usage_1", 
            "text": "The  nextflow.config  file sets most of the defaults you need to get the pipeline running locally for debugging purposes or on Quest.", 
            "title": "Usage"
        }, 
        {
            "location": "/pipeline-concordance/#debugging_the_pipeline_locally", 
            "text": "The pipeline comes with a test dataset that you can use to make changes or fix problems. When running locally, you should install the  andersen-lab-env  which will install all the required dependencies.  Fetching the reference  You will need a reference genome to align to. You can fetch one by running the following command:  curl https://storage.googleapis.com/elegansvariation.org/genome/WS245/WS245.tar.gz   WS245.tar.gz\ntar -xvzf WS245.tar.gz  Run the pipeline locally by setting  -profile debug :  nextflow run main.nf -profile debug -resume", 
            "title": "Debugging the pipeline locally"
        }, 
        {
            "location": "/pipeline-concordance/#debugging_the_pipeline_on_quest", 
            "text": "When running on Quest, you should first run the quest debug profile. The Quest debug profile will use the test dataset and sample sheet which runs much faster and will encounter errors much sooner should they need to be fixed. If the debug dataset runs to completion it is likely that the full dataset will as well.  nextflow run main.nf -profile quest_debug -resume --email  your email", 
            "title": "Debugging the pipeline on Quest"
        }, 
        {
            "location": "/pipeline-concordance/#running_the_pipeline_on_quest", 
            "text": "The pipeline can be run on Quest using the following command:  nextflow run main.nf -profile quest -resume --email  your email", 
            "title": "Running the pipeline on Quest"
        }, 
        {
            "location": "/pipeline-concordance/#parameters", 
            "text": "The nextflow profiles configured in  nextflow.config  are designed to make it so that you don't need to change the parameters. However, the pipeline offers this flexibility if it is ever called for.", 
            "title": "Parameters"
        }, 
        {
            "location": "/pipeline-concordance/#--cores", 
            "text": "The number of cores to use during alignments and variant calling.", 
            "title": "--cores"
        }, 
        {
            "location": "/pipeline-concordance/#--out", 
            "text": "A directory in which to output results. By default it will be  concordance-YYYY-MM-DD  where YYYY-MM-DD is todays date.", 
            "title": "--out"
        }, 
        {
            "location": "/pipeline-concordance/#--sample_sheet", 
            "text": "The sample sheet to use. Normally you can use the  sample_sheet.tsv  located in the base of the repo. This sample sheet is constructed usign  scripts/construct_sample_sheet.sh .  When running with  -debug  the sample sheet located in  test_data/sample_sheet.tsv  will be used.  More information on the sample sheet and adding new sequence data on the  Sample sheet page  section.", 
            "title": "--sample_sheet"
        }, 
        {
            "location": "/pipeline-concordance/#--fq_prefix", 
            "text": "A prefix path for FASTQs defined in the sample sheet. The sample sheet designed for usage on Quest ( sample_sheet.tsv ) uses absolute paths so no FASTQ prefix is necessary. It is set to  null  by default.", 
            "title": "--fq_prefix"
        }, 
        {
            "location": "/pipeline-concordance/#--reference", 
            "text": "A fasta reference indexed with BWA. On Quest, the reference is available here:  /projects/b1059/data/genomes/c_elegans/WS245/WS245.fa.gz", 
            "title": "--reference"
        }, 
        {
            "location": "/pipeline-concordance/#--tmpdir", 
            "text": "A directory for storing temporary data.", 
            "title": "--tmpdir"
        }, 
        {
            "location": "/pipeline-concordance/#--bamdir", 
            "text": "A directory to output strain-level BAM files to. On Quest this is set by default to  /projects/b1059/data/alignments/WI/strain/", 
            "title": "--bamdir"
        }, 
        {
            "location": "/pipeline-concordance/#--email", 
            "text": "Set an email to get notified when the pipeline succeeds or fails.", 
            "title": "--email"
        }, 
        {
            "location": "/pipeline-concordance/#output", 
            "text": "", 
            "title": "Output"
        }, 
        {
            "location": "/pipeline-concordance/#concordance", 
            "text": "", 
            "title": "Concordance/"
        }, 
        {
            "location": "/pipeline-concordance/#concordancepng", 
            "text": "An image showing the distribution of pairwise concordances across all strains. The cutoff is at 99.9% above which pairs are considered to be in the same isotype unless issues arise.", 
            "title": "concordance.png"
        }, 
        {
            "location": "/pipeline-concordance/#concordance_above_99png", 
            "text": "A close up view of the concordances showing more detail.", 
            "title": "concordance_above_99.png"
        }, 
        {
            "location": "/pipeline-concordance/#isotype_groupstsv", 
            "text": "This is the most important output file . It illustrates the isotypes identified for each strain and identifies potential issues.  A file with the following structure:     group  strain  isotype  latitude  longitude  coverage  unique_isotypes_per_group  unique_groups_per_isotype  strain_in_multiple_isotypes  location_issue  strain_conflict      1  AB1  AB1  -34.93  138.59  69.4687  1  1  FALSE  FALSE  FALSE    112  AB4  CB4858  -34.93  138.59  158.358  1  1  FALSE  TRUE  TRUE    112  ECA251  CB4858  34.1  -118.1  73.5843  1  1  FALSE  TRUE  TRUE    112  JU1960  CB4858  34.1897  -118.131  55.0373  1  1  FALSE  TRUE  TRUE    175  BRC20067  BRC20067  24.073  121.17  33.5934  1  1  FALSE  FALSE  FALSE    175  BRC20113  BRC20067  24.1242  121.283  38.9916  1  1  FALSE  FALSE  FALSE    186  BRC20231  MY23  23.5415  120.908  44.1452  1  1  FALSE  TRUE  TRUE    186  MY23  MY23  51.96  7.53  132.185  1  1  FALSE  TRUE  TRUE      group  - A number used to group strains (in each row) into an isotype automatically. This number should be unique with the isotype column (e.g. 1--  AB1, 112 --  CB4858, BRC20067 --  175). The number can change between analyses.  strain  - the strain  isotype  - the currently assigned isotype for a strain taken from the  WI Strain Info  spreadsheet. When new strains are added this is blank.   latitude  longitude  coverage  - Depth of coverage for strain.  unique_isotypes_per_group  - Number of unique isotypes when grouping by the group column. This should be 1. If it is more than 1, it indicates that multiple isotypes were assigned to a grouping and that a previously assigned isotype is now being called into question.  unique_groups_per_isotype  Number of unique groups assigned to an isotype. This should be 1. If it is higher than 1, it indicates that a strain is concordant with strains in two different isotypes (including blanks). If it is equal to 2 and contains blanks in the isotype column it likely means that an isotype should be assigned to that strain.  strain_in_multiple_isotypes  - Indicates that a strain is falling into multiple isotypes (a problem!).  location_issue  - Indicates a location issue. This occurs when strains fall into an isotype but are located far away from one another. Some are known issues and can be ignored.  strain_conflict  -  TRUE  if any issue is present that should be investigated.", 
            "title": "isotype_groups.tsv"
        }, 
        {
            "location": "/pipeline-concordance/#gtcheck", 
            "text": "File produced using  bcftools gtcheck ; Raw genotype differences between strains.", 
            "title": "gtcheck"
        }, 
        {
            "location": "/pipeline-concordance/#isotype_counttxt", 
            "text": "Gives a count of the number of isotypes identified.", 
            "title": "isotype_count.txt"
        }, 
        {
            "location": "/pipeline-concordance/#heterozygosity", 
            "text": "Number of heterozygous sites/strain. Can be an indicator of mixed samples or other issues.", 
            "title": "Heterozygosity"
        }, 
        {
            "location": "/pipeline-concordance/#fq_concordancetsv", 
            "text": "Intra-strain FASTQ-pair concordances. The format is:", 
            "title": "fq_concordance.tsv"
        }, 
        {
            "location": "/pipeline-concordance/#concordancepairwise_directory", 
            "text": "Contains images showing locations where regional discordance occurs among strains classified as being the isotype. You must look through all these images to ensure there are no strains being grouped that have regions with significant differences (  2%). The image below illustrates an example of this. ED3049 and ED3046 are highly similar (  99.9%). However, they differ in a region on the right arm of chromosome II. We believe this was enough reason to consider them separate isotypes.", 
            "title": "concordance/pairwise/ (directory)"
        }, 
        {
            "location": "/pipeline-concordance/#duplicates", 
            "text": "", 
            "title": "Duplicates/"
        }, 
        {
            "location": "/pipeline-concordance/#bam_duplicatestsv", 
            "text": "Summary of duplicate reads (determined by picard).", 
            "title": "bam_duplicates.tsv"
        }, 
        {
            "location": "/pipeline-concordance/#fq", 
            "text": "The following files are output in the fastq directory  fq_bam_idxstats.tsv   # Stats generated with `samtools idxstats`\nfq_bam_stats.tsv      # Stats generated with `samtools stats`\nfq_coverage.full.tsv  # Detailed coverage numbers\nfq_coverage.tsv       # Summary coverage of individual fastqs.", 
            "title": "fq/"
        }, 
        {
            "location": "/pipeline-concordance/#strain", 
            "text": "The following files are output in the strain directory  strain_bam_idxstats.tsv   # Stats generated with `samtools idxstats`\nstrain_bam_stats.tsv      # Stats generated with `samtools stats`\nstrain_coverage.full.tsv  # Detailed coverage numbers\nstrain_coverage.tsv       # Summary coverage for strains.", 
            "title": "strain/"
        }, 
        {
            "location": "/pipeline-concordance/#variation", 
            "text": "", 
            "title": "variation/"
        }, 
        {
            "location": "/pipeline-concordance/#sitelisttsvgztbi", 
            "text": "The union-variant sitelist from all strains.", 
            "title": "sitelist.tsv.gz(+tbi)"
        }, 
        {
            "location": "/pipeline-concordance/#union_vcftxt", 
            "text": "Locations of the union VCFs.", 
            "title": "union_vcf.txt"
        }, 
        {
            "location": "/pipeline-concordance/#mergedrawvcfgzcsi", 
            "text": "The raw VCF. The following filters are applied before calculating concordance:      min_depth 3        # Minimum Depth\n    qual 30            # Quality (VCF=QUAL)\n    mq 40              # Mapping Quality\n    dv_dp=0.5          # DV/DP   0.5 (high-quality allelic ALT bases over total depth)\n    max_missing 0.05   # Max number of missing sites cannot exceed 5%", 
            "title": "merged.raw.vcf.gz(+csi)"
        }, 
        {
            "location": "/pipeline-concordance/#concordancevcfgzcsi", 
            "text": "Filtered VCF, filtered for true SNPs (no monomorphic sites)", 
            "title": "concordance.vcf.gz(+csi)"
        }, 
        {
            "location": "/pipeline-concordance/#concordancestats", 
            "text": "Stats from concordance vcf. Contains the number of SNPs.", 
            "title": "concordance.stats"
        }, 
        {
            "location": "/pipeline-wi/", 
            "text": "wi-nf\n\n\nThe \nwi-nf\n pipeline aligns, calls variants, and performs analysis from wild isolate sequence data. The output of the \nwi-nf\n pipeline can be uploaded to google storage as a new release for the CeNDR website.\n\n\n\n\n\n\nwi-nf\n\n\nUsage\n\n\n\n\n\n\nPipeline Overview\n\n\nUsage\n\n\nDocker File\n\n\npyenv environments\n\n\nProfiles and Running the Pipeline\n\n\nRunning the pipeline locally\n\n\nDebugging the pipeline on Quest\n\n\nRunning the pipeline on Quest\n\n\n\n\n\n\n\n\n\n\nConfiguration\n\n\n--cores\n\n\n--out\n\n\n--fqs (FASTQs)\n\n\n--fqs_file_prefix\n\n\n--reference\n\n\n--tmpdir\n\n\n\n\n\n\nSample Sheet\n\n\nOutput\n\n\nlog.txt\n\n\nalignment/\n\n\ncegwas/\n\n\nisotype\n\n\nisotype/tsv/\n\n\nisotype/vcf/\n\n\nphenotype/\n\n\npopgen/\n\n\nreport/\n\n\ntrack/\n\n\nVariation/\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsage\n\n\n\n     \u2584         \u2584  \u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584                         \u2584\u2584        \u2584  \u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584 \n    \u2590\u2591\u258c       \u2590\u2591\u258c\u2590\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u258c                       \u2590\u2591\u2591\u258c      \u2590\u2591\u258c\u2590\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u258c\n    \u2590\u2591\u258c       \u2590\u2591\u258c \u2580\u2580\u2580\u2580\u2588\u2591\u2588\u2580\u2580\u2580\u2580                        \u2590\u2591\u258c\u2591\u258c     \u2590\u2591\u258c\u2590\u2591\u2588\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580 \n    \u2590\u2591\u258c       \u2590\u2591\u258c     \u2590\u2591\u258c                            \u2590\u2591\u258c\u2590\u2591\u258c    \u2590\u2591\u258c\u2590\u2591\u258c          \n    \u2590\u2591\u258c   \u2584   \u2590\u2591\u258c     \u2590\u2591\u258c           \u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584      \u2590\u2591\u258c \u2590\u2591\u258c   \u2590\u2591\u258c\u2590\u2591\u2588\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584 \n    \u2590\u2591\u258c  \u2590\u2591\u258c  \u2590\u2591\u258c     \u2590\u2591\u258c          \u2590\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u258c     \u2590\u2591\u258c  \u2590\u2591\u258c  \u2590\u2591\u258c\u2590\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u258c\n    \u2590\u2591\u258c \u2590\u2591\u258c\u2591\u258c \u2590\u2591\u258c     \u2590\u2591\u258c           \u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580      \u2590\u2591\u258c   \u2590\u2591\u258c \u2590\u2591\u258c\u2590\u2591\u2588\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580 \n    \u2590\u2591\u258c\u2590\u2591\u258c \u2590\u2591\u258c\u2590\u2591\u258c     \u2590\u2591\u258c                            \u2590\u2591\u258c    \u2590\u2591\u258c\u2590\u2591\u258c\u2590\u2591\u258c          \n    \u2590\u2591\u258c\u2591\u258c   \u2590\u2591\u2590\u2591\u258c \u2584\u2584\u2584\u2584\u2588\u2591\u2588\u2584\u2584\u2584\u2584                        \u2590\u2591\u258c     \u2590\u2591\u2590\u2591\u258c\u2590\u2591\u258c          \n    \u2590\u2591\u2591\u258c     \u2590\u2591\u2591\u258c\u2590\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u258c                       \u2590\u2591\u258c      \u2590\u2591\u2591\u258c\u2590\u2591\u258c          \n     \u2580\u2580       \u2580\u2580  \u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580                         \u2580        \u2580\u2580  \u2580           \n\n    parameters              description                    Set/Default\n    ==========              ===========                    =======\n\n    --debug                 Set to 'true' to test          ${params.debug}\n    --cores                 Regular job cores              ${params.cores}\n    --out                   Directory to output results    ${params.out}\n    --fqs                   fastq file (see help)          ${params.fqs}\n    --fq_file_prefix        fastq prefix                   ${params.fq_file_prefix}\n    --reference             Reference Genome               ${params.reference}\n    --annotation_reference  SnpEff annotation              ${params.annotation_reference}\n    --bamdir                Location for bams              ${params.bamdir}\n    --tmpdir                A temporary directory          ${params.tmpdir}\n    --email                 Email to be sent results       ${params.email}\n\n    HELP: http://andersenlab.org/dry-guide/pipeline-wi/\n\n\n\n\n\nPipeline Overview\n\n\n\n\nUsage\n\n\nDocker File\n\n\nandersenlab/wi-nf\n is the docker image used within the wi-nf pipeline. If Quest ever supports singularity, it can be converted to a singularity image and used with Nextflow.\n\n\npyenv environments\n\n\nThe pipeline uses two python environments due to clashing dependencies. The two environments are:\n\n\n\n\nvcf-kit\n - A python 2.7.14 environment with vcf-kit and cyvcf installed.\n\n\nmultiqc\n - A python 3.6.0 environment with multiqc installed.\n\n\n\n\nIf you are not using the docker container you must install these virtual environments by running the \nsetup_pyenv.sh\n script:\n\n\nbash setup_pyenv.sh\n\n\n\n\nIf you require one of these environments for a process within the pipeline, add the following as the first line of your script:\n\n\nsource init_pyenv.sh \n pyenv activate \nenvironment name\n\n\n\n\n\nProfiles and Running the Pipeline\n\n\nThe \nnextflow.config\n file included with this pipeline contains three profiles. These set up the environment for testing local development, testing on Quest, and running the pipeline on Quest.\n\n\n\n\nlocal\n - Used for local development. Uses the docker container.\n\n\nquest_debug\n - Runs a small subset of available test data. Should complete within a couple of hours. For testing/diagnosing issues on Quest.\n\n\nquest\n - Runs the entire dataset.\n\n\n\n\nRunning the pipeline locally\n\n\nWhen running locally, the pipeline will run using the \nandersenlab/wi-nf\n docker image. You must have docker installed.\n\n\nnextflow run main.nf -profile local -resume\n\n\n\n\nDebugging the pipeline on Quest\n\n\nWhen running on Quest, you should first run the quest debug profile. The Quest debug profile will use a test dataset and sample sheet which runs much faster and will encounter errors much sooner should they need to be fixed. If the debug dataset runs to completion it is likely that the full dataset will as well.\n\n\nnextflow run main.nf -profile quest_debug -resume\n\n\n\n\nRunning the pipeline on Quest\n\n\nThe pipeline can be run on Quest using the following command:\n\n\nnextflow run main.nf -profile quest -resume\n\n\n\n\nConfiguration\n\n\nMost configuration is handled using the \n-profile\n flag and \nnextflow.config\n; If you want to fine tune things you can use the options below.\n\n\n--cores\n\n\nThe number of cores to use during alignments and variant calling.\n\n\n--out\n\n\nA directory in which to output results. By default it will be \nWI-YYYY-MM-DD\n where YYYY-MM-DD is todays date.\n\n\n--fqs (FASTQs)\n\n\nWhen running the \nwi-nf\n pipeline you must provide a sample sheet that tells it where fastqs are and which samples group into isotypes. By default, this is the sample sheet in the base of the wi-nf repo (\nSM_sample_sheet.tsv\n), but can be specified using \n--fqs\n if an alternative is required. The sample sheet provides information on the isotype, fastq_pairs, library, location of fastqs, and sequencing folder.\n\n\nMore information on the sample sheet and adding new sequence data in the \nSample sheet\n section.\n\n\n--fqs_file_prefix\n\n\nA prefix path for FASTQs defined in a sample sheet. The sample sheet designed for usage on Quest (\nSM_sample_sheet.tsv\n) uses absolute paths so no FASTQ prefix is necessary. Additionally, there is no need to set this option as it is set for you when using the \n-profile\n flag. This option is only necessary (maybe) with a custom dataset where you are not using absolute paths to reference FASTQs.\n\n\n--reference\n\n\nA fasta reference indexed with BWA. On Quest, the reference is available here:\n\n\n/projects/b1059/data/genomes/c_elegans/WS245/WS245.fa.gz\n\n\n\n\n--tmpdir\n\n\nA directory for storing temporary data.\n\n\nSample Sheet\n\n\nThe sample sheet defines which FASTQs belong to which isotypes, set FASTQ IDs, library, and more. The sample sheet is constructed using the \nscripts/construct_SM_sheet.sh\n script. When new sequence data is added, this needs to be modified to add information about the new FASTQs. Updates to this script should be committed to the repo.\n\n\n\n\nImportant\n\n\nYou have to assign isotypes using the concordance script before they can be processed using wi-nf.\n\n\n\n\nWhen you run the \nscripts/construct_SM_sheet.sh\n, it will output the \nSM_Sample_sheet.tsv\n file in the base of the repo. You should carefully examine the diff of this file using Git to ensure it is modifying the sample sheet correctly. \nErrors can be disasterous\n. Note that the output uses absolute paths to FASTQ files.\n\n\nSample sheet structure\n\n\nAB1 BGI1-RET2-AB1   RET2    /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI1-RET2-AB1-trim-1P.fq.gz /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI1-RET2-AB1-trim-2P.fq.gz original_wi_set\nAB1 BGI2-RET2-AB1   RET2    /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI2-RET2-AB1-trim-1P.fq.gz /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI2-RET2-AB1-trim-2P.fq.gz original_wi_set\nAB1 BGI3-RET2b-AB1  RET2b   /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI3-RET2b-AB1-trim-1P.fq.gz    /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI3-RET2b-AB1-trim-2P.fq.gz    original_wi_set\n\n\n\n\nNotice that the file does not include a header. The table with corresponding columns looks like this.\n\n\n\n\n\n\n\n\nisotype\n\n\nfastq_pair_id\n\n\nlibrary\n\n\nfastq-1-path\n\n\nfastq-2-path\n\n\nsequencing_folder\n\n\n\n\n\n\n\n\n\n\nAB1\n\n\nBGI1-RET2-AB1\n\n\nRET2\n\n\n/projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI1-RET2-AB1-trim-1P.fq.gz\n\n\n/projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI1-RET2-AB1-trim-2P.fq.gz\n\n\noriginal_wi_set\n\n\n\n\n\n\nAB1\n\n\nBGI2-RET2-AB1\n\n\nRET2\n\n\n/projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI2-RET2-AB1-trim-1P.fq.gz\n\n\n/projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI2-RET2-AB1-trim-2P.fq.gz\n\n\noriginal_wi_set\n\n\n\n\n\n\nAB1\n\n\nBGI3-RET2b-AB1\n\n\nRET2b\n\n\n/projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI3-RET2b-AB1-trim-1P.fq.gz\n\n\n/projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI3-RET2b-AB1-trim-2P.fq.gz\n\n\noriginal_wi_set\n\n\n\n\n\n\n\n\nThe columns are detailed below:\n\n\n\n\nisotype\n - The name of the isotype. It is used to group FASTQ-pairs into BAMs which are treated as individuals.\n\n\nfastq_pair_id\n - This must be unique identifier for all individual FASTQ pairs. There is (unfortunately) no standard for this.\n\n\nlibrary\n - A name identifying the DNA library. If the FASTQ-pairs for a strain were sequenced using different library preps they should be assigned different library names. Likewise, if they were the same DNA library they should have the same library name. Keep in mind that within an isotype the library names for each strain must be independent.\n\n\nfastq-1-path\n - The \nabsolute\n path of the first fastq.\n\n\nfastq-2-path\n - The \nabsolute\n path of the second fastq.\n\n\n\n\nOutput\n\n\nThe output from the pipeline is structured to enable it to easily be integrated with CeNDR. The final output directory looks like this:\n\n\n\u251c\u2500\u2500 log.txt\n\u251c\u2500\u2500 alignment\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 isotype_bam_idxstats.tsv\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 isotype_bam_stats.tsv\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 isotype_coverage.full.tsv\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 isotype_coverage.tsv\n\u251c\u2500\u2500 cegwas\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 kinship.Rda\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 snps.Rda\n\u251c\u2500\u2500 isotype\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 tsv\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 \nisotype\n.(date).tsv.gz\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 \nisotype\n.(date).tsv.gz\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 vcf\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 \nisotype\n.(date).vcf.gz\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 \nisotype\n.(date).vcf.gz.tbi\n\u251c\u2500\u2500 phenotype\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 MT_content.tsv\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 kmers.tsv\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 telseq.tsv\n\u251c\u2500\u2500 popgen\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 WI.(date).tajima.bed.gz\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 WI.(date).tajima.bed.gz.tbi\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 trees\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 I.pdf\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 I.png\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 I.tree\n\u2502       \u251c\u2500\u2500 ...\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 genome.pdf\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 genome.png\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 genome.tree\n\u251c\u2500\u2500 report\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 multiqc.html\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 multiqc_data\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 multiqc_bcftools_stats.json\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 multiqc_data.json\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 multiqc_fastqc.json\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 multiqc_general_stats.json\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 multiqc_picard_AlignmentSummaryMetrics.json\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 multiqc_picard_dups.json\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 multiqc_picard_insertSize.json\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 multiqc_samtools_idxstats.json\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 multiqc_samtools_stats.json\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 multiqc_snpeff.json\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 multiqc_sources.json\n\u251c\u2500\u2500 tracks\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 (date).HIGH.bed.gz\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 (date).HIGH.bed.gz.tbi\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 (date).LOW.bed.gz\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 (date).LOW.bed.gz.tbi\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 (date).MODERATE.bed.gz\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 (date).MODERATE.bed.gz.tbi\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 (date).MODIFIER.bed.gz\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 (date).MODIFIER.bed.gz.tbi\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 phastcons.bed.gz\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 phastcons.bed.gz.tbi\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 phylop.bed.gz\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 phylop.bed.gz.tbi\n\u2514\u2500\u2500 variation\n    \u251c\u2500\u2500 WI.(date).soft-filter.vcf.gz\n    \u251c\u2500\u2500 WI.(date).soft-filter.vcf.gz.csi\n    \u251c\u2500\u2500 WI.(date).soft-filter.vcf.gz.tbi\n    \u251c\u2500\u2500 WI.(date).soft-filter.stats.txt\n    \u251c\u2500\u2500 WI.(date).hard-filter.vcf.gz\n    \u251c\u2500\u2500 WI.(date).hard-filter.vcf.gz.csi\n    \u251c\u2500\u2500 WI.(date).hard-filter.vcf.gz.tbi\n    \u251c\u2500\u2500 WI.(date).hard-filter.stats.txt\n    \u251c\u2500\u2500 WI.(date).hard-filter.genotypes.tsv\n    \u251c\u2500\u2500 WI.(date).hard-filter.genotypes.frequency.tsv\n    \u251c\u2500\u2500 WI.(date).impute.vcf.gz\n    \u251c\u2500\u2500 WI.(date).impute.vcf.gz.csi\n    \u251c\u2500\u2500 WI.(date).impute.vcf.gz.tbi\n    \u251c\u2500\u2500 WI.(date).impute.stats.txt\n    \u251c\u2500\u2500 sitelist.tsv.gz\n    \u2514\u2500\u2500 sitelist.tsv.gz.tbi\n\n\n\n\nlog.txt\n\n\nA summary of the nextflow run.\n\n\nalignment/\n\n\nAlignment statistics by isotype\n\n\n\n\nisotype_bam_idxstats.tsv\n - A summary of mapped and unmapped reads by sample.\n\n\nisotype_bam_stats.tsv\n - BAM summary at the sample level\n\n\nisotype_coverage.full.tsv\n - Coverage at the sample level\n\n\nisotype_coverage.tsv\n - Simple coverage at the sample level.\n\n\n\n\ncegwas/\n\n\n\n\nkinship.Rda\n - A kinship matrix constructed from \nWI.(date).impute.vcf.gz\n\n\nsnps.Rda\n - A mapping snp set generated from \nWI.(date).hard-filter.vcf.gz\n\n\n\n\nisotype\n\n\nThis directory contains files that integrate with the genome browser on CeDNR.\n\n\nisotype/tsv/\n\n\nThis directory contains tsv's that are used to show where variants are in CeNDR.\n\n\nisotype/vcf/\n\n\nThis direcoty contains the VCFs of isotypes.\n\n\nphenotype/\n\n\n\n\nMT_content.tsv\n - Mitochondrial content (MtDNA cov / Nuclear cov).\n\n\nkmers.tsv\n - 6-mers for each isotype.\n\n\ntelseq.tsv\n - Telomere length for each isotype ~ split out by read group.\n\n\n\n\npopgen/\n\n\n\n\nWI.(date).tajima.bed.gz\n - Tajima's D bedfile for use on the report viewer of CeNDR.\n\n\nWI.(date).tajima.bed.gz.tbi\n - Tajima's D index.\n\n\ntrees/\n - Phylogenetic trees for each chromosome and the entire genome.\n\n\nI.(pdf|png|tree)\n\n\n...\n\n\ngenome.(pdf|png|tree)\n\n\n\n\n\n\n\n\nreport/\n\n\n\n\nmultiqc.html - A comprehensive report of the sequencing run.\n\n\nmultiqc_data/\n\n\nmultiqc_bcftools_stats.json\n\n\nmultiqc_data.json\n\n\nmultiqc_fastqc.json\n\n\nmultiqc_general_stats.json\n\n\nmultiqc_picard_AlignmentSummaryMetrics.json\n\n\nmultiqc_picard_dups.json\n\n\nmultiqc_picard_insertSize.json\n\n\nmultiqc_samtools_idxstats.json\n\n\nmultiqc_samtools_stats.json\n\n\nmultiqc_snpeff.json\n\n\nmultiqc_sources.json\n\n\n\n\n\n\n\n\ntrack/\n\n\n\n\n(date).LOW.bed.gz(+.tbi)\n - LOW effect mutations bed track and index.\n\n\n(date).MODERATE.bed.gz(+.tbi)\n - MODERATE effect mutations bed track and index.\n\n\n(date).HIGH.bed.gz(+.tbi)\n - HIGH effect mutations bed track and index.\n\n\n(date).MODIFIER.bed.gz(+.tbi)\n - MODERATE effect mutations bed track and index.\n\n\nphastcons.bed.gz(+.tbi)\n - Phastcons bed track and index.\n\n\nphylop.bed.gz(+.tbi)\n - PhyloP bed track and index\n\n\n\n\nVariation/\n\n\n\n\nWI.(date).soft-filter.vcf.gz(+.csi|+.tbi)\n -\n\n\nWI.(date).soft-filter.stats.txt\n -\n\n\nWI.(date).hard-filter.vcf.gz(+.csi|+.tbi)\n -\n\n\nWI.(date).hard-filter.stats.txt\n - \n\n\nWI.(date).hard-filter.genotypes.tsv\n - \n\n\nWI.(date).hard-filter.genotypes.frequency.tsv\n - \n\n\nWI.(date).impute.vcf.gz(+.csi|+.tbi)\n - \n\n\nWI.(date).impute.stats.txt\n - \n\n\nsitelist.tsv.gz(+.tbi)\n - Union of all sites identified in original SNV variant calling round.", 
            "title": "WI"
        }, 
        {
            "location": "/pipeline-wi/#wi-nf", 
            "text": "The  wi-nf  pipeline aligns, calls variants, and performs analysis from wild isolate sequence data. The output of the  wi-nf  pipeline can be uploaded to google storage as a new release for the CeNDR website.    wi-nf  Usage    Pipeline Overview  Usage  Docker File  pyenv environments  Profiles and Running the Pipeline  Running the pipeline locally  Debugging the pipeline on Quest  Running the pipeline on Quest      Configuration  --cores  --out  --fqs (FASTQs)  --fqs_file_prefix  --reference  --tmpdir    Sample Sheet  Output  log.txt  alignment/  cegwas/  isotype  isotype/tsv/  isotype/vcf/  phenotype/  popgen/  report/  track/  Variation/", 
            "title": "wi-nf"
        }, 
        {
            "location": "/pipeline-wi/#usage", 
            "text": "\u2584         \u2584  \u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584                         \u2584\u2584        \u2584  \u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584 \n    \u2590\u2591\u258c       \u2590\u2591\u258c\u2590\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u258c                       \u2590\u2591\u2591\u258c      \u2590\u2591\u258c\u2590\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u258c\n    \u2590\u2591\u258c       \u2590\u2591\u258c \u2580\u2580\u2580\u2580\u2588\u2591\u2588\u2580\u2580\u2580\u2580                        \u2590\u2591\u258c\u2591\u258c     \u2590\u2591\u258c\u2590\u2591\u2588\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580 \n    \u2590\u2591\u258c       \u2590\u2591\u258c     \u2590\u2591\u258c                            \u2590\u2591\u258c\u2590\u2591\u258c    \u2590\u2591\u258c\u2590\u2591\u258c          \n    \u2590\u2591\u258c   \u2584   \u2590\u2591\u258c     \u2590\u2591\u258c           \u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584      \u2590\u2591\u258c \u2590\u2591\u258c   \u2590\u2591\u258c\u2590\u2591\u2588\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584 \n    \u2590\u2591\u258c  \u2590\u2591\u258c  \u2590\u2591\u258c     \u2590\u2591\u258c          \u2590\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u258c     \u2590\u2591\u258c  \u2590\u2591\u258c  \u2590\u2591\u258c\u2590\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u258c\n    \u2590\u2591\u258c \u2590\u2591\u258c\u2591\u258c \u2590\u2591\u258c     \u2590\u2591\u258c           \u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580      \u2590\u2591\u258c   \u2590\u2591\u258c \u2590\u2591\u258c\u2590\u2591\u2588\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580 \n    \u2590\u2591\u258c\u2590\u2591\u258c \u2590\u2591\u258c\u2590\u2591\u258c     \u2590\u2591\u258c                            \u2590\u2591\u258c    \u2590\u2591\u258c\u2590\u2591\u258c\u2590\u2591\u258c          \n    \u2590\u2591\u258c\u2591\u258c   \u2590\u2591\u2590\u2591\u258c \u2584\u2584\u2584\u2584\u2588\u2591\u2588\u2584\u2584\u2584\u2584                        \u2590\u2591\u258c     \u2590\u2591\u2590\u2591\u258c\u2590\u2591\u258c          \n    \u2590\u2591\u2591\u258c     \u2590\u2591\u2591\u258c\u2590\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u258c                       \u2590\u2591\u258c      \u2590\u2591\u2591\u258c\u2590\u2591\u258c          \n     \u2580\u2580       \u2580\u2580  \u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580                         \u2580        \u2580\u2580  \u2580           \n\n    parameters              description                    Set/Default\n    ==========              ===========                    =======\n\n    --debug                 Set to 'true' to test          ${params.debug}\n    --cores                 Regular job cores              ${params.cores}\n    --out                   Directory to output results    ${params.out}\n    --fqs                   fastq file (see help)          ${params.fqs}\n    --fq_file_prefix        fastq prefix                   ${params.fq_file_prefix}\n    --reference             Reference Genome               ${params.reference}\n    --annotation_reference  SnpEff annotation              ${params.annotation_reference}\n    --bamdir                Location for bams              ${params.bamdir}\n    --tmpdir                A temporary directory          ${params.tmpdir}\n    --email                 Email to be sent results       ${params.email}\n\n    HELP: http://andersenlab.org/dry-guide/pipeline-wi/", 
            "title": "Usage"
        }, 
        {
            "location": "/pipeline-wi/#pipeline_overview", 
            "text": "", 
            "title": "Pipeline Overview"
        }, 
        {
            "location": "/pipeline-wi/#usage_1", 
            "text": "", 
            "title": "Usage"
        }, 
        {
            "location": "/pipeline-wi/#docker_file", 
            "text": "andersenlab/wi-nf  is the docker image used within the wi-nf pipeline. If Quest ever supports singularity, it can be converted to a singularity image and used with Nextflow.", 
            "title": "Docker File"
        }, 
        {
            "location": "/pipeline-wi/#pyenv_environments", 
            "text": "The pipeline uses two python environments due to clashing dependencies. The two environments are:   vcf-kit  - A python 2.7.14 environment with vcf-kit and cyvcf installed.  multiqc  - A python 3.6.0 environment with multiqc installed.   If you are not using the docker container you must install these virtual environments by running the  setup_pyenv.sh  script:  bash setup_pyenv.sh  If you require one of these environments for a process within the pipeline, add the following as the first line of your script:  source init_pyenv.sh   pyenv activate  environment name", 
            "title": "pyenv environments"
        }, 
        {
            "location": "/pipeline-wi/#profiles_and_running_the_pipeline", 
            "text": "The  nextflow.config  file included with this pipeline contains three profiles. These set up the environment for testing local development, testing on Quest, and running the pipeline on Quest.   local  - Used for local development. Uses the docker container.  quest_debug  - Runs a small subset of available test data. Should complete within a couple of hours. For testing/diagnosing issues on Quest.  quest  - Runs the entire dataset.", 
            "title": "Profiles and Running the Pipeline"
        }, 
        {
            "location": "/pipeline-wi/#running_the_pipeline_locally", 
            "text": "When running locally, the pipeline will run using the  andersenlab/wi-nf  docker image. You must have docker installed.  nextflow run main.nf -profile local -resume", 
            "title": "Running the pipeline locally"
        }, 
        {
            "location": "/pipeline-wi/#debugging_the_pipeline_on_quest", 
            "text": "When running on Quest, you should first run the quest debug profile. The Quest debug profile will use a test dataset and sample sheet which runs much faster and will encounter errors much sooner should they need to be fixed. If the debug dataset runs to completion it is likely that the full dataset will as well.  nextflow run main.nf -profile quest_debug -resume", 
            "title": "Debugging the pipeline on Quest"
        }, 
        {
            "location": "/pipeline-wi/#running_the_pipeline_on_quest", 
            "text": "The pipeline can be run on Quest using the following command:  nextflow run main.nf -profile quest -resume", 
            "title": "Running the pipeline on Quest"
        }, 
        {
            "location": "/pipeline-wi/#configuration", 
            "text": "Most configuration is handled using the  -profile  flag and  nextflow.config ; If you want to fine tune things you can use the options below.", 
            "title": "Configuration"
        }, 
        {
            "location": "/pipeline-wi/#--cores", 
            "text": "The number of cores to use during alignments and variant calling.", 
            "title": "--cores"
        }, 
        {
            "location": "/pipeline-wi/#--out", 
            "text": "A directory in which to output results. By default it will be  WI-YYYY-MM-DD  where YYYY-MM-DD is todays date.", 
            "title": "--out"
        }, 
        {
            "location": "/pipeline-wi/#--fqs_fastqs", 
            "text": "When running the  wi-nf  pipeline you must provide a sample sheet that tells it where fastqs are and which samples group into isotypes. By default, this is the sample sheet in the base of the wi-nf repo ( SM_sample_sheet.tsv ), but can be specified using  --fqs  if an alternative is required. The sample sheet provides information on the isotype, fastq_pairs, library, location of fastqs, and sequencing folder.  More information on the sample sheet and adding new sequence data in the  Sample sheet  section.", 
            "title": "--fqs (FASTQs)"
        }, 
        {
            "location": "/pipeline-wi/#--fqs_file_prefix", 
            "text": "A prefix path for FASTQs defined in a sample sheet. The sample sheet designed for usage on Quest ( SM_sample_sheet.tsv ) uses absolute paths so no FASTQ prefix is necessary. Additionally, there is no need to set this option as it is set for you when using the  -profile  flag. This option is only necessary (maybe) with a custom dataset where you are not using absolute paths to reference FASTQs.", 
            "title": "--fqs_file_prefix"
        }, 
        {
            "location": "/pipeline-wi/#--reference", 
            "text": "A fasta reference indexed with BWA. On Quest, the reference is available here:  /projects/b1059/data/genomes/c_elegans/WS245/WS245.fa.gz", 
            "title": "--reference"
        }, 
        {
            "location": "/pipeline-wi/#--tmpdir", 
            "text": "A directory for storing temporary data.", 
            "title": "--tmpdir"
        }, 
        {
            "location": "/pipeline-wi/#sample_sheet", 
            "text": "The sample sheet defines which FASTQs belong to which isotypes, set FASTQ IDs, library, and more. The sample sheet is constructed using the  scripts/construct_SM_sheet.sh  script. When new sequence data is added, this needs to be modified to add information about the new FASTQs. Updates to this script should be committed to the repo.   Important  You have to assign isotypes using the concordance script before they can be processed using wi-nf.   When you run the  scripts/construct_SM_sheet.sh , it will output the  SM_Sample_sheet.tsv  file in the base of the repo. You should carefully examine the diff of this file using Git to ensure it is modifying the sample sheet correctly.  Errors can be disasterous . Note that the output uses absolute paths to FASTQ files.  Sample sheet structure  AB1 BGI1-RET2-AB1   RET2    /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI1-RET2-AB1-trim-1P.fq.gz /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI1-RET2-AB1-trim-2P.fq.gz original_wi_set\nAB1 BGI2-RET2-AB1   RET2    /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI2-RET2-AB1-trim-1P.fq.gz /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI2-RET2-AB1-trim-2P.fq.gz original_wi_set\nAB1 BGI3-RET2b-AB1  RET2b   /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI3-RET2b-AB1-trim-1P.fq.gz    /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI3-RET2b-AB1-trim-2P.fq.gz    original_wi_set  Notice that the file does not include a header. The table with corresponding columns looks like this.     isotype  fastq_pair_id  library  fastq-1-path  fastq-2-path  sequencing_folder      AB1  BGI1-RET2-AB1  RET2  /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI1-RET2-AB1-trim-1P.fq.gz  /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI1-RET2-AB1-trim-2P.fq.gz  original_wi_set    AB1  BGI2-RET2-AB1  RET2  /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI2-RET2-AB1-trim-1P.fq.gz  /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI2-RET2-AB1-trim-2P.fq.gz  original_wi_set    AB1  BGI3-RET2b-AB1  RET2b  /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI3-RET2b-AB1-trim-1P.fq.gz  /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI3-RET2b-AB1-trim-2P.fq.gz  original_wi_set     The columns are detailed below:   isotype  - The name of the isotype. It is used to group FASTQ-pairs into BAMs which are treated as individuals.  fastq_pair_id  - This must be unique identifier for all individual FASTQ pairs. There is (unfortunately) no standard for this.  library  - A name identifying the DNA library. If the FASTQ-pairs for a strain were sequenced using different library preps they should be assigned different library names. Likewise, if they were the same DNA library they should have the same library name. Keep in mind that within an isotype the library names for each strain must be independent.  fastq-1-path  - The  absolute  path of the first fastq.  fastq-2-path  - The  absolute  path of the second fastq.", 
            "title": "Sample Sheet"
        }, 
        {
            "location": "/pipeline-wi/#output", 
            "text": "The output from the pipeline is structured to enable it to easily be integrated with CeNDR. The final output directory looks like this:  \u251c\u2500\u2500 log.txt\n\u251c\u2500\u2500 alignment\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 isotype_bam_idxstats.tsv\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 isotype_bam_stats.tsv\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 isotype_coverage.full.tsv\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 isotype_coverage.tsv\n\u251c\u2500\u2500 cegwas\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 kinship.Rda\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 snps.Rda\n\u251c\u2500\u2500 isotype\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 tsv\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500  isotype .(date).tsv.gz\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500  isotype .(date).tsv.gz\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 vcf\n\u2502\u00a0\u00a0     \u251c\u2500\u2500  isotype .(date).vcf.gz\n\u2502\u00a0\u00a0     \u2514\u2500\u2500  isotype .(date).vcf.gz.tbi\n\u251c\u2500\u2500 phenotype\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 MT_content.tsv\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 kmers.tsv\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 telseq.tsv\n\u251c\u2500\u2500 popgen\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 WI.(date).tajima.bed.gz\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 WI.(date).tajima.bed.gz.tbi\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 trees\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 I.pdf\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 I.png\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 I.tree\n\u2502       \u251c\u2500\u2500 ...\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 genome.pdf\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 genome.png\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 genome.tree\n\u251c\u2500\u2500 report\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 multiqc.html\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 multiqc_data\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 multiqc_bcftools_stats.json\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 multiqc_data.json\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 multiqc_fastqc.json\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 multiqc_general_stats.json\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 multiqc_picard_AlignmentSummaryMetrics.json\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 multiqc_picard_dups.json\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 multiqc_picard_insertSize.json\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 multiqc_samtools_idxstats.json\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 multiqc_samtools_stats.json\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 multiqc_snpeff.json\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 multiqc_sources.json\n\u251c\u2500\u2500 tracks\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 (date).HIGH.bed.gz\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 (date).HIGH.bed.gz.tbi\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 (date).LOW.bed.gz\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 (date).LOW.bed.gz.tbi\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 (date).MODERATE.bed.gz\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 (date).MODERATE.bed.gz.tbi\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 (date).MODIFIER.bed.gz\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 (date).MODIFIER.bed.gz.tbi\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 phastcons.bed.gz\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 phastcons.bed.gz.tbi\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 phylop.bed.gz\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 phylop.bed.gz.tbi\n\u2514\u2500\u2500 variation\n    \u251c\u2500\u2500 WI.(date).soft-filter.vcf.gz\n    \u251c\u2500\u2500 WI.(date).soft-filter.vcf.gz.csi\n    \u251c\u2500\u2500 WI.(date).soft-filter.vcf.gz.tbi\n    \u251c\u2500\u2500 WI.(date).soft-filter.stats.txt\n    \u251c\u2500\u2500 WI.(date).hard-filter.vcf.gz\n    \u251c\u2500\u2500 WI.(date).hard-filter.vcf.gz.csi\n    \u251c\u2500\u2500 WI.(date).hard-filter.vcf.gz.tbi\n    \u251c\u2500\u2500 WI.(date).hard-filter.stats.txt\n    \u251c\u2500\u2500 WI.(date).hard-filter.genotypes.tsv\n    \u251c\u2500\u2500 WI.(date).hard-filter.genotypes.frequency.tsv\n    \u251c\u2500\u2500 WI.(date).impute.vcf.gz\n    \u251c\u2500\u2500 WI.(date).impute.vcf.gz.csi\n    \u251c\u2500\u2500 WI.(date).impute.vcf.gz.tbi\n    \u251c\u2500\u2500 WI.(date).impute.stats.txt\n    \u251c\u2500\u2500 sitelist.tsv.gz\n    \u2514\u2500\u2500 sitelist.tsv.gz.tbi", 
            "title": "Output"
        }, 
        {
            "location": "/pipeline-wi/#logtxt", 
            "text": "A summary of the nextflow run.", 
            "title": "log.txt"
        }, 
        {
            "location": "/pipeline-wi/#alignment", 
            "text": "Alignment statistics by isotype   isotype_bam_idxstats.tsv  - A summary of mapped and unmapped reads by sample.  isotype_bam_stats.tsv  - BAM summary at the sample level  isotype_coverage.full.tsv  - Coverage at the sample level  isotype_coverage.tsv  - Simple coverage at the sample level.", 
            "title": "alignment/"
        }, 
        {
            "location": "/pipeline-wi/#cegwas", 
            "text": "kinship.Rda  - A kinship matrix constructed from  WI.(date).impute.vcf.gz  snps.Rda  - A mapping snp set generated from  WI.(date).hard-filter.vcf.gz", 
            "title": "cegwas/"
        }, 
        {
            "location": "/pipeline-wi/#isotype", 
            "text": "This directory contains files that integrate with the genome browser on CeDNR.", 
            "title": "isotype"
        }, 
        {
            "location": "/pipeline-wi/#isotypetsv", 
            "text": "This directory contains tsv's that are used to show where variants are in CeNDR.", 
            "title": "isotype/tsv/"
        }, 
        {
            "location": "/pipeline-wi/#isotypevcf", 
            "text": "This direcoty contains the VCFs of isotypes.", 
            "title": "isotype/vcf/"
        }, 
        {
            "location": "/pipeline-wi/#phenotype", 
            "text": "MT_content.tsv  - Mitochondrial content (MtDNA cov / Nuclear cov).  kmers.tsv  - 6-mers for each isotype.  telseq.tsv  - Telomere length for each isotype ~ split out by read group.", 
            "title": "phenotype/"
        }, 
        {
            "location": "/pipeline-wi/#popgen", 
            "text": "WI.(date).tajima.bed.gz  - Tajima's D bedfile for use on the report viewer of CeNDR.  WI.(date).tajima.bed.gz.tbi  - Tajima's D index.  trees/  - Phylogenetic trees for each chromosome and the entire genome.  I.(pdf|png|tree)  ...  genome.(pdf|png|tree)", 
            "title": "popgen/"
        }, 
        {
            "location": "/pipeline-wi/#report", 
            "text": "multiqc.html - A comprehensive report of the sequencing run.  multiqc_data/  multiqc_bcftools_stats.json  multiqc_data.json  multiqc_fastqc.json  multiqc_general_stats.json  multiqc_picard_AlignmentSummaryMetrics.json  multiqc_picard_dups.json  multiqc_picard_insertSize.json  multiqc_samtools_idxstats.json  multiqc_samtools_stats.json  multiqc_snpeff.json  multiqc_sources.json", 
            "title": "report/"
        }, 
        {
            "location": "/pipeline-wi/#track", 
            "text": "(date).LOW.bed.gz(+.tbi)  - LOW effect mutations bed track and index.  (date).MODERATE.bed.gz(+.tbi)  - MODERATE effect mutations bed track and index.  (date).HIGH.bed.gz(+.tbi)  - HIGH effect mutations bed track and index.  (date).MODIFIER.bed.gz(+.tbi)  - MODERATE effect mutations bed track and index.  phastcons.bed.gz(+.tbi)  - Phastcons bed track and index.  phylop.bed.gz(+.tbi)  - PhyloP bed track and index", 
            "title": "track/"
        }, 
        {
            "location": "/pipeline-wi/#variation", 
            "text": "WI.(date).soft-filter.vcf.gz(+.csi|+.tbi)  -  WI.(date).soft-filter.stats.txt  -  WI.(date).hard-filter.vcf.gz(+.csi|+.tbi)  -  WI.(date).hard-filter.stats.txt  -   WI.(date).hard-filter.genotypes.tsv  -   WI.(date).hard-filter.genotypes.frequency.tsv  -   WI.(date).impute.vcf.gz(+.csi|+.tbi)  -   WI.(date).impute.stats.txt  -   sitelist.tsv.gz(+.tbi)  - Union of all sites identified in original SNV variant calling round.", 
            "title": "Variation/"
        }, 
        {
            "location": "/pipeline-cegwas/", 
            "text": "cegwas-nf\n\n\nDocker image\n\n\nThe wild-isolate docker file can be used. The \ncegwas-nf\n pipeline requires R/cegwas to be installed and a python module, awesome-slugify.\n\n\nandersenlab/wi-nf\n\n\nRequirements\n\n\nThe cegwas-nf pipeline requires \nawesome-slugify\n; Install using:\n\n\npip install awesome-slugify\n\n\n\n\nIt also requires that cegwas be installed. See the \ncegwas\n repo for more information. Alternatively, you can use the wi-nf docker image.\n\n\nUsage\n\n\n# cd to directory containing a trait file.\nnextflow run Andersenlab/cegwas-nf --in=\ninput file\n\n\n\n\n\nInput Format\n\n\nSave trait data as a \n.tsv\n; Strains in column 1. Traits are in columns 2 and above.\n\n\n\n\n\n\n\n\nstrain\n\n\ntrait1\n\n\ntrait2\n\n\ntrait3\n\n\ntrait4\n\n\n\n\n\n\n\n\n\n\nAB1\n\n\n0\n\n\n0\n\n\n19.6825\n\n\n16.5026\n\n\n\n\n\n\nAB4\n\n\n14.5294\n\n\n13.9775\n\n\n18.9721\n\n\n20.6803\n\n\n\n\n\n\nBRC20067\n\n\n18.0132\n\n\n17.1509\n\n\n18.4466\n\n\n21.0243\n\n\n\n\n\n\nCB4855_UK\n\n\n0\n\n\n13.2552\n\n\n19.5265\n\n\n21.7389\n\n\n\n\n\n\nCB4856\n\n\n14.4711\n\n\n12.2563\n\n\n19.3584\n\n\n21.2358\n\n\n\n\n\n\nCB4932\n\n\n0\n\n\n0\n\n\n19.8662\n\n\n20.326\n\n\n\n\n\n\nCX11254\n\n\n17.5516\n\n\n16.9135\n\n\n19.5696\n\n\n21.7276\n\n\n\n\n\n\nCX11264\n\n\n15.3574\n\n\n13.8575\n\n\n18.9888\n\n\n21.3832", 
            "title": "Cegwas"
        }, 
        {
            "location": "/pipeline-cegwas/#cegwas-nf", 
            "text": "", 
            "title": "cegwas-nf"
        }, 
        {
            "location": "/pipeline-cegwas/#docker_image", 
            "text": "The wild-isolate docker file can be used. The  cegwas-nf  pipeline requires R/cegwas to be installed and a python module, awesome-slugify.  andersenlab/wi-nf", 
            "title": "Docker image"
        }, 
        {
            "location": "/pipeline-cegwas/#requirements", 
            "text": "The cegwas-nf pipeline requires  awesome-slugify ; Install using:  pip install awesome-slugify  It also requires that cegwas be installed. See the  cegwas  repo for more information. Alternatively, you can use the wi-nf docker image.", 
            "title": "Requirements"
        }, 
        {
            "location": "/pipeline-cegwas/#usage", 
            "text": "# cd to directory containing a trait file.\nnextflow run Andersenlab/cegwas-nf --in= input file", 
            "title": "Usage"
        }, 
        {
            "location": "/pipeline-cegwas/#input_format", 
            "text": "Save trait data as a  .tsv ; Strains in column 1. Traits are in columns 2 and above.     strain  trait1  trait2  trait3  trait4      AB1  0  0  19.6825  16.5026    AB4  14.5294  13.9775  18.9721  20.6803    BRC20067  18.0132  17.1509  18.4466  21.0243    CB4855_UK  0  13.2552  19.5265  21.7389    CB4856  14.4711  12.2563  19.3584  21.2358    CB4932  0  0  19.8662  20.326    CX11254  17.5516  16.9135  19.5696  21.7276    CX11264  15.3574  13.8575  18.9888  21.3832", 
            "title": "Input Format"
        }, 
        {
            "location": "/cendr/", 
            "text": "CeNDR\n\n\n\n\n\n\nCeNDR\n\n\nSetting up CeNDR\n\n\nClone the repo\n\n\nDownload and install the gcloud-sdk\n\n\nCreate a cendr gcloud configuration\n\n\nInstall direnv\n\n\nObtain credentials\n\n\nLoad the database\n\n\nTest the site\n\n\n\n\n\n\nCreating a new release\n\n\nUploading BAMs\n\n\nUploading Release Data\n\n\nBump the CeNDR version number and change-log\n\n\nAdding the release to the CeNDR website\n\n\nAdding isotype images\n\n\nUpdate the current variant datasets.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSetting up CeNDR\n\n\nClone the repo\n\n\nClone the repo first.\n\n\ngit clone http://www.github.com/andersenlab/cendr\n\n\n\n\nSwitch to the development branch\n\n\ngit checkout --track origin/development\n\n\n\n\nNext you need to install the requirements. You should do this in a virtualenv or pyenv-based environment.\n\n\npip install -r requirements.txt\n\n\n\n\nOnce the requirements are installed you need to load credentials. Create a folder called \nenv_config\n and download the appropriate credential files from the secret credentials location. Add these to the \nenv_config\n folder. \nDo not commit these credentials to github\n.\n\n\nDownload and install the gcloud-sdk\n\n\nInstall the \ngcloud-sdk\n\n\nCreate a cendr gcloud configuration\n\n\ngcloud config configurations create cendr\n\n\n\n\nInstall direnv\n\n\ndirenv\n allows you to load a configuration file when you enter the development directory. Please read about how it works. CeNDR uses a \n.envrc\n file within the repo\nto set up the appropriate environmental variables.\n\n\nObtain credentials\n\n\nCredentials are located in a secret credentials location and should be downloaded to a new folder called \nenv_config\n at the base of the repo.\n\n\nLoad the database\n\n\nThe site uses an SQLite database that can be setup by running:\n\n\nflask init_db\n\n\n\n\nThis will update the SQLite database used by CeNDR (\nbase/cendr.db\n). The tables are:\n\n\n\n\nhomologs\n - A table of homologs+orthologs.\n\n\nstrain\n - Strain info pulled from the google spreadsheet \nC. elegans WI Strain Info\n.\n\n\nwormbase_gene\n - Summarizes gene information; Broken into component parts (e.g. exons, introns etc.).\n\n\nwormbase_gene_summary\n - Summarizes gene information. One line per gene.\n\n\nmetadata\n - tracks how data was obtained. When. Where. etc.\n\n\n\n\n\n\nImportant\n\n\nYou must have credentials in place to install the database\n\n\n\n\nTest the site\n\n\nYou can at this point test the site locally by running:\n\n\nflask run\n\n\n\n\n\n\nCreating a new release\n\n\nBefore a new release is possible, you must have first completed the following tasks:\n\n\nSee \nAdd new sequence data for further details\n.\n\n\n\n\nAdd new wild isolate sequence data, and process with the \ntrimmomatic-nf\n pipeline.\n\n\nIdentified new isotypes using the \nconcordance-nf\n\n\nUpdated the \nC. elegans WI Strain Info\n spreadsheet, adding in new isotypes. \n\n\nUpdate the release column to reflect the release data in the \nC. elegans WI Strain Info\n spreadsheet\n\n\nRun and process sequence data with the \nwi-nf\n pipeline.\n\n\n\n\nPushing a new release requires a series of steps described below.\n\n\nUploading BAMs\n\n\nYou will need AWS credentials to upload BAMs to Amazon S3. These are available in the secret credentials location. \n\n\npip install aws-shell\naws configure # Use s3 user credentials\n\n\n\n\nOnce configured, navigate to the BAM location on b1059.\n\n\ncd /projects/b1059/data/alignments/WI/isotype\n# CD to bams folder...\naws s3 sync . s3://elegansvariation.org/bam\n\n\n\n\nRun this command in screen to ensure that it completes (it's going to take a while)\n\n\nUploading Release Data\n\n\nWhen you run the \nwi-nf\n pipeline it will create a folder with the format \nWI-YYYYMMDD\n. These data are output in a format that CeNDR can read as a release. You must upload the \nWI-YYYYMMDD\n folder to google storage with a command that looks like this:\n\n\n# First cd to the path of the results folder (WI-YYYYMMDD) from the `wi-nf` pipeline.\ngsutil rsync . gs://elegansvariation.org/releases/YYYYMMDD/\n\n\n\n\n\n\nImportant\n\n\nUse rsync to copy the files up to google storage. Note that the \nWI-\n prefix has been dropped from the \nYYYYMMDD\n declaration.\n\n\n\n\nBump the CeNDR version number and change-log\n\n\nBecause we are creating a new data release, we need to \"bump\" or move up the CeNDR version. The CeNDR version number is specified in a file at the base of the repo: \ntravis.yml\n. Modify this line:\n\n\n- export VERSION_NUM=1-2-8\n\n\n\n\nAnd increase the version number by 1 (e.g. 1-2-9).\n\n\nYou should also update the change log and/or add a news item. The change-log is a markdown file located at \nbase/static/content/help/Change-Log.md\n; News items are located at \nbase/static/news/\n. Look at existing content to get an idea of how to add new items. It is fairly straightforward. You should be able to see changes on the test site.\n\n\nAdding the release to the CeNDR website\n\n\nAfter the site is loaded, the BAMs and release data are up, and the database is updated, you need to modify the file \nbase/constants.py\n to add the new release. The date must match the date of the release that was uploaded. Add your release with the appropriate \ndate and the annotation database used (e.g. \n(\"YYYYMMDD\", \"Annotation Database\")\n).\n\n\nRELEASES = [(\n20180413\n, \nWS263\n),\n            (\n20170531\n, \nWS258\n),\n            (\n20160408\n, \nWS245\n)]\n\n\n\n\nCommit your changes to the \ndevelopment branch\n of CeNDR and push them to github. Once pushed, travis-ci will build the app and deploy it to a test branch. Use the google app engine interface to identify and test the app.\n\n\nIf everything looks good open a pull request bringing the changes on the development branch to the master branch. Again, travis-ci will launch the new site.\n\n\n\n\nImportant\n\n\nYou need to shut down development instances and older versions of the site on the google-app engine interface once you are done testing/deploying new instances to prevent us from incurring charges for those running instances.\n\n\n\n\nAdding isotype images\n\n\nIsolation photos are initially prepared on dropbox and are located in the folder here:\n\n\n~/Dropbox/Andersenlab/Reagents/WormReagents/isolation_photos/c_elegans\n\n\n\n\nEach file should be named using the isotype name and the strain name strain name in the following format:\n\n\nisotype\n_\nstrain\n.jpg\n\n\n\n\nThen you will use \nimagemagick\n (a commandline-based utility) to scale the images down to 1000 pixels (width) and generate a 150px thumbnail.\n\n\nfor img in `ls *.jpg`; do\n    convert ${img} -density 300 -resize 1000 ${img}\n    convert ${img} -density 300 -resize 150  ${img/.jpg/}.thumb.jpg\ndone;\n\n\n\n\nOnce you have generated the images you can upload them to google storage. They should be uploaded to the following location:\n\n\ngs://elegansvariation.org/photos/isolation\n\n\n\n\nYou can drag/drop the photos using the web-based browser or use \ngsutil\n:\n\n\n# First cd to the appropriate directory\n# cd ~/Dropbox/Andersenlab/Reagents/WormReagents/isolation_photos/c_elegans\n\ngsutil rsync -x \n.DS_Store\n . gs://elegansvariation.org/photos/isolation\n\n\n\n\nThe script for processing files is located in the dropbox folder and is called 'process_images.sh'. It's also here:\n\n\nfor img in `ls *.jpg`; do\n    convert ${img} -density 300 -resize 1000 ${img}\n    convert ${img} -density 300 -resize 150  ${img/.jpg/}.thumb.jpg\ndone;\n\n# Copy using rsync; Skip .DS_Store files.\ngsutil rsync -x \n.DS_Store\n . gs://elegansvariation.org/photos/isolation\n\n\n\n\nUpdate the \ncurrent\n variant datasets.\n\n\nThe \ncurrent\n folder located in \ngs://elegansvariation.org/releases\n contains the latest variant datasets and is used by WormBase to display natural variation data. Once you've completed a new release, update the files in this folder \ngs://elegansvariation.org/releases/current\n folder.", 
            "title": "CeNDR"
        }, 
        {
            "location": "/cendr/#cendr", 
            "text": "CeNDR  Setting up CeNDR  Clone the repo  Download and install the gcloud-sdk  Create a cendr gcloud configuration  Install direnv  Obtain credentials  Load the database  Test the site    Creating a new release  Uploading BAMs  Uploading Release Data  Bump the CeNDR version number and change-log  Adding the release to the CeNDR website  Adding isotype images  Update the current variant datasets.", 
            "title": "CeNDR"
        }, 
        {
            "location": "/cendr/#setting_up_cendr", 
            "text": "", 
            "title": "Setting up CeNDR"
        }, 
        {
            "location": "/cendr/#clone_the_repo", 
            "text": "Clone the repo first.  git clone http://www.github.com/andersenlab/cendr  Switch to the development branch  git checkout --track origin/development  Next you need to install the requirements. You should do this in a virtualenv or pyenv-based environment.  pip install -r requirements.txt  Once the requirements are installed you need to load credentials. Create a folder called  env_config  and download the appropriate credential files from the secret credentials location. Add these to the  env_config  folder.  Do not commit these credentials to github .", 
            "title": "Clone the repo"
        }, 
        {
            "location": "/cendr/#download_and_install_the_gcloud-sdk", 
            "text": "Install the  gcloud-sdk", 
            "title": "Download and install the gcloud-sdk"
        }, 
        {
            "location": "/cendr/#create_a_cendr_gcloud_configuration", 
            "text": "gcloud config configurations create cendr", 
            "title": "Create a cendr gcloud configuration"
        }, 
        {
            "location": "/cendr/#install_direnv", 
            "text": "direnv  allows you to load a configuration file when you enter the development directory. Please read about how it works. CeNDR uses a  .envrc  file within the repo\nto set up the appropriate environmental variables.", 
            "title": "Install direnv"
        }, 
        {
            "location": "/cendr/#obtain_credentials", 
            "text": "Credentials are located in a secret credentials location and should be downloaded to a new folder called  env_config  at the base of the repo.", 
            "title": "Obtain credentials"
        }, 
        {
            "location": "/cendr/#load_the_database", 
            "text": "The site uses an SQLite database that can be setup by running:  flask init_db  This will update the SQLite database used by CeNDR ( base/cendr.db ). The tables are:   homologs  - A table of homologs+orthologs.  strain  - Strain info pulled from the google spreadsheet  C. elegans WI Strain Info .  wormbase_gene  - Summarizes gene information; Broken into component parts (e.g. exons, introns etc.).  wormbase_gene_summary  - Summarizes gene information. One line per gene.  metadata  - tracks how data was obtained. When. Where. etc.    Important  You must have credentials in place to install the database", 
            "title": "Load the database"
        }, 
        {
            "location": "/cendr/#test_the_site", 
            "text": "You can at this point test the site locally by running:  flask run", 
            "title": "Test the site"
        }, 
        {
            "location": "/cendr/#creating_a_new_release", 
            "text": "Before a new release is possible, you must have first completed the following tasks:  See  Add new sequence data for further details .   Add new wild isolate sequence data, and process with the  trimmomatic-nf  pipeline.  Identified new isotypes using the  concordance-nf  Updated the  C. elegans WI Strain Info  spreadsheet, adding in new isotypes.   Update the release column to reflect the release data in the  C. elegans WI Strain Info  spreadsheet  Run and process sequence data with the  wi-nf  pipeline.   Pushing a new release requires a series of steps described below.", 
            "title": "Creating a new release"
        }, 
        {
            "location": "/cendr/#uploading_bams", 
            "text": "You will need AWS credentials to upload BAMs to Amazon S3. These are available in the secret credentials location.   pip install aws-shell\naws configure # Use s3 user credentials  Once configured, navigate to the BAM location on b1059.  cd /projects/b1059/data/alignments/WI/isotype\n# CD to bams folder...\naws s3 sync . s3://elegansvariation.org/bam  Run this command in screen to ensure that it completes (it's going to take a while)", 
            "title": "Uploading BAMs"
        }, 
        {
            "location": "/cendr/#uploading_release_data", 
            "text": "When you run the  wi-nf  pipeline it will create a folder with the format  WI-YYYYMMDD . These data are output in a format that CeNDR can read as a release. You must upload the  WI-YYYYMMDD  folder to google storage with a command that looks like this:  # First cd to the path of the results folder (WI-YYYYMMDD) from the `wi-nf` pipeline.\ngsutil rsync . gs://elegansvariation.org/releases/YYYYMMDD/   Important  Use rsync to copy the files up to google storage. Note that the  WI-  prefix has been dropped from the  YYYYMMDD  declaration.", 
            "title": "Uploading Release Data"
        }, 
        {
            "location": "/cendr/#bump_the_cendr_version_number_and_change-log", 
            "text": "Because we are creating a new data release, we need to \"bump\" or move up the CeNDR version. The CeNDR version number is specified in a file at the base of the repo:  travis.yml . Modify this line:  - export VERSION_NUM=1-2-8  And increase the version number by 1 (e.g. 1-2-9).  You should also update the change log and/or add a news item. The change-log is a markdown file located at  base/static/content/help/Change-Log.md ; News items are located at  base/static/news/ . Look at existing content to get an idea of how to add new items. It is fairly straightforward. You should be able to see changes on the test site.", 
            "title": "Bump the CeNDR version number and change-log"
        }, 
        {
            "location": "/cendr/#adding_the_release_to_the_cendr_website", 
            "text": "After the site is loaded, the BAMs and release data are up, and the database is updated, you need to modify the file  base/constants.py  to add the new release. The date must match the date of the release that was uploaded. Add your release with the appropriate \ndate and the annotation database used (e.g.  (\"YYYYMMDD\", \"Annotation Database\") ).  RELEASES = [( 20180413 ,  WS263 ),\n            ( 20170531 ,  WS258 ),\n            ( 20160408 ,  WS245 )]  Commit your changes to the  development branch  of CeNDR and push them to github. Once pushed, travis-ci will build the app and deploy it to a test branch. Use the google app engine interface to identify and test the app.  If everything looks good open a pull request bringing the changes on the development branch to the master branch. Again, travis-ci will launch the new site.   Important  You need to shut down development instances and older versions of the site on the google-app engine interface once you are done testing/deploying new instances to prevent us from incurring charges for those running instances.", 
            "title": "Adding the release to the CeNDR website"
        }, 
        {
            "location": "/cendr/#adding_isotype_images", 
            "text": "Isolation photos are initially prepared on dropbox and are located in the folder here:  ~/Dropbox/Andersenlab/Reagents/WormReagents/isolation_photos/c_elegans  Each file should be named using the isotype name and the strain name strain name in the following format:  isotype _ strain .jpg  Then you will use  imagemagick  (a commandline-based utility) to scale the images down to 1000 pixels (width) and generate a 150px thumbnail.  for img in `ls *.jpg`; do\n    convert ${img} -density 300 -resize 1000 ${img}\n    convert ${img} -density 300 -resize 150  ${img/.jpg/}.thumb.jpg\ndone;  Once you have generated the images you can upload them to google storage. They should be uploaded to the following location:  gs://elegansvariation.org/photos/isolation  You can drag/drop the photos using the web-based browser or use  gsutil :  # First cd to the appropriate directory\n# cd ~/Dropbox/Andersenlab/Reagents/WormReagents/isolation_photos/c_elegans\n\ngsutil rsync -x  .DS_Store  . gs://elegansvariation.org/photos/isolation  The script for processing files is located in the dropbox folder and is called 'process_images.sh'. It's also here:  for img in `ls *.jpg`; do\n    convert ${img} -density 300 -resize 1000 ${img}\n    convert ${img} -density 300 -resize 150  ${img/.jpg/}.thumb.jpg\ndone;\n\n# Copy using rsync; Skip .DS_Store files.\ngsutil rsync -x  .DS_Store  . gs://elegansvariation.org/photos/isolation", 
            "title": "Adding isotype images"
        }, 
        {
            "location": "/cendr/#update_the_current_variant_datasets", 
            "text": "The  current  folder located in  gs://elegansvariation.org/releases  contains the latest variant datasets and is used by WormBase to display natural variation data. Once you've completed a new release, update the files in this folder  gs://elegansvariation.org/releases/current  folder.", 
            "title": "Update the current variant datasets."
        }, 
        {
            "location": "/r/", 
            "text": "R\n\n\nR Packages\n\n\nThe Andersen lab maintains several R packages.\n\n\ncegwas\n\n\nandersenlab/cegwas\n\n\nlinkagemapping\n\n\nandersenlab/linkagemapping\n\n\neasysorter\n\n\nandersenlab/easysorter", 
            "title": "R"
        }, 
        {
            "location": "/r/#r", 
            "text": "", 
            "title": "R"
        }, 
        {
            "location": "/r/#r_packages", 
            "text": "The Andersen lab maintains several R packages.", 
            "title": "R Packages"
        }, 
        {
            "location": "/r/#cegwas", 
            "text": "andersenlab/cegwas", 
            "title": "cegwas"
        }, 
        {
            "location": "/r/#linkagemapping", 
            "text": "andersenlab/linkagemapping", 
            "title": "linkagemapping"
        }, 
        {
            "location": "/r/#easysorter", 
            "text": "andersenlab/easysorter", 
            "title": "easysorter"
        }, 
        {
            "location": "/backup/", 
            "text": "Configuring AWS Client\n\n\nThe aws commandline utility is installed as a part of the \nandersen-lab-env\n\n\nYou must provide the appropriate s3 credentials in order to backup BAMs. Erik knows where these are.\n\n\naws configure\n\n\n\n\nBacking up BAMS to Amazon S3\n\n\nBAMS are uploaded to S3. This allows us to back them up there and access them through CeNDR.\n\n\nCurrently, the only BAMs that we upload to s3 are wild isolate BAMs. Once you've added new sequence data you can sync the new BAMs by running the following:\n\n\n# First, cd to the location of isotype bams\ncd /projects/b1059/data/alignments/WI/isotype\n# Then run the following command\naws s3 sync s3://elegansvariation.org/bam .\n\n\n\n\nLocal Backup\n\n\nIn addition to backing up BAMs on S3, we also backup FASTQs locally. There are four hard drives:\n\n\n\n\nPortusTotus - FASTQ Backup 1\n\n\nHawkeye - FASTQ Backup 2\n\n\nArmadillo - open\n\n\nRhino - open\n\n\n\n\nEach one is labeled with what is backed up on it. After you have added new sequenced data you should sync \nFROM\n the data/fastq folder on b1059 \nTO\n the the PortusTotus (FASTQ Backup 1) and Hawkeye (FASTQ Backup 2) hard drives.", 
            "title": "Backup"
        }, 
        {
            "location": "/backup/#configuring_aws_client", 
            "text": "The aws commandline utility is installed as a part of the  andersen-lab-env  You must provide the appropriate s3 credentials in order to backup BAMs. Erik knows where these are.  aws configure", 
            "title": "Configuring AWS Client"
        }, 
        {
            "location": "/backup/#backing_up_bams_to_amazon_s3", 
            "text": "BAMS are uploaded to S3. This allows us to back them up there and access them through CeNDR.  Currently, the only BAMs that we upload to s3 are wild isolate BAMs. Once you've added new sequence data you can sync the new BAMs by running the following:  # First, cd to the location of isotype bams\ncd /projects/b1059/data/alignments/WI/isotype\n# Then run the following command\naws s3 sync s3://elegansvariation.org/bam .", 
            "title": "Backing up BAMS to Amazon S3"
        }, 
        {
            "location": "/backup/#local_backup", 
            "text": "In addition to backing up BAMs on S3, we also backup FASTQs locally. There are four hard drives:   PortusTotus - FASTQ Backup 1  Hawkeye - FASTQ Backup 2  Armadillo - open  Rhino - open   Each one is labeled with what is backed up on it. After you have added new sequenced data you should sync  FROM  the data/fastq folder on b1059  TO  the the PortusTotus (FASTQ Backup 1) and Hawkeye (FASTQ Backup 2) hard drives.", 
            "title": "Local Backup"
        }, 
        {
            "location": "/cloud/", 
            "text": "AndersenLab cloud resources\n\n\nFor full documentation visit \nmkdocs.org\n.\n\n\n\n\n\n\nAndersenLab cloud resources\n\n\nGoogle Domains\n\n\nGoogle Cloud\n\n\nGoogle Cloud Storage\n\n\nBuckets\n\n\nelegansvariation.org\n\n\nandersenlab.org\n\n\nOther buckets\n\n\nSecret Bucket\n\n\ncegwas (deprecated)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGoogle datastore\n\n\nApp engine\n\n\nError Reporting\n\n\nBigQuery\n\n\nAWS\n\n\nS3\n\n\nFargate\n\n\n\n\n\n\n\n\n\n\nGoogle Domains\n\n\nAny domain names the lab uses should be registered with Google Domains. The two ones currently are:\n\n\n\n\nandersenlab.org\n\n\nelegansvariation.org\n\n\n\n\nGoogle domains can be used to forward domain-specific email addresses if necessary. \nFor example, example@andersenlab.org could be created and forwarded to an email address.\n\n\nGoogle Cloud\n\n\nGoogle cloud is used for a variety of services that the lab uses.\n\n\nGoogle Cloud Storage\n\n\nGoogle cloud storage is used to store and distribute files on CeNDR, for cegwas, and for some files on the lab website.\n\n\nBuckets\n\n\nFiles are grouped into 'buckets' on google storage. We use the following buckets:\n\n\nelegansvariation.org\n\n\nThis bucket contains all the data associated with elegansvariation.org. It is broken down into six primary directories.\n\n\n\n\nbrowser_tracks\n - for genome-browser tracks that rarely if ever change.\n\n\ndb\n - Storage/access to the SQLite database.\n\n\nphotos\n - sample collection photos.\n\n\nreleases\n - dataset releases. For more detail, see \nwi-nf\n.\n\n\nreports\n - images and data files within reports.\n\n\nstatic\n - static assets used by the site.\n\n\n\n\nandersenlab.org\n\n\nIn some cases the data associated with a publication is too large to put on github. We store those data here, along with a \ncouple other odds and ends.\n\n\nOther buckets\n\n\nGoogle Cloud creates a bunch of other buckets too. Most of these should be ignored as they are part of the \n\n\nSecret Bucket\n\n\nThere is one other secret bucket. Ask Erik about it.\n\n\ncegwas (deprecated)\n\n\nCurrently this bucket has a SQLite database used by cegwas. This will be replaced to use the same database\nused by CeNDR in the \ndb/\n folder.\n\n\nOnce the new version of cegwas is developed - this section should be removed, and that bucket should be deleted.\n\n\nGoogle datastore\n\n\nGoogle datastore used as the database for CeNDR. It stores information on mappings, traits, and more.\n\n\nApp engine\n\n\nApp engine is the platform CeNDR runs on.\n\n\nError Reporting\n\n\nGoogle cloud contains a really nice error reporting interface. Error reports are generated whenever something goes wrong on\nCeNDR. A github issue can be created for these errors and they can be addressed.\n\n\nBigQuery\n\n\nWe have used bigquery in the past for large query jobs. We are not actively using it as of late.\n\n\nAWS\n\n\nS3\n\n\nS3 = simple storage service. We use it to store BAMs.\n\n\nFargate\n\n\nAmazon Fargate is used to run the mapping pipeline on CeNDR", 
            "title": "Cloud"
        }, 
        {
            "location": "/cloud/#andersenlab_cloud_resources", 
            "text": "For full documentation visit  mkdocs.org .    AndersenLab cloud resources  Google Domains  Google Cloud  Google Cloud Storage  Buckets  elegansvariation.org  andersenlab.org  Other buckets  Secret Bucket  cegwas (deprecated)        Google datastore  App engine  Error Reporting  BigQuery  AWS  S3  Fargate", 
            "title": "AndersenLab cloud resources"
        }, 
        {
            "location": "/cloud/#google_domains", 
            "text": "Any domain names the lab uses should be registered with Google Domains. The two ones currently are:   andersenlab.org  elegansvariation.org   Google domains can be used to forward domain-specific email addresses if necessary. \nFor example, example@andersenlab.org could be created and forwarded to an email address.", 
            "title": "Google Domains"
        }, 
        {
            "location": "/cloud/#google_cloud", 
            "text": "Google cloud is used for a variety of services that the lab uses.", 
            "title": "Google Cloud"
        }, 
        {
            "location": "/cloud/#google_cloud_storage", 
            "text": "Google cloud storage is used to store and distribute files on CeNDR, for cegwas, and for some files on the lab website.", 
            "title": "Google Cloud Storage"
        }, 
        {
            "location": "/cloud/#buckets", 
            "text": "Files are grouped into 'buckets' on google storage. We use the following buckets:", 
            "title": "Buckets"
        }, 
        {
            "location": "/cloud/#elegansvariationorg", 
            "text": "This bucket contains all the data associated with elegansvariation.org. It is broken down into six primary directories.   browser_tracks  - for genome-browser tracks that rarely if ever change.  db  - Storage/access to the SQLite database.  photos  - sample collection photos.  releases  - dataset releases. For more detail, see  wi-nf .  reports  - images and data files within reports.  static  - static assets used by the site.", 
            "title": "elegansvariation.org"
        }, 
        {
            "location": "/cloud/#andersenlaborg", 
            "text": "In some cases the data associated with a publication is too large to put on github. We store those data here, along with a \ncouple other odds and ends.", 
            "title": "andersenlab.org"
        }, 
        {
            "location": "/cloud/#other_buckets", 
            "text": "Google Cloud creates a bunch of other buckets too. Most of these should be ignored as they are part of the", 
            "title": "Other buckets"
        }, 
        {
            "location": "/cloud/#secret_bucket", 
            "text": "There is one other secret bucket. Ask Erik about it.", 
            "title": "Secret Bucket"
        }, 
        {
            "location": "/cloud/#cegwas_deprecated", 
            "text": "Currently this bucket has a SQLite database used by cegwas. This will be replaced to use the same database\nused by CeNDR in the  db/  folder.  Once the new version of cegwas is developed - this section should be removed, and that bucket should be deleted.", 
            "title": "cegwas (deprecated)"
        }, 
        {
            "location": "/cloud/#google_datastore", 
            "text": "Google datastore used as the database for CeNDR. It stores information on mappings, traits, and more.", 
            "title": "Google datastore"
        }, 
        {
            "location": "/cloud/#app_engine", 
            "text": "App engine is the platform CeNDR runs on.", 
            "title": "App engine"
        }, 
        {
            "location": "/cloud/#error_reporting", 
            "text": "Google cloud contains a really nice error reporting interface. Error reports are generated whenever something goes wrong on\nCeNDR. A github issue can be created for these errors and they can be addressed.", 
            "title": "Error Reporting"
        }, 
        {
            "location": "/cloud/#bigquery", 
            "text": "We have used bigquery in the past for large query jobs. We are not actively using it as of late.", 
            "title": "BigQuery"
        }, 
        {
            "location": "/cloud/#aws", 
            "text": "", 
            "title": "AWS"
        }, 
        {
            "location": "/cloud/#s3", 
            "text": "S3 = simple storage service. We use it to store BAMs.", 
            "title": "S3"
        }, 
        {
            "location": "/cloud/#fargate", 
            "text": "Amazon Fargate is used to run the mapping pipeline on CeNDR", 
            "title": "Fargate"
        }, 
        {
            "location": "/labsite/", 
            "text": "Andersenlab.org\n\n\n\n\n\n\nAndersenlab.org\n\n\nGetting Started\n\n\nSoftware-Dependencies\n\n\nCloning the repo\n\n\nUpdating the site\n\n\n\n\n\n\nandersenlab.github.io\n\n\nAnnouncements\n\n\nGeneral Announcements\n\n\nPublication Post\n\n\n\n\n\n\nLab members\n\n\nAdding new lab members:\n\n\nSet Status to Former\n\n\nRemove lab members\n\n\n\n\n\n\nFunding\n\n\nProtocols\n\n\nResearch\n\n\nPublications\n\n\nPhoto Albums\n\n\nSoftware\n\n\n\n\n\n\n\n\n\n\nGetting Started\n\n\nThe Andersen Lab website was built using \njekyll\n and runs using the \nGithub Pages\n service.\n\n\nSoftware-Dependencies\n\n\nSeveral software packages are required for editing/maintaining the Andersen Lab site. They can be installed using \nHomebrew\n:\n\n\nbrew install ruby imagemagick exiftool python ghostscript\nbrew upgrade ruby # If Ruby has not been updated in a while, you may need to do this.\nsudo gem install jekyll -v 3.6.0\n# If you get an error when trying to run pip, try:\n# brew link --overwrite python\npip install metapub pyyaml\n\n\n\n\n\n\nRuby\n - Is used to run jekyll, which is the software that builds the site.\n\n\nJekyll\n - As stated earlier, jekyll builds the static site and is written in Ruby.\n\n\nImagemagick\n - Handles thumbnail generation and scaling photos. Imagemagick is used in the \nbuild.sh\n script.\n\n\nexiftool\n Extract data about photos as part of the \nbuild.sh\n script for use in scaling images.\n\n\nPython\n Retrieves information about publications and updates \n_data/pubs_data.yaml\n.\n\n\n\n\nCloning the repo\n\n\nTo get started editing, clone the repo:\n\n\ngit clone https://github.com/andersenlab/andersenlab.github.io\n\n\n\n\nThis repo contains documents that get compiled into the Andersen Lab website. When you make changes to this repo and push them to GitHub, Github will trigger a 'build' of the labsite and update it. This usually takes less than a minute. Instructions for changing various aspects of the site are listed below.\n\n\nYou can also use \nGithub Desktop\n to manage changes to the site. \n\n\nIf you want to edit the site locally and preview changes, run the following in the \nroot\n directory of the git repo:\n\n\njekyll serve\n\n\n\n\nThe site should become available at \nlocalhost:4000\n and any changes you make will be reflected at that local url.\n\n\nUpdating the site\n\n\nIn order for any change to become visible, you need to use git. Any subsequent directions that suggest modifying, adding, or removing files assumes you will be \ncommitting\n these changes to the repo and \npushing\n the commit to GitHub.com. See \nGit-SCM\n for a basic introduction to git.\n\n\nandersenlab.github.io\n\n\nThe structure of the Andersen Lab repo looks like this:\n\n\nCNAME\nLICENSE\nREADME.md\nbuild.sh\nindex.html\n_config.yml\n_data/\n_includes/\n_layouts/\n_posts/\n_site/\nassets/\nfeeds/\nfiles/\npages/\npeople/\npublications/\nscripts/\nprotocols/\nfunding/\n\n\n\n\nThe folders prefixed with\n\n\nAnnouncements\n\n\nAnnouncements are stored in the \n_posts\n folder. Posts are organized into folders by year. There is also a \n_photo_albums\n folder that you can ignore (more on this below). Two types of announcements can be made. A 'general' announcement regarding anything, or a new publication.\n\n\nGeneral Announcements\n\n\nTo add a new post create a new text file with the following naming scheme:\n\n\nYYYY-MM-DD-title.md\n\n\n\n\nFor example:\n\n\n2017-09-24-A new post.md\n\n\n\n\nThe contents of the file should correspond to the following structure:\n\n\n---\ntitle: \nThe title of the post\n\nlayout: post\ntags: news\npublished: true\n---\n\nThe post content goes here!\n\n\n\n\nThe top part surrounded by \n---\n is known as the \nheader\n and has to define a number of variables:\n\n\nlayout: post\n, \ntags: news\n, and \npublished: true\n should always be set and should not change. The only thing you will change is the \ntitle\n. Set a title, and add content below.\n\n\nBecause we used a \n*.md\n extension when naming the file, we can use markdown in the post to create headings, links, images, and more.\n\n\nPublication Post\n\n\nNew publication posts can be created. These posts embed a publication summary identical to what you see on the publication page. They follow the same paradigm as above except they require two additional lines in the header:\n\n\n\n\nsubtitle:\n - Usually the title of the paper; Appears on homepage.\n\n\nPMID:\n - The pubmed identifier\n\n\n\n\nExample\n:\n\n\n---\ntitle: \nKatie's paper accepted at \nem\nG3\n/em\n!\n\nsubtitle: \nCorrelations of geneotype with climate parameters suggest \nem\nCaenorhabditis elegans\n/em\n niche adaptations\n\nlayout: post\ntags: news\npublished: true\nPMID: 27866149\n---\n\nCongratulations to Katie for her paper accepted at G3!\n\n\n\n\nLab members\n\n\nAdding new lab members:\n\n\n\n\n(1)\n - Add a photo of the individual to the \npeople/\n folder.\n\n\n(2)\n - Edit the \n_data/people.yaml\n file, and add the information about that individual.\n\n\n\n\nEach individual should have - at a minimum, the following:\n\n\n- first_name: \nfirst name\n\n  last_name: \nlast name\n\n  title:  \nThe job title of the individual; e.g. Graduate Student; Research Associate; Undergrad; Postdoctoral Researcher; Lab technician\n\n  photo: \nfilename of the photo located in the people/ directory\n\n\n\n\n\nAdditional fields can also be added:\n\n\n- first_name: \nfirst name\n\n  last_name: \nlast name\n\n  title: \nThe job title of the individual; e.g. Graduate Student; Research Associate; Undergrad; Postdoctoral Researcher; Lab technician\n\n  pub_names: [\nan array\n, \nof possible\n, \npublication\n, \nnames\n]\n  photo: \nbase filename of the photo located in the people/ directory; e.g. 'dan.jpg'\n\n  website: \nwebsite\n\n  description: \na description of research\n\n  email: \nemail\n\n  github: \ngithub username\n\n\n\n\n\n\n\nNote\n\n\npub_names\n is a list of possible ways an individual is referenced in the author list of a publication. This creates links from the publications page back to lab members on the people page.\n\n\n\n\nSet Status to Former\n\n\nLab members can be moved to the bottom of the people page under the 'former member' area. To do this add a \nformer: true\n line for that individual and a \ncurrent_status:\n line indicating what they are up to.\n\n\nFor example:\n\n\n- first_name: Mostafa\n  pub_names: \n    - Zamanian M\n  last_name: Zamanian\n  description: My research broadly spans \nneglected disease\n genomics and drug discovery. I am currently working to uncover new genetic determinants of anthelmintic resistance and to develop genome editing technology for human pathogenic helminths.\n  title: Postdoctoral Researcher, 2015-2017\n  photo: Mostafa2014.jpg\n  former: true\n  github: mzamanian\n  email: zamanian@northwestern.edu\n  current_status: Assistant Professor at UW Madison -- \na href='http://www.zamanianlab.org/'\nZamanian Lab Website\n/a\n\n\n\n\n\nRemove lab members\n\n\nRemove the persons information from \n_data/people.yaml\n; Optionally delete their photo.\n\n\nFunding\n\n\nFunding is managed using the \nfunding/\n folder in the root directory and the data file \n_data/funding_links.yaml\n. The \nfunding/\n folder has two subfolders: \npast/\n and \ncurrent/\n for past funding and current funding. Rename the logo file to be lowercase and simple.\n\n\nTo update funding simply place the logo of the institution providing funding in one of these folders and it will appear on the funding page under the heading corresponding to the subfolder in which it was placed. If you would like to add a link for the funding of that organization you can edit the \n_data/funding_links.yaml\n file.\n\n\nThis file is structured as a set of \nbasename: url\n pairs:\n\n\nnigms: https://www.nigms.nih.gov/Pages/default.aspx\nacs: http://www.cancer.org/\npew: http://www.pewtrusts.org/en\nniaid: https://www.niaid.nih.gov/\naws: https://aws.amazon.com/\nweinberg: http://www.weinberg.northwestern.edu/\nmod: http://www.marchofdimes.org/\ncbc: http://www.chicagobiomedicalconsortium.org/\n\n\n\n\nEach acronym above corresponds with an image file in the \ncurrent/\n or \npast/\n folder. Notice that the extension (e.g. jpeg/png, etc) does not matter. Just use the basename of the file and its associated link here.\n\n\nProtocols\n\n\nProtocols are stored in the \nprotocols/\n folder and their titles and pdfs are managed in \n_data/protocols.yaml\n. To add a new protocol, add the PDF to the \nprotocols/\n folder. Then add these lines to the \n_data/protocols.yaml\n file:\n\n\n- Name: Title of Protocol\n  file: filename_of_protocol_in_protocols_folder.pdf\n  group: \nem\nC. elegans\n/em\n Phenotyping methods\n\n\n\n\n\n\nname\n - The name of the protocol\n\n\nfile\n - The filename of the protocol within the \nprotocols/\n folder.\n\n\ngroup\n - The grouping of the protocol; It will be nested under this grouping on the protocols page.\n\n\n\n\n- name: Semi-Quantitative Brood Assay\n  file: SemiQuantitativeBroodAssay.pdf\n  group: \nem\nC. elegans\n/em\n Phenotyping Methods\n- name: \nem\nPseudomonas aeruginosa\n/em\n Fast-killing assay\n/a\n\n  file: FKAprotocol.pdf\n  group: \nem\nC. elegans\n/em\n Phenotyping Methods\n- name: \nem\nStaphylococcus aureus\n/em\n killing assay\n/a\n\n  file: Staphaureus_Protocol.pdf\n  group: \nem\nC. elegans\n/em\n Phenotyping Methods\n- name: \nem\nBacillus thuringiensis\n/em\n toxin assay on plates\n/a\n\n  file: Bacillus-thuringiensis-toxin-plate-assay.pdf\n  group: \nem\nC. elegans\n/em\n Phenotyping Methods\n\n\n\n\nTo remove a protocol, delete the pdf and remove the corresponding lines.\n\n\nResearch\n\n\nThe research portion of the site is structured as a set of sections - each devoted to a project/area. Navigate to \n/pages/research\n and you will see a set of files:\n\n\n\n\nresearch.html\n - This page controls the content at the top of the research page. It's an overview of research in the Andersen lab. You can edit the top portion between the \np\n[content]\n/p\n tags freely to modify the top of the research page.\n\n\nresearch-*.md\n - These are the individual projects. These files look like this:\n\n\n\n\n---\ntitle: High-throughput approaches to understand conserved drug responses\nimage: worms_drugs2.jpg\norder: 1\n---\n\nBecause of the efforts of a number of highly dedicated scientists and citizen volunteers...\n\nTo this end, we deep sequenced all of these strains...\n\n\n\n\nThe page includes a header (the items located between \n---\n) which includes a number of important items.\n\n\n\n\ntitle\n - the title to display for the research area.\n\n\nimage\n - An image for that research area/project. This is the base name of the image placed in \n/assets/img/research/\n\n\norder\n - A number indicating the order you would like the page ot appear in. Order is descending and any set of numbers can be used to determine sort order (e.g. 1, 2, 5, 8, 1000). \n\n\n\n\nPublications\n\n\nElements used to construct the publications page of the website are stored in two places:\n\n\n\n\n_data/pubs_data.yaml\n - The publications data stores authors, pub date, journal, etc.\n\n\npublications/\n - The publications folder for PDFs, thumbnails, and supplementary files.\n\n\n\n\n(1) Download a PDF of the publication\n\n\nYou will want to remove any additional PDF pages (e.g. cover sheets) if there are any present in the PDF. See \nthis guide\n for information on removing pages from a PDF.\n\n\nSave the PDF to \n/publications/[year][tag]\n\n\nWhere \ntag\n is a unique identifier for the publication. In general, these have been the first author or the journal or a combination of both.\n\n\n(Optional) PMID Known\n\n\nIf the PubMed Identifier (PMID) is known for the publication, you can add it to the file \npublications/publications_list.txt\n.\n\n\n(2) Run \nbuild.sh\n\n\nThe \nbuild.sh\n script does a variety of tasks for the website. For publications - it will generate thumbnails. It will also fetch information for publications and add it to the \n_data/pubs_data.yaml\n file \nif\n a PMID has been provided. If you did not add a PMID, you will have to manually add authors, journal, etc. to the \n_data/pubs_data.yaml\n file.\n\n\n(3) Edit \n_data/pubs_data.yaml\n\n\nThe publication should now be added either manually or automatically to \n_data/pubs_data.yaml\n and should look something like this:\n\n\n- Authors: [Laricchia KM, Zdraljevic S, Cook DE, Andersen EC]\n  Citations: 0\n  DOI: 10.1093/molbev/msx155\n  Journal: Molecular Biology and Evolution\n  PMID: 28486636\n  Title: Natural variation in the distribution and abundance of transposable elements\n    across the Caenorhabditis elegans species\n\n\n\n\nYou will need to add a few things:\n  - Add a \nPDF:\n line to associate the publication with the correct PDF and its thumbnail. This is the same \ntag\n you used above.\n  - If there is no \nDate_Published:\n line you will want to add that. The format is \nYYYY-month_abbr-DD\n (e.g. \n2017 Aug 17\n).\n  - Add \nem\n tags around items you want to italicize: \nem\nCaenorhabditis elegans\n/em\n\n\nFinal result:\n\n\n- Authors: [Laricchia KM, Zdraljevic S, Cook DE, Andersen EC]\n  Citations: 0\n  DOI: 10.1093/molbev/msx155\n  Date_Published: 2017 May 09\n  Journal: Molecular Biology and Evolution\n  PMID: 28486636\n  Title: Natural variation in the distribution and abundance of transposable elements\n    across the \nem\nCaenorhabditis elegans\n/em\n species\n  PDF: 2017Laricchia\n\n\n\n\n(4) Add supplementary data\n\n\nSupplemental data and figures are stored in \npublications/[pdf_name]\n. For example, \n2017Laricchia\n has an associated folder in \npublications/\n where supplemental data and figures are stored: \npublications/2017Laricchia/\nsupplemental files\n\n\nOnce you have added supplemental files, you'll need to add some information to \n_data/pubs_data.yaml\n to describe them. These are the lines that were added for \n2017Laricchia\n:\n\n\n  pub_data:\n    files:\n      Supplemental_Figures.pdf: {title: Supplemental Figures}\n      Supplemental_Files.zip: {title: Supplemental Files}\n      Supplemental_Tables.zip: {title: Supplemental Tables}\\\n      ...\n      \nname of file\n: {title: \ntitle to display\n}\n\n\n\n\nThe last line above illustrates the format. The name of the file must match exactly what is in the \npublications/[pdf_name]\n folder. Resulting supplemental data will be listed under publications and on the data page.\n\n\nPhoto Albums\n\n\nPhoto albums can be added to the Andersen Labsite. Adding albums requires two utilities to be installed on your computer:\n\n\n\n\n(a) Image Magick\n\n\n(b) exiftool\n\n\n\n\nThese can easily be installed with \nhomebrew\n. should have been installed during Setup (above), but if not you can install them using the following:\n\n\nbrew install imagemagick\nbrew install exiftool\n\n\n\n\n(1) Place images in a folder and name it according to the following schema:\n\n\nYYYY-MM-DD-title\n\n\nFor example, \n2017-08-05-Hawaii Trip\n.\n\n\n(2) Move that folder to \n/people/albums/\n\n\n(3) Run the \nbuild.sh\n script in the root of the \nandersenlab.github.io\n repo.\n\n\nThe \nbuild.sh\n script will do the following:\n\n\n\n\n(a) Construct pages for the album being published.\n\n\n(b) Decrease the size of the images in the album (max width=1200). \n\n\n\n\n\n\nNote\n\n\nThe \nbuild.sh\n script also performs other maintenance-related tasks. It is fine to run this script at anytime.\n\n\n\n\nYou can run the script using:\n\n\nbash build.sh\n\n\n\n\n(4) Add the images using git and push to GitHub\n\n\nYou can easily add all images using:\n\n\ngit add *.jpg\n\n\n\n\n(5) Push changes to github\n\n\ngit push\n\n\n\n\nSoftware\n\n\nIf you can write software you should be able to figure out how to update this section. It's markdown/html and not overly complicated.", 
            "title": "Andersen Labsite"
        }, 
        {
            "location": "/labsite/#andersenlaborg", 
            "text": "Andersenlab.org  Getting Started  Software-Dependencies  Cloning the repo  Updating the site    andersenlab.github.io  Announcements  General Announcements  Publication Post    Lab members  Adding new lab members:  Set Status to Former  Remove lab members    Funding  Protocols  Research  Publications  Photo Albums  Software", 
            "title": "Andersenlab.org"
        }, 
        {
            "location": "/labsite/#getting_started", 
            "text": "The Andersen Lab website was built using  jekyll  and runs using the  Github Pages  service.", 
            "title": "Getting Started"
        }, 
        {
            "location": "/labsite/#software-dependencies", 
            "text": "Several software packages are required for editing/maintaining the Andersen Lab site. They can be installed using  Homebrew :  brew install ruby imagemagick exiftool python ghostscript\nbrew upgrade ruby # If Ruby has not been updated in a while, you may need to do this.\nsudo gem install jekyll -v 3.6.0\n# If you get an error when trying to run pip, try:\n# brew link --overwrite python\npip install metapub pyyaml   Ruby  - Is used to run jekyll, which is the software that builds the site.  Jekyll  - As stated earlier, jekyll builds the static site and is written in Ruby.  Imagemagick  - Handles thumbnail generation and scaling photos. Imagemagick is used in the  build.sh  script.  exiftool  Extract data about photos as part of the  build.sh  script for use in scaling images.  Python  Retrieves information about publications and updates  _data/pubs_data.yaml .", 
            "title": "Software-Dependencies"
        }, 
        {
            "location": "/labsite/#cloning_the_repo", 
            "text": "To get started editing, clone the repo:  git clone https://github.com/andersenlab/andersenlab.github.io  This repo contains documents that get compiled into the Andersen Lab website. When you make changes to this repo and push them to GitHub, Github will trigger a 'build' of the labsite and update it. This usually takes less than a minute. Instructions for changing various aspects of the site are listed below.  You can also use  Github Desktop  to manage changes to the site.   If you want to edit the site locally and preview changes, run the following in the  root  directory of the git repo:  jekyll serve  The site should become available at  localhost:4000  and any changes you make will be reflected at that local url.", 
            "title": "Cloning the repo"
        }, 
        {
            "location": "/labsite/#updating_the_site", 
            "text": "In order for any change to become visible, you need to use git. Any subsequent directions that suggest modifying, adding, or removing files assumes you will be  committing  these changes to the repo and  pushing  the commit to GitHub.com. See  Git-SCM  for a basic introduction to git.", 
            "title": "Updating the site"
        }, 
        {
            "location": "/labsite/#andersenlabgithubio", 
            "text": "The structure of the Andersen Lab repo looks like this:  CNAME\nLICENSE\nREADME.md\nbuild.sh\nindex.html\n_config.yml\n_data/\n_includes/\n_layouts/\n_posts/\n_site/\nassets/\nfeeds/\nfiles/\npages/\npeople/\npublications/\nscripts/\nprotocols/\nfunding/  The folders prefixed with", 
            "title": "andersenlab.github.io"
        }, 
        {
            "location": "/labsite/#announcements", 
            "text": "Announcements are stored in the  _posts  folder. Posts are organized into folders by year. There is also a  _photo_albums  folder that you can ignore (more on this below). Two types of announcements can be made. A 'general' announcement regarding anything, or a new publication.", 
            "title": "Announcements"
        }, 
        {
            "location": "/labsite/#general_announcements", 
            "text": "To add a new post create a new text file with the following naming scheme:  YYYY-MM-DD-title.md  For example:  2017-09-24-A new post.md  The contents of the file should correspond to the following structure:  ---\ntitle:  The title of the post \nlayout: post\ntags: news\npublished: true\n---\n\nThe post content goes here!  The top part surrounded by  ---  is known as the  header  and has to define a number of variables:  layout: post ,  tags: news , and  published: true  should always be set and should not change. The only thing you will change is the  title . Set a title, and add content below.  Because we used a  *.md  extension when naming the file, we can use markdown in the post to create headings, links, images, and more.", 
            "title": "General Announcements"
        }, 
        {
            "location": "/labsite/#publication_post", 
            "text": "New publication posts can be created. These posts embed a publication summary identical to what you see on the publication page. They follow the same paradigm as above except they require two additional lines in the header:   subtitle:  - Usually the title of the paper; Appears on homepage.  PMID:  - The pubmed identifier   Example :  ---\ntitle:  Katie's paper accepted at  em G3 /em ! \nsubtitle:  Correlations of geneotype with climate parameters suggest  em Caenorhabditis elegans /em  niche adaptations \nlayout: post\ntags: news\npublished: true\nPMID: 27866149\n---\n\nCongratulations to Katie for her paper accepted at G3!", 
            "title": "Publication Post"
        }, 
        {
            "location": "/labsite/#lab_members", 
            "text": "", 
            "title": "Lab members"
        }, 
        {
            "location": "/labsite/#adding_new_lab_members", 
            "text": "(1)  - Add a photo of the individual to the  people/  folder.  (2)  - Edit the  _data/people.yaml  file, and add the information about that individual.   Each individual should have - at a minimum, the following:  - first_name:  first name \n  last_name:  last name \n  title:   The job title of the individual; e.g. Graduate Student; Research Associate; Undergrad; Postdoctoral Researcher; Lab technician \n  photo:  filename of the photo located in the people/ directory   Additional fields can also be added:  - first_name:  first name \n  last_name:  last name \n  title:  The job title of the individual; e.g. Graduate Student; Research Associate; Undergrad; Postdoctoral Researcher; Lab technician \n  pub_names: [ an array ,  of possible ,  publication ,  names ]\n  photo:  base filename of the photo located in the people/ directory; e.g. 'dan.jpg' \n  website:  website \n  description:  a description of research \n  email:  email \n  github:  github username    Note  pub_names  is a list of possible ways an individual is referenced in the author list of a publication. This creates links from the publications page back to lab members on the people page.", 
            "title": "Adding new lab members:"
        }, 
        {
            "location": "/labsite/#set_status_to_former", 
            "text": "Lab members can be moved to the bottom of the people page under the 'former member' area. To do this add a  former: true  line for that individual and a  current_status:  line indicating what they are up to.  For example:  - first_name: Mostafa\n  pub_names: \n    - Zamanian M\n  last_name: Zamanian\n  description: My research broadly spans  neglected disease  genomics and drug discovery. I am currently working to uncover new genetic determinants of anthelmintic resistance and to develop genome editing technology for human pathogenic helminths.\n  title: Postdoctoral Researcher, 2015-2017\n  photo: Mostafa2014.jpg\n  former: true\n  github: mzamanian\n  email: zamanian@northwestern.edu\n  current_status: Assistant Professor at UW Madison --  a href='http://www.zamanianlab.org/' Zamanian Lab Website /a", 
            "title": "Set Status to Former"
        }, 
        {
            "location": "/labsite/#remove_lab_members", 
            "text": "Remove the persons information from  _data/people.yaml ; Optionally delete their photo.", 
            "title": "Remove lab members"
        }, 
        {
            "location": "/labsite/#funding", 
            "text": "Funding is managed using the  funding/  folder in the root directory and the data file  _data/funding_links.yaml . The  funding/  folder has two subfolders:  past/  and  current/  for past funding and current funding. Rename the logo file to be lowercase and simple.  To update funding simply place the logo of the institution providing funding in one of these folders and it will appear on the funding page under the heading corresponding to the subfolder in which it was placed. If you would like to add a link for the funding of that organization you can edit the  _data/funding_links.yaml  file.  This file is structured as a set of  basename: url  pairs:  nigms: https://www.nigms.nih.gov/Pages/default.aspx\nacs: http://www.cancer.org/\npew: http://www.pewtrusts.org/en\nniaid: https://www.niaid.nih.gov/\naws: https://aws.amazon.com/\nweinberg: http://www.weinberg.northwestern.edu/\nmod: http://www.marchofdimes.org/\ncbc: http://www.chicagobiomedicalconsortium.org/  Each acronym above corresponds with an image file in the  current/  or  past/  folder. Notice that the extension (e.g. jpeg/png, etc) does not matter. Just use the basename of the file and its associated link here.", 
            "title": "Funding"
        }, 
        {
            "location": "/labsite/#protocols", 
            "text": "Protocols are stored in the  protocols/  folder and their titles and pdfs are managed in  _data/protocols.yaml . To add a new protocol, add the PDF to the  protocols/  folder. Then add these lines to the  _data/protocols.yaml  file:  - Name: Title of Protocol\n  file: filename_of_protocol_in_protocols_folder.pdf\n  group:  em C. elegans /em  Phenotyping methods   name  - The name of the protocol  file  - The filename of the protocol within the  protocols/  folder.  group  - The grouping of the protocol; It will be nested under this grouping on the protocols page.   - name: Semi-Quantitative Brood Assay\n  file: SemiQuantitativeBroodAssay.pdf\n  group:  em C. elegans /em  Phenotyping Methods\n- name:  em Pseudomonas aeruginosa /em  Fast-killing assay /a \n  file: FKAprotocol.pdf\n  group:  em C. elegans /em  Phenotyping Methods\n- name:  em Staphylococcus aureus /em  killing assay /a \n  file: Staphaureus_Protocol.pdf\n  group:  em C. elegans /em  Phenotyping Methods\n- name:  em Bacillus thuringiensis /em  toxin assay on plates /a \n  file: Bacillus-thuringiensis-toxin-plate-assay.pdf\n  group:  em C. elegans /em  Phenotyping Methods  To remove a protocol, delete the pdf and remove the corresponding lines.", 
            "title": "Protocols"
        }, 
        {
            "location": "/labsite/#research", 
            "text": "The research portion of the site is structured as a set of sections - each devoted to a project/area. Navigate to  /pages/research  and you will see a set of files:   research.html  - This page controls the content at the top of the research page. It's an overview of research in the Andersen lab. You can edit the top portion between the  p [content] /p  tags freely to modify the top of the research page.  research-*.md  - These are the individual projects. These files look like this:   ---\ntitle: High-throughput approaches to understand conserved drug responses\nimage: worms_drugs2.jpg\norder: 1\n---\n\nBecause of the efforts of a number of highly dedicated scientists and citizen volunteers...\n\nTo this end, we deep sequenced all of these strains...  The page includes a header (the items located between  --- ) which includes a number of important items.   title  - the title to display for the research area.  image  - An image for that research area/project. This is the base name of the image placed in  /assets/img/research/  order  - A number indicating the order you would like the page ot appear in. Order is descending and any set of numbers can be used to determine sort order (e.g. 1, 2, 5, 8, 1000).", 
            "title": "Research"
        }, 
        {
            "location": "/labsite/#publications", 
            "text": "Elements used to construct the publications page of the website are stored in two places:   _data/pubs_data.yaml  - The publications data stores authors, pub date, journal, etc.  publications/  - The publications folder for PDFs, thumbnails, and supplementary files.   (1) Download a PDF of the publication  You will want to remove any additional PDF pages (e.g. cover sheets) if there are any present in the PDF. See  this guide  for information on removing pages from a PDF.  Save the PDF to  /publications/[year][tag]  Where  tag  is a unique identifier for the publication. In general, these have been the first author or the journal or a combination of both.  (Optional) PMID Known  If the PubMed Identifier (PMID) is known for the publication, you can add it to the file  publications/publications_list.txt .  (2) Run  build.sh  The  build.sh  script does a variety of tasks for the website. For publications - it will generate thumbnails. It will also fetch information for publications and add it to the  _data/pubs_data.yaml  file  if  a PMID has been provided. If you did not add a PMID, you will have to manually add authors, journal, etc. to the  _data/pubs_data.yaml  file.  (3) Edit  _data/pubs_data.yaml  The publication should now be added either manually or automatically to  _data/pubs_data.yaml  and should look something like this:  - Authors: [Laricchia KM, Zdraljevic S, Cook DE, Andersen EC]\n  Citations: 0\n  DOI: 10.1093/molbev/msx155\n  Journal: Molecular Biology and Evolution\n  PMID: 28486636\n  Title: Natural variation in the distribution and abundance of transposable elements\n    across the Caenorhabditis elegans species  You will need to add a few things:\n  - Add a  PDF:  line to associate the publication with the correct PDF and its thumbnail. This is the same  tag  you used above.\n  - If there is no  Date_Published:  line you will want to add that. The format is  YYYY-month_abbr-DD  (e.g.  2017 Aug 17 ).\n  - Add  em  tags around items you want to italicize:  em Caenorhabditis elegans /em  Final result:  - Authors: [Laricchia KM, Zdraljevic S, Cook DE, Andersen EC]\n  Citations: 0\n  DOI: 10.1093/molbev/msx155\n  Date_Published: 2017 May 09\n  Journal: Molecular Biology and Evolution\n  PMID: 28486636\n  Title: Natural variation in the distribution and abundance of transposable elements\n    across the  em Caenorhabditis elegans /em  species\n  PDF: 2017Laricchia  (4) Add supplementary data  Supplemental data and figures are stored in  publications/[pdf_name] . For example,  2017Laricchia  has an associated folder in  publications/  where supplemental data and figures are stored:  publications/2017Laricchia/ supplemental files  Once you have added supplemental files, you'll need to add some information to  _data/pubs_data.yaml  to describe them. These are the lines that were added for  2017Laricchia :    pub_data:\n    files:\n      Supplemental_Figures.pdf: {title: Supplemental Figures}\n      Supplemental_Files.zip: {title: Supplemental Files}\n      Supplemental_Tables.zip: {title: Supplemental Tables}\\\n      ...\n       name of file : {title:  title to display }  The last line above illustrates the format. The name of the file must match exactly what is in the  publications/[pdf_name]  folder. Resulting supplemental data will be listed under publications and on the data page.", 
            "title": "Publications"
        }, 
        {
            "location": "/labsite/#photo_albums", 
            "text": "Photo albums can be added to the Andersen Labsite. Adding albums requires two utilities to be installed on your computer:   (a) Image Magick  (b) exiftool   These can easily be installed with  homebrew . should have been installed during Setup (above), but if not you can install them using the following:  brew install imagemagick\nbrew install exiftool  (1) Place images in a folder and name it according to the following schema:  YYYY-MM-DD-title  For example,  2017-08-05-Hawaii Trip .  (2) Move that folder to  /people/albums/  (3) Run the  build.sh  script in the root of the  andersenlab.github.io  repo.  The  build.sh  script will do the following:   (a) Construct pages for the album being published.  (b) Decrease the size of the images in the album (max width=1200).     Note  The  build.sh  script also performs other maintenance-related tasks. It is fine to run this script at anytime.   You can run the script using:  bash build.sh  (4) Add the images using git and push to GitHub  You can easily add all images using:  git add *.jpg  (5) Push changes to github  git push", 
            "title": "Photo Albums"
        }, 
        {
            "location": "/labsite/#software", 
            "text": "If you can write software you should be able to figure out how to update this section. It's markdown/html and not overly complicated.", 
            "title": "Software"
        }
    ]
}